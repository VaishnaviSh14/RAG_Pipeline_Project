{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7b489e",
   "metadata": {},
   "source": [
    "### Data Ingestion \n",
    "### Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0860f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "685613e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Vaishnavi Sharma', 'date_created': '2025-08-01'}, page_content='This is the main content I am using to create RAG')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\"This is the main content I am using to create RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Vaishnavi Sharma\",\n",
    "        \"date_created\":\"2025-08-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59dcca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt dir inside data\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "06c0d6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text file created\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/langchain.txt\":\"\"\"Langchain Introduction\n",
    "LangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\n",
    "\n",
    "With LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\n",
    "\n",
    "This framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\n",
    "\"\"\",\n",
    "\n",
    "\n",
    " \"../data/text_files/ml_basics.txt\":\"\"\"Machine Learning (ML) is a field of artificial intelligence that allows computers to learn patterns from data and make decisions without being explicitly programmed.\n",
    " \n",
    "It includes supervised learning, unsupervised learning, and reinforcement learning. \n",
    "\n",
    "Popular ML libraries include scikit-learn, TensorFlow, and PyTorch.\n",
    "\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "024c4c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/langchain.txt'}, page_content='Langchain Introduction\\nLangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\\n\\nWith LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\\n\\nThis framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\\n')]\n"
     ]
    }
   ],
   "source": [
    "### Text Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/langchain.txt\",encoding=\"utf-8\")\n",
    "### reads the file and returns a list of Document objects\n",
    "document=loader.load()\n",
    "print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "950b301f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\langchain.txt'}, page_content='Langchain Introduction\\nLangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\\n\\nWith LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\\n\\nThis framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\\n'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\ml_basics.txt'}, page_content='Machine Learning (ML) is a field of artificial intelligence that allows computers to learn patterns from data and make decisions without being explicitly programmed.\\n\\nIt includes supervised learning, unsupervised learning, and reinforcement learning. \\n\\nPopular ML libraries include scikit-learn, TensorFlow, and PyTorch.\\n\\n')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "## Load all the text files from directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files\n",
    "    loader_cls= TextLoader, ##Loader class to use\n",
    "    loader_kwargs={'encoding':'utf-8'},\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25bdcb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\\nrobotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\\nyet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\\nsystematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\nWizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020\\n2021\\n2022\\n2023\\n2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\nPaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\\nand open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs.\\nWhile\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot.\\nFine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.\\nThese abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41].\\nParam-\\neter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of\\nLLM-related contributions.\\nResearchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners effec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other.\\nMoreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs.\\nTo speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0, x)\\n(1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product (⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\\n(2)\\nwhere X is the input of layer and l, W, b, V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\\nS wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can differenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write efficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based.\\nClassifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can affect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources.\\nThis data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The difference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output.\\nHere, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an efficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives.\\nFor\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of different training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with different\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning.\\nGenerating\\nreasons is possible only by using different prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu-α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nhWk\\nhTHT\\nL\\n(3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model.\\nThe training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the effect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF(LN2(x))\\n(4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with differences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90].\\nTo gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7× larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3× GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little different data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R, S, X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-α and extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector.\\nMLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='3.1.2. Coding\\nCodeGen [140]:\\nCodeGen has a similar architecture to\\nPaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\\nbeddings. The model is trained on both natural language and\\nprogramming language data sequentially (trained on the first\\ndataset, then the second, and so on) on the following datasets\\n1) PILE, 2) BIGQUERY, and 3) BIGPYTHON. CodeGen pro-\\nposed a multi-step approach to synthesizing code. The purpose\\nis to simplify the generation of long sequences where the previ-\\nous prompt and generated code are given as input with the next\\nprompt to generate the next code sequence. CodeGen open-\\nsource a Multi-Turn Programming Benchmark (MTPB) to eval-\\nuate multi-step program synthesis.\\nCodex [141]: This LLM is trained on a subset of public Python\\nGithub repositories to generate code from docstrings. Com-\\nputer programming is an iterative process where the programs\\nare often debugged and updated before fulfilling the require-\\nments. Similarly, Codex generates 100 versions of a program\\nby repetitive sampling for a given description, which produces\\na working solution for 77.5% of the problems passing unit tests.\\nIts powerful version powers Github Copilot2.\\nAlphaCode [142]: A set of large language models, ranging\\nfrom 300M to 41B parameters, designed for competition-level\\ncode generation tasks. It uses the multi-query attention [143] to\\nreduce memory and cache costs. Since competitive program-\\nming problems highly require deep reasoning and an under-\\nstanding of complex natural language algorithms, the Alpha-\\nCode models are pre-trained on filtered GitHub code in popular\\nlanguages and then fine-tuned on a new competitive program-\\nming dataset named CodeContests. The CodeContests dataset\\nmainly contains problems, solutions, and test cases collected\\nfrom the Codeforces platform3. The pre-training employs stan-\\ndard language modeling objectives, while GOLD [144] with\\ntempering [145] serves as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\nCodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with\\nshallow encoder and deep decoder, trained in multiple stages\\ninitially unimodal data (code) and later bimodal data (text-code\\npairs). Each training stage has different training objectives and\\nactivates different model blocks encoder, decoder, or both ac-\\ncording to the task. The unimodal pre-training includes span\\ndenoising and CLM objectives, whereas bimodal pre-training\\nobjectives contain contrastive learning, matching, and CLM for\\ntext-code pairs. CodeT5+ adds special tokens with the text to\\nenable task modes, for example, [CLS ] for contrastive loss,\\n[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder\\narchitecture, employing Flash attention to scale up the context\\nlength to 8k. The StarCoder trains an encoder to filter names,\\n2https://github.com/features/copilot\\n3https://codeforces.com/\\nemails, and other personal data from the training data. Its fine-\\ntuned variant outperforms PaLM, LLaMA, and LAMDA on\\nHumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge\\nGalactica [148]: A large curated corpus of human scientific\\nknowledge with 48 million papers, textbooks, lecture notes,\\nmillions of compounds and proteins, scientific websites, en-\\ncyclopedias, and more are trained using the metaseq library3,\\nwhich is built on PyTorch and fairscale [149]. The model wraps\\nreasoning datasets with the < work > token to provide step-by-\\nstep reasoning context to the model, which has been shown to\\nimprove the performance on reasoning tasks.\\n3.1.4. Dialog\\nLaMDA [150]: A decoder-only model pre-trained on pub-\\nlic dialog data, public dialog utterances, and public web doc-\\numents, where more than 90% of the pre-training data is in\\nEnglish. LaMDA is trained with the objective of producing re-\\nsponses that exhibit high levels of quality, safety, and grounded-\\nness. To achieve this, discriminative and generative fine-tuning\\ntechniques are incorporated to enhance the model’s safety and\\nquality aspects. As a result, the LaMDA models can be utilized\\nas a general language model performing various tasks.\\n3.1.5. Finance\\nBloombergGPT [151]: A non-causal decoder model trained\\nusing both financial (“FINPILE” from the Bloomberg archive)\\nand general-purpose datasets. The model’s architecture is sim-\\nilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\\neters to different blocks of the model using the approach [113].\\nFor effective training, BloombergGPT packs documents to-\\ngether with < |endoftext| > to use the maximum sequence\\nlength, uses warmup batch size starting from 1024 to 2048, and\\nmanually reduces the learning rate multiple times during the\\ntraining.\\nXuan Yuan 2.0 [152]: A Chinese financial chat model with\\nBLOOM’s [13] architecture trained on a combination of general\\npurpose, financial, general purpose instructions, and financial\\ninstitutions datasets. Xuan Yuan 2.0 combined the pre-training\\nand fine-tuning stages to avoid catastrophic forgetting.\\n3.2. Fine-Tuned LLMs\\nPre-trained LLMs have excellent generalization abilities to\\nunseen tasks. However, because they are generally trained with\\nthe objective of next token prediction, LLMs have limited ca-\\npacity to follow user intent and are prone to generate unethical,\\ntoxic or inaccurate responses [20]. For their effective utiliza-\\ntion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\\ngenerate safe responses [20], which also results in increasing\\nzero-shot, few-shot, and cross-task generalization [97, 16, 18],\\nwith minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].\\nWe review various fine-tuned LLMs and strategies for effective\\nfine-tuning in this section.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\\nModels\\nFindings & Insights\\nT5\\n• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\\nclassification layers\\nGPT-3\\n• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\\nlearners\\nmT5\\n• Large multi-lingual models perform equivalently to single language models on downstream tasks.\\nHowever, smaller multi-lingual models perform worse\\nPanGu-α\\n• LLMs have good few shot capabilities\\nCPM-2\\n• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\\nble to full model fine-tuning\\n• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n• Inserting prompt tokens in-between sentences can allow the model to understand relations between\\nsentences and long sequences\\n• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\\n(aggregate information with the input text) for the model\\nERNIE 3.0\\n• A modular LLM architecture with a universal representation module and task-specific representa-\\ntion module helps in the finetuning phase\\n• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\\nan efficient way to take advantage of the powerful pre-trained model\\nJurassic-1\\n• The performance of LLM is highly related to the network size\\n• To improve runtime performance, more operations can be performed in parallel (width) rather than\\nsequential (depth)\\n• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\\ncabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\\nbenefits in few-shot learning tasks\\nHyperCLOVA\\n• By employing prompt-based tuning, the performances of models can be improved, often surpassing\\nthose of state-of-the-art models when the backward gradients of inputs are accessible\\nYuan 1.0\\n• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\\nbehavior in zero-shot and few-shot learning\\nGopher\\n• Relative encodings enable the model to evaluate for longer sequences than training.\\nERNIE 3.0 Titan\\n• Additional self-supervised adversarial loss to distinguish between real and generated text improves\\nthe model performance as compared to ERNIE 3.0\\nGPT-NeoX-20B\\n• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\\nlayers\\n• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations\\nfrom growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot\\nTable Continued on Next Page\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='Models\\nFindings & Insights\\nOPT\\n• Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n• Model is prone to generate repetitive text and stuck in a loop\\nGalactica\\n• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\\ndomain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\\nresearch on LLMs\\n• A working memory token approach can achieve strong performance over existing methods on\\nmathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\\ntasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\nGLaM\\n• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\\nin each transformer layer with a mixture-of-experts (MoE)\\n• The model trained on filtered data shows consistently better performances on both NLG and NLU\\ntasks, where the effect of filtering is more significant on the former tasks\\n• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\\nthe downstream tasks\\n• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\\nthe MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\\nmance\\nLaMDA\\n• The model can be fine-tuned to learn to call different external information resources and tools\\nAlphaCode\\n• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed\\nwith a shallower encoder and a deeper decoder\\n• To achieve better performances, it is necessary to employ strategies such as massively scaling\\nupsampling, followed by the filtering and clustering of samples into a compact set\\n• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-\\nscale sampling is crucial\\n• Simplifying problem descriptions can effectively improve the model’s performance\\nChinchilla\\n• The model size and the number of training tokens should be scaled proportionately: for each dou-\\nbling of the model size, the number of training tokens should be doubled as well\\nPaLM\\n• English-centric models produce better translations when translating to English as compared to non-\\nEnglish\\n• Generalized models can have equivalent performance for language translation to specialized small\\nmodels\\n• Larger models have a higher percentage of training data memorization\\n• Performance has not yet saturated even at 540B scale, which means larger models are likely to\\nperform better\\nAlexaTM\\n• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\\ncontext than decoder-only\\n• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\\nlearning\\n• Placing layer norm at the beginning of each transformer layer improves the training stability\\nTable Continued on Next Page\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='Models\\nFindings & Insights\\nU-PaLM\\n• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\\ndiversity\\nUL2\\n• Mode switching training enables better performance on downstream tasks\\n• CoT prompting outperforms standard prompting for UL2\\nGLM-130B\\n• Pre-training data with a small proportion of multi-task instruction data improves the overall model\\nperformance\\nCodeGen\\n• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\\neration\\nLLaMA\\n• A constant performance improvement is observed when scaling the model\\n• Smaller models can achieve good performances with more training data and computing time\\nPanGu-Σ\\n• Sparse models provide the benefits of large models at a lower computation cost\\n• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for\\ncontinual learning\\n• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\\ncost-efficient while maintaining a performance similar to the original\\nBloombergGPT\\n• Pre-training with general-purpose and task-specific data improves task performance without hurt-\\ning other model capabilities\\nXuanYuan 2.0\\n• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5+\\n• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\\nfor better performance\\nStarCoder\\n• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2\\n• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\\nfine-tuning\\n• Model trained on unfiltered data requires fewer samples for safety alignment\\nPaLM-2\\n• Data quality is important to train better models\\n• Model and data size should be scaled with 1:1 proportions\\n• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1\\n• Increasing batch size gradually stabilizes the training without loss spikes\\n• High-quality data at the final stages of training improves the model performance\\n• Increasing model context length windows step-wise allows it to better adapt to various sequence\\nlengths\\nNemotron-40B\\n• Model aligned iteratively on synthetic data with data generated from the previously aligned model\\nachieves competitive performance\\nDeepSeek\\n• Batch size should increase with the increase in compute budget while decreasing the learning rate\\nDeepSeek-v2\\n• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring\\na significantly smaller KV cache, therefore achieving faster data generation\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels\\nFindings & Insights\\nT0\\n• Multi-task prompting enables zero-shot generalization and outperforms baselines\\n• Even a single prompt per dataset task is enough to improve performance\\nWebGPT\\n• To aid the model in effectively filtering and utilizing relevant information, human labelers play a\\ncrucial role in answering questions regarding the usefulness of the retrieved documents\\n• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\\nend-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n• Generating answers with references can make labelers easily judge the factual accuracy of answers\\nTk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks\\n• More tasks improve generalization whereas only increasing task instances does not help\\n• Supervised trained models are better than generalized models\\n• Models pre-trained with instructions and examples perform well for different types of inputs\\nmT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before\\n• Multi-lingual training leads to even better zero-shot generalization for both English and non-\\nEnglish\\n• Training on machine-translated prompts improves performance for held-out tasks with non-English\\nprompts\\n• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\\nother pre-trained language tasks\\nOPT-IML\\n• Creating a batch with multiple task examples is important for better performance\\n• Only example proportional sampling is not enough, training datasets should also be proportional\\nfor better generalization/performance\\n• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\\nwhereas fully supervised tasks have no effect\\n• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\\n• Only 1% reasoning data improves the performance, adding more deteriorates performance\\n• Adding dialogue data makes the performance worse\\nSparrow\\n• Labelers’ judgment and well-defined alignment rules help the model generate better responses\\n• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\\nraters\\n• The combination of reinforcement learning (RL) with reranking yields optimal performance in\\nterms of preference win rates and resilience against adversarial probing\\nFlan\\n• Finetuning with CoT improves performance on held-out tasks\\n• Fine-tuning along with CoT data improves reasoning abilities\\n• CoT tuning improves zero-shot reasoning\\n• Performance improves with more tasks\\n• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n• Improving the model’s performance with instruction tuning is compute-efficient\\n• Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder\\n• Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\nLLaMA-2-Chat\\n• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\\nRLHF step further improves model safety and make it less prone to jailbreak attacks\\nLIMA\\n• Less high quality data is enough for fine-tuned model generalization\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Figure 10: This example illustrates the PanGu-P architecture, as depicted in\\nthe image sourced from [92].\\n3.2.1. Instruction-Tuning with Manually Created Datasets\\nNumerous hand-crafted instruction-tuning datasets with\\ndifferent design choices are proposed in the literature to\\ninstruction-tune LLMs. The performance of fine-tuned LLMs\\ndepends on multiple factors, such as dataset, instruction diver-\\nsity, prompting templates, model size, and training objectives.\\nKeeping this in view, diverse fine-tuned models have emerged\\nin the literature using manually created datasets.\\nThe models T0 [17] and mT0 (multi-lingual) [154] employ\\ntemplates to convert existing datasets into prompt datasets.\\nThey have shown improvements in generalization to zero-shot\\nand held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\\nwith in-context instructions to study generalization on unseen\\ntasks when given in-context instructions during test time. The\\nmodel outperformed Instruct-GPT, despite being smaller in\\nsize, i.e., 11B parameters as compared to 175B of GPT-3.\\nIncreasing Tasks and Prompt Setups: Zero-shot and few-shot\\nperformance improves significantly by expanding task collec-\\ntion and prompt styles. OPT-IML [97] and Flan [16] curated\\nlarger 2k and 1.8k task datasets, respectively. While increasing\\ntask size alone is not enough, OPT-IML and Flan add more\\nprompting setups in their datasets, zero-shot, few-shot, and\\nCoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\\nfurther on 1.88M CoT samples. Another method [102] uses\\nsymbolic tasks with tasks in T0, Flan, etc.\\n3.2.2. Instruction-Tuning with LLMs Generated Datasets\\nGenerating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are often\\nwritten by humans, smaller in size, and less diverse. To over-\\ncome this, self-instruct [19] proposed an approach to prompt\\navailable LLMs to generate instruction-tuning datasets. Self-\\ninstruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\\ninstruction, and 1 sample per task and iteratively generates new\\ninstructions (52k) and instances (82k input-output pairs) using\\nFigure 11: An example image shows an instance of the Flan training paradigm,\\ntaken from [16].\\nGPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data\\nof datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.\\nLLaMA Tuned: Various models in the literature instruction-\\ntune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-\\nated datasets.\\nAmong these, Alpaca [158], Vicuna [159],\\nand LLaMA-GPT-4 [160] are a few general-purpose fine-tuned\\nmodels, where Alpaca is trained on 52k samples from text-\\ndavinci-003, Vicuna on 70k samples from ShareGPT.com, and\\nLLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\\n4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million\\nsamples) by generating data from ChatGPT and outperforms\\nGPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the\\nLLaMA’s consistent tokenization of numbers. HuaTuo [162] is\\na medical knowledge model, fine-tuned with a generated QA\\ndataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs\\nto convert given instructions into a more complex set. The in-\\nstructions are iteratively evolved with re-writing instructions in\\ncomplex wording and creating new instructions. With this style\\nof automated instruction generation, WizardLM [163] (fine-\\ntuned LLaMA on 250k instructions), outperforms Vicuna and\\nAlpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.\\n3.2.3. Aligning with Human Preferences\\nIncorporating human preferences into LLMs presents a\\nsignificant advantage in mitigating undesirable behaviors and\\nensuring accurate outputs. The initial work on alignment, such\\nas InstructGPT [20] aligns GPT-3 using a 3-step approach,\\ninstruction-tuning, reward modeling, and fine-tuning with\\nreinforcement learning (RL). The supervised fine-tuned GPT-3\\non demonstrations is queried to generate responses, which\\nhuman labelers rank according to human values, and a reward\\nmodel is trained on the ranked data. Lastly, the GPT-3 is trained\\nwith proximal policy optimization (PPO) using rewards on the\\ngenerated data from the reward model. LLaMA 2-Chat [21]\\nimproves alignment by dividing reward modeling into help-\\nfulness and safety rewards and using rejection sampling in\\naddition to PPO. The initial four versions of LLaMA 2-Chat\\nare fine-tuned with rejection sampling and then with PPO on\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='top of rejection sampling.\\nAligning with Supported Evidence: This style of alignment\\nallows the model to generate responses with proofs and facts,\\nreduces hallucination, and assists humans more effectively,\\nwhich increases trust in the model’s output.\\nSimilar to\\nthe RLHF training style, a reward model is trained to rank\\ngenerated responses containing web citations in answers\\nto questions, which is later used to train the model, as in\\nGopherCite [165], WebGPT [166], and Sparrow [167]. The\\nranking model in Sparrow [167] is divided into two branches,\\npreference reward and rule reward, where human annotators\\nadversarial probe the model to break a rule. These two rewards\\ntogether rank a response to train with RL.\\nAligning Directly with SFT: The PPO in the RLHF pipeline\\nis complex, memory-intensive, and unstable, requiring mul-\\ntiple models, reward, value, policy, and reference models.\\nAvoiding this sophisticated alignment pipeline is possible by\\nincorporating minimal changes in the supervised fine-tuning\\n(SFT) pipeline as in [168, 169, 170], with better or compa-\\nrable performance to PPO. Direct preference optimization\\n(DPO) [168] trains a model directly on the human-preferred\\nresponses to maximize the likelihood of preferred against\\nunpreferred responses, with per-sample importance weight.\\nReward ranked fine-tuning RAFT [169] fine-tunes the model\\non ranked responses by the reward model. Preference ranking\\noptimization (PRO) [171] and RRHF [170] penalize the model\\nto rank responses with human preferences and supervised loss.\\nOn the other hand, chain-of-hindsight (CoH) [172] provides\\nfeedback to the model in language rather than reward, to learn\\ngood versus bad responses.\\nAligning with Synthetic Feedback:\\nAligning LLMs with\\nhuman feedback is slow and costly. The literature suggests a\\nsemi-automated process to align LLMs by prompting LLMs to\\ngenerate helpful, honest, and ethical responses to the queries,\\nand fine-tuning using the newly created dataset. Constitutional\\nAI [173] replaces human feedback in RLHF with AI, calling\\nit RL from AI feedback (RLAIF). AlpacaFarm [174] designs\\nprompts to imitate human feedback using LLMs APIs. Oppo-\\nsite to constitutional AI, AlpacaFarm injects noise in feedback\\nto replicate human mistakes.\\nSelf-Align [98] prompts the\\nLLM with ICL examples, instructing the LLM about what the\\nresponse should contain to be considered useful and ethical.\\nThe same LLM is later fine-tuned with the new dataset.\\nAligning with Prompts: LLMs can be steered with prompts to\\ngenerate desirable responses without training [175, 176]. The\\nself-correction prompting in [176] concatenates instructions\\nand CoT with questions, guiding the model to answer its\\ninstruction following a strategy to ensure moral safety before\\nthe actual answer. This strategy is shown to reduce the harm in\\ngenerated responses significantly.\\nRed-Teaming/Jailbreaking/Adversarial\\nAttacks:\\nLLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-\\nformation, and other shortcomings through adversarial probing.\\nThe models are susceptible to generating harmful responses\\neven though they are aligned for safety [177, 178].\\nRed-\\nteaming is a common approach to address illicit outputs, where\\nthe LLMs are prompted to generate harmful outputs [178, 179].\\nThe dataset collected through red-teaming is used to fine-tune\\nmodels for safety. While red-teaming largely relies on human\\nannotators, another work [180] red-team LLMs to find prompts\\nthat lead to harmful outputs for other LLMs.\\n3.2.4. Continue Pre-Training\\nAlthough fine-tuning boosts a model’s performance, it leads\\nto catastrophic forgetting of previously learned information.\\nConcatenating fine-tuning data with a few randomly selected\\npre-training samples in every iteration avoids network forget-\\nting [181, 152]. This is also effective in adapting LLMs for\\ncases where fine-tuning data is small and the original capac-\\nity is to be maintained. Prompt-based continued pre-training\\n(PCP) [182] trains the model with text and instructions related\\nto tasks and then finally instruction-tunes the model for down-\\nstream tasks.\\n3.2.5. Sample Efficiency\\nWhile fine-tuning data is generally many-fold smaller than\\nthe pre-training data, it still has to be large enough for accept-\\nable performance [16, 97, 18] and requires proportional com-\\nputing resources. Studying the effects on performance with less\\ndata, existing literature [183, 184] finds that models trained\\non less data can outperform models trained with more data.\\nIn [183], 25% of the total downstream data is found enough\\nfor state-of-the-art performance. Selecting coreset-based 0.5%\\nof the total instruction-tuning data improves the model perfor-\\nmance by 2% in [184], as compared to the complete data tun-\\ning. Less is more for alignment (LIMA) [185] uses only 1000\\ncarefully created demonstrations to fine-tune the model and has\\nachieved comparable performance to GPT-4.\\n3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-\\npensive attention and high memory requirements.\\nA model\\ntrained on limited sequence lengths fails to generalize to unseen\\nlengths at inference time [186, 49]. Alternatively, LLMs with\\nALiBi [65] positional encodings can perform zero-shot length\\nextrapolation. However, ALiBi has less expressive power [66]\\nand inferior performance on multiple benchmarks [46], and\\nmany LLMs use RoPE positional embedding that is unable to\\nperform zero-shot extrapolation. A larger context length has\\nbenefits such as a better understanding of longer documents,\\nmore samples in in-context learning, execution of bigger rea-\\nsoning processes, etc. Expanding context length during fine-\\ntuning is slow, inefficient, and computationally expensive [49].\\nTherefore, researchers employ various context window extrap-\\nolation techniques discussed below.\\nPosition Interpolation: Rather than extrapolating, [49] shows\\nthat interpolating position encodings within the pre-trained con-\\ntext window are more effective. The work demonstrates that\\nonly 1000 steps of fine-tuning are enough to achieve better re-\\nsults on larger windows without reducing performance com-\\npared to the original context size. Giraffe [46] uses power scal-\\ning in RoPE, and YaRN [47] proposed NTK-aware interpola-\\ntion.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='Efficient Attention Mechanism:\\nDense global attention is\\none of the major constraints in training larger context win-\\ndow LLMs.\\nUsing efficient attention variants, such as lo-\\ncal, sparse, and dilated attention, reduces the computation cost\\nsignificantly.\\nLongT5 [48] proposes transient global atten-\\ntion (TGlobal), applying attention to local and global tokens\\n(windowed token averaging).\\nThe model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on\\n4098 sequence length, fine-tunes on larger window sizes, as\\nlarge as 16k, and improves task performance on longer inputs.\\nThis shows the extrapolation ability of TGlobal attention with\\nonly fine-tuning. COLT5 [187] uses two branches, one with\\nlightweight and the other with heavyweight attention and feed-\\nforward layers. All tokens are processed from the lightweight\\nbranch, and only important tokens are routed to the heavy-\\nweight branch. LongNet [188] replaces standard attention with\\ndilated attention, expanding sequence length to 1 billion tokens.\\nLongLoRA [189] proposes shift-short attention, used during\\nfine-tuning to reduce dense attention costs. However, the model\\nduring inference uses dense attention and achieves similar per-\\nformance as full attention fine-tuning.\\nExtrapolation without Training: LM-Infinite [186] and par-\\nallel context windows (PCW) [190] show length extrapolation\\nis possible using pre-trained LLMs. LM-Infinite suggested Λ-\\nshaped attention applied within the original context window\\nlimits. Likewise, PCW chunks larger inputs into the pre-trained\\ncontext lengths and applies the same positional encodings to\\neach chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-\\nnated with the input, known as context augmentation, in-\\ncontext learning (ICL), or few-shot prompting. They show ex-\\ncellent generalization to unseen tasks with few-shot prompt-\\ning, enabling LLMs to answer queries beyond the capacity ac-\\nquired during training [6, 55]. These emergent abilities allow\\nfor adapting the model without fine-tuning—a costly process.\\nAside from this, hallucination, producing inaccurate, unsafe,\\nor factually incorrect responses, is common for LLMs, which is\\navoided by augmenting contextual data. While the user can pro-\\nvide in-context samples in the query [54, 32], here we specifi-\\ncally refer to the methods that access external storage program-\\nmatically, calling them augmented LLMs.\\nThe literature suggests various external memory designs to aug-\\nment LLMs, long-term [191, 192, 193, 194], short-term [195],\\nsymbolic [196], and non-symbolic [197, 198]. The memory\\ncan be maintained in different formats such as documents, vec-\\ntors, or databases. A few systems maintain intermediate mem-\\nory representations to retain information across multiple iter-\\nations [194, 192], while others extract important information\\nfrom the datasets and save it in memory for recall [199]. The\\nmemory read and write operations are performed either with\\nor without LLMs cooperation [192, 200, 194, 201], acting as\\na feedback signal in [195]. We discuss different types of aug-\\nmented LLMs below.\\nFigure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\\ntracts a similar context to the input and forwards it to the LLM either in simple\\nlanguage or encoded through Fusion-in-Decoder (FiD). Depending on the task,\\nretrieval and generation may repeat multiple times.\\n3.4.1. Retrieval Augmented LLMs\\nLLMs may have limited memory and outdated information,\\nleading to inaccurate responses. Retrieving relevant informa-\\ntion from external up-to-date storage enables the LLMs to\\naccurately answer with references and utilize more informa-\\ntion. With retrieval augmentation, smaller models have been\\nshown to perform at par with larger models. For instance, the\\n11B model can become competitive to 540B PaLM in [25] and\\n7.5B to 280B Gopher in [193]. Retrieval augmented language\\nmodeling (RALM) has two major components, shown in\\nFigure 12, namely: 1) retriever and 2) language model.\\nIn\\nRALM, the retriever plays a crucial role in driving LLM\\nresponse, where incorrect information can steer LLMs to false\\nbehavior. This leads to the development of various methods to\\nretrieve accurate information and fuse with the query for better\\nperformance.\\nZero-Shot Retrieval Augmentation: This kind of augmen-\\ntation keeps the original LLM architecture and weights\\nunchanged and uses BM25 [202], nearest neighbors, or frozen\\npre-trained models like Bert [7] as a retriever. The retrieved\\ninformation is provided as input to the model for response\\ngeneration, shown to improve performance over LLMs without\\nretrieval [198, 203].\\nIn some scenarios, multiple retrieval\\niterations are required to complete the task.\\nThe output\\ngenerated in the first iteration is forwarded to the retriever\\nto fetch similar documents. Forward-looking active retrieval\\n(FLARE) [197] initially generates the response and corrects\\nthe output by retrieving relevant documents if the response\\ncontains low-confidence tokens. Similarly, RepoCoder [204]\\nfetches code snippets recursively for code completion.\\nTraining with Retrieval Augmentation: To reduce failures in\\nretrieval augmentation generation (RAG), researchers train or\\nfine-tune retrievers and LLMs with a retrieval augmentation\\npipeline. We discuss the literature below based on their focus\\non the respective training processes of the pipeline.\\nTraining LLM: Retrieval-enhanced transformer (RETRO) [193]\\nshows pre-training smaller LLMs with RAG pipeline outper-\\nforms larger LLMs, such as GPT-3 trained without RAG.\\nRETRO uses a 2-trillion token subset of MassiveText as\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='a database.\\nThe retrieval pipeline divides the input query\\ninto subsets and retrieves relevant chunks from the database\\nfor each subset, encoded together with input intermediate\\nrepresentations for generating tokens. It uses cross-chunked\\nattention to attend to previous chunks auto-regressively.\\nA\\nstudy on RETRO [205] shows models pre-trained without RAG\\nbut fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.\\nTraining Retriever: Quality of responses generated by LLMs\\nis highly dependent on the in-context examples.\\nThere-\\nfore, [206, 207, 208, 209] train retrievers to retrieve accurate\\nfew-shot samples while keeping the LLM frozen for gener-\\nation.\\nRetrieved samples are ranked to build ground-truth\\ndata to train retrievers with contrastive learning in [206, 208].\\nRoBERTa is trained for downstream tasks in [207] for ICL\\nsamples retrieval.\\nREPLUG [209] trains the retriever with\\nsupervised signals from the frozen LLM-generated outputs.\\nTraining Retriever and LLM: Further benefits are achieved by\\ntraining both the retriever and the model in [25, 210, 211]. In\\nthis case, the error propagates back to the retriever, updating\\nboth the language model and the retriever.\\nWhile masked\\nlanguage modeling (MLM) is a common pre-training objec-\\ntive [25, 211], retrieval pre-trained transformer (RPT) [210]\\nused document chunk prediction as a pre-training objective for\\nlong text modeling.\\nEncoded Context Augmentation:\\nConcatenating retrieved\\ndocuments with the query becomes infeasible as the sequence\\nlength and sample size grow. Encoding the context and fusing\\nit with the decoder (Fusion-in-Decoder) using cross-attention\\nmakes it possible to augment more samples without increasing\\ncomputation costs significantly [212, 193, 210, 25].\\nWeb Augmented:\\nLocally stored memory, but external to\\nLLM, has limited information. However, a large amount of\\ninformation is available on the internet, which gets updated\\nregularly.\\nRather than storing information locally, various\\nmethods retrieve query-related context through a web search\\nand forward it to LLMs [213, 214, 166].\\n3.4.2. Tool Augmented LLMs\\nWhile RAG relies on the retriever to provide context to the\\nLLM to answer queries, tool augmented LLMs capitalize on the\\nreasoning abilities of LLMs to iteratively plan by dividing tasks\\ninto sub-tasks, selecting necessary tools, and taking actions to\\ncomplete the task [215, 216, 217, 27]. A generic pipeline of\\ntool-augmented LLMs is shown in Figure 13, where different\\nmodules in Figure 13 are selected in a loop until the task com-\\npletion.\\nZero-Shot Tool Augmentation: LLMs in-context learning and\\nreasoning abilities enable them to interact with tools with-\\nout training. Automatic reasoning and tool-use (ART) [217]\\nbuilds a task library with demonstrations of reasoning steps and\\ncalling external tools. It retrieves similar task examples and\\nprovides the context to the LLM for inference. Aside from\\nthis, [218] shows tool documentation is enough to teach LLMs\\nto use tools without demonstrations. RestGPT [219] integrates\\nLLMs with RESTful APIs by decomposing tasks into planning\\nFigure 13: A basic flow diagram of tool augmented LLMs. Given an input and\\na set of available tools, the model generates a plan to complete the task. The\\ntool augmented LLMs utilize different modules iteratively, such as retriever,\\ntool execution, read-write to memory, feedback, etc., depending on the task.\\nand API selection steps. The API selector understands the API\\ndocumentation to select a suitable API for the task and plan the\\nexecution. ToolkenGPT [220] uses tools as tokens by concate-\\nnating tool embeddings with other token embeddings. During\\ninference, the LLM generates the tool tokens representing the\\ntool call, stops text generation, and restarts using the tool exe-\\ncution output.\\nTraining with Tool Augmentation: LLMs are trained to inter-\\nact with diverse tools, enhancing planning abilities to overcome\\nthe limitations of zero-shot tool augmentation [221, 27, 222,\\n223]. Gorilla [221] instruction-tunes LLaMA with information\\nretrieval from API documentation. It uses the self-instruct [19]\\ndata generation pipeline with GPT-4 by providing in-context\\nexamples retrieved from API documentation. Tool augmented\\nlanguage model (TALM) [27] fine-tunes T5 [10] for tool use\\nwith a self-play approach, where it iteratively completes tool\\nmanipulation tasks and includes them back in the training set.\\nToolLLM [223] collects 16k APIs from RapidAPI. It samples\\nAPIs from the list to generate an instruction-tuning dataset us-\\ning ChatGPT in single-tool and multi-tool scenarios. For high-\\nquality datasets, ToolLLM suggested a depth-first search-based\\ndecision tree (DFSDT) method to generate ground-truths with\\ndiverse reasoning and planning.\\nMultimodal Tool Augmentation: The compositional reasoning\\ncapacity of LLMs allows them to manipulate tools in multi-\\nmodal settings [215, 216, 224]. Following the pipeline shown\\nin Figure 13, the LLM outlines a plan, generally executing in a\\nsequence: Plan →Tool selection →Execute →Inspect →\\nGenerate, to respond to the user query. Here, the database of\\ntools is rich in modalities, including text, images, etc. Many of\\nthe multimodal tool augmentation systems employ multimodal\\nLLMs [31, 225, 224, 216], while others utilize single modality\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='LLMs and generate a plan on using different modality tools to\\nsolve multimodal queries [226].\\n3.5. LLMs-Powered Agents\\nAI agents are autonomous entities, capable of planning,\\ndecision-making, and performing actions to achieve complex\\ngoals.\\nIn the early days, AI agents were rule-based, de-\\nsigned for narrow tasks, and had limited capabilities, such\\nas Clippy [227] and Deep Blue [228].\\nIn contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it\\npossible to incorporate them in diverse applications, includ-\\ning LLMs-powered agents [224, 216], where LLMs behave\\nas the brain of agents. LLMs have been incorporated in web\\nagents [166, 167], coding agents [229], tool agents [27, 223],\\nembodied agents [26], and conversational agents [195], requir-\\ning minimal to no fine-tuning\". Below we summarize the re-\\nsearch in LLMs-based autonomous agents. For a more detailed\\ndiscussion, please refer to [230, 231].\\nLLMs Steering Autonomous Agents: LLMs are the cognitive\\ncontrollers of the autonomous agents. They generate plans, rea-\\nson about tasks, incorporate memory to complete tasks, and\\nadapt the outline depending on the feedback from the environ-\\nment. Depending on the acquired capabilities of LLMs, many\\nmethods fine-tune, propose a better prompting approach, or uti-\\nlize different modules to enhance agents’ performance. Mod-\\nules and strategies employed in autonomous agents are briefly\\ndiscussed below.\\nPlanning and Reasoning: Completing a complex task requires\\nhuman-like logical thinking, planning necessary steps, and\\nreasoning current and future directions. Prompting methods\\nlike chain-of-thoughts [103], tree-of-thoughts [105], and self-\\nconsistency [104] are central to agents, eliciting LLMs to rea-\\nson its actions and choose among different paths for task com-\\npletion. When LLMs are prompted with a task description and\\na sequence of actions, they can accurately generate plan ac-\\ntions without any fine-tuning [232]. Reasoning via planning\\n(RAP) [233] incorporates a re-purposed LLM as a world model\\nto reason about future outcomes and explore alternative paths\\nfor task completion. Retroformer [234] uses a retrospective\\nLLM to improve main LLM planning and reasoning capabil-\\nities by providing helpful task cues.\\nFeedback: LLMs in open-loop systems generate plans and as-\\nsume that the agent will complete them successfully. However,\\nthe actual scenario is different with failures and variable re-\\nsponses from the environment. To correctly complete tasks,\\nmany methods use LLMs in a closed-loop where the action re-\\nsponse is provided as feedback to the LLMs to re-assess and\\nupdate the plan as required [235, 236, 237, 195]. Another di-\\nrection of research exploits LLMs as reward functions to train\\nreinforcement learning (RL) policies instead of humans [238].\\nMemory: LLMs can learn from the context provided in the\\nprompt. In addition to internal memory, various systems em-\\nploy external memory to save the response history.\\nReflex-\\nion [195] maintains an episodic memory to use previous re-\\nsponses as feedback to improve future decision-making. Retro-\\nformer [234] improves its responses by employing short-term\\nand long-term memory, where short-term memory contains re-\\ncent responses and long-term memory keeps summarized failed\\nattempts to add in the prompt as reflection.\\nMulti-Agents Systems: LLMs can play user-defined roles and\\nbehave like a specific domain expert. In multi-agent systems,\\neach LLM is assigned a unique role, simulating human behav-\\nior and collaborating with other agents to complete a complex\\ntask [229, 239].\\nLLMs in Physical Environment:\\nLLMs are good at\\ninstruction-following, however, utilizing them for physically\\ngrounded tasks requires adaptation, as they lack real-world\\nknowledge. This could lead to generating illogical responses\\nfor a particular physical situation [240, 26].\\nSayCan [240]\\nmake LLMs aware of the available low-level task operations.\\nLLM (Say) builds a high-level plan to complete the task and\\na learned affordance function (Can) explores the possibility of\\nexecuting the plan in the real world. SayCan uses RL to train\\nthe language-conditioned affordance function. PaLM-E enables\\nthe LLM to solve grounded tasks by training multi-modal LLM\\nfeeding inputs directly from the sensors.\\nManipulation: In the area of manipulation [236, 241], LLMs\\nenhance a robot’s dexterity and adaptability, excelling in tasks\\nlike object recognition, grasping, and collaboration. They ana-\\nlyze visual and spatial information to determine the most effec-\\ntive approach to interact with objects.\\nNavigation: LLMs enhance a robot’s ability to navigate com-\\nplex environments with precision and adaptability [242, 243,\\n244, 245]. They generate feasible paths and trajectories for\\nrobots, accounting for intricate environmental details [246].\\nThis ability is valuable in scenarios requiring precise and\\ndynamically adaptable navigation in environments like ware-\\nhouses, transport, healthcare facilities, and residences.\\n3.6. Efficient LLMs\\nDeploying LLMs in production is expensive. Reducing their\\nrunning costs while preserving performance is an appealing\\narea of research. This section summarizes the approaches sug-\\ngested to enhance LLMs’ efficiency.\\n3.6.1. Parameter Efficient Fine-Tuning\\nFine-tuning LLMs with tens or hundreds of billions of pa-\\nrameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\\n(540B), etc., is computationally intensive and time-consuming.\\nTo avoid complete model fine-tuning, numerous parameter-\\nefficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try\\nto achieve acceptable model fine-tuning performance at reduced\\ncosts. As compared to full fine-tuning [248], PEFT performs\\nbetter in low-resource setups, achieves comparable perfor-\\nmance on medium-resource scenarios, and performs worse than\\nfull fine-tuning under high-resource availability. An overview\\nof different PEFT approaches is shown in Figure 14.\\nAdapter Tuning: Adds a few trainable parameters within the\\ntransformer block. The adapter layer is a sequence of feature\\ndownscaling, non-linearity, and upscaling [106]. Variants of\\nadapter tuning inject adapter layers sequentially [106] and in\\nparallel [38], whereas the mixture of adapter (AdaMix) [249]\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\\nthe adapter tuning category.\\nemploys multiple adapter modules in a single layer. AdaMix\\nroutes input instances randomly to one of the multiple down-\\nscale and upscale modules. The mixture of adapters is averaged\\nout for inference to avoid additional latency. Low-Rank Adap-\\ntation (LoRA) [250] learns low-rank decomposed matrices to\\nfreeze original weights. The learned weights are fused with the\\noriginal weights for inference, avoiding latency.\\nPrompt Tuning: Prompting is an effective way to adapt a\\npre-trained LLM for the downstream task. However, manual\\nprompts bring uncertainty in the model’s prediction, where a\\nchange in a single word drops the performance [247]. Prompt\\ntuning alleviates this problem by fine-tuning only 0.001%-3%\\nadditional parameters [251]. It concatenates trainable prompt\\nparameters with the model embeddings [247, 40, 251]. Task-\\nspecific fixed discrete prompts are concatenated with input em-\\nbeddings in [40]. As discrete prompts bring instability, prompts\\nare encoded through a learnable mapping in P-Tuning [247],\\nnaming continuous prompts, which are appended with the dis-\\ncrete prompts.\\nOnly the prompt encoder is trainable in the\\nmodel. In an extension of P-Tuning, continuous prompts are\\nconcatenated with each layer of the network in [251]. Progres-\\nsive prompts [252] avoid catastrophic forgetting and transfer\\npreviously learned knowledge by sequentially adding trainable\\nprompt embeddings to the previously frozen task embeddings.\\nPrefix Tuning: A set of trainable task-specific prefix vectors\\nare appended to the frozen transformer layers in prefix tun-\\ning [41]. The prefix vectors are virtual tokens attended by the\\ncontext tokens on the right. In addition, adaptive prefix tun-\\ning [253] applies a gating mechanism to control the information\\nfrom the prefix and actual tokens.\\nBias Tuning: Fine-tuning only bias terms in small to medium\\ntraining data has been found effective in BitFit [254].\\nThis\\nmethod achieves full fine-tuning performance for tasks with less\\ntraining data and comparable performance with more training\\ndata.\\n3.6.2. Quantization\\nLLMs require extensive computing and memory for infer-\\nence.\\nDeploying a 175B parameter GPT-3 model needs at\\nleast five 80GB A100 GPUs and 350GB of memory to store in\\nFP16 format [44]. Such demanding requirements for deploying\\nLLMs make it harder for smaller organizations to utilize them.\\nModel compression is an effective solution but comes at the cost\\nof degraded performance, especially at large scales greater than\\n6B. These models exhibit very large magnitude outliers that do\\nnot exist in smaller models [255], making it challenging and re-\\nquiring specialized methods for quantizing LLMs [44, 256].\\nPost-Training Quantization: Minimal or no training is re-\\nquired in this type of quantization, without significantly com-\\npromising the model performance. LLM-8-bit [255] uses full-\\nprecision matrix multiplication for weights associated with out-\\nlier features and 8-bit for remaining features. The lower pre-\\ncision multiplication outputs are converted to FP-16 and con-\\ncatenated with others. The quantized models have homogenous\\nword embeddings, which may degrade their performance. To\\nfix this, token-level knowledge distillation is employed in [45]\\nalong with independent quantization scaling factors for each\\nmodule due to varying weight distribution. Feature distribu-\\ntions are asymmetric and appear in different channels; outlier\\nsuppression [257] shifts and scales per-channel activation dis-\\ntributions for effective quantization. SmoothQuant [44] quan-\\ntizes activations and weights to INT8 format by smoothing\\nactivations and migrating the quantization difficulty toward\\nweights. It multiplies the inverse of the smoothing factor with\\nweights, which introduces a few outliers in the weights but is\\neasier to quantify than unsmoothed activations. OPTQ [256]\\nuses the optimal brain compression (OBC) [258] algorithm to\\nquantize the model layer-by-layer and update weights to com-\\npensate for quantization error.\\nTo improve speed and per-\\nformance, OPTQ updates weights in arbitrary order, employs\\nlazy updates, and uses better Cholesky kernels. Outlier-aware\\nweight quantization (OWQ) [259] uses the OPTQ algorithm for\\nquantization but assigns higher precision to vulnerable weights,\\ncausing outliers and lower precision for others.\\nQuantization-Aware Training:\\nTo compensate for perfor-\\nmance degradation,\\na quantized model is fine-tuned in\\nquantization-aware training (QAT) [260, 261, 262].\\nAl-\\npha Tuning quantizes the model using binary coding quan-\\ntization (BCQ) [263] and fine-tunes only quantization scal-\\ning factors.\\nThis approach improves performance over\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='parameter-efficient fine-tuning of the pre-trained model. Sim-\\nilarly, parameter-efficient and quantization-aware adaptation\\n(PEQA) [264] reduces the precision of fully-connected layers\\nand fine-tunes only quantization scaling parameters.\\nLLM-\\nQAT [262] generates training data from the pre-trained network\\nand trains a quantized student model with knowledge distilla-\\ntion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM\\nwith LoRA [250] using a 4-bit normal float, which shows better\\nperformance over a 4-bit integer and float.\\n3.6.3. Pruning\\nPruning is an alternative approach to quantization to com-\\npress model size, thereby reducing LLMs deployment costs\\nsignificantly. Compared to task-agnostic pruning, task-specific\\npruning is easily achievable with good performance, where a\\nmodel is fine-tuned on the downstream task and pruned for\\nfaster inference. It is possible to prune LLMs for individual\\ntasks, but the cost of pruning and deploying task-specific mod-\\nels is high. To overcome this, many structured and unstructured\\npruning methods for LLMs have been proposed to maintain rea-\\nsonable performance across all tasks while shrinking the model\\nsize [265, 42, 266].\\nUnstructured Pruning: This kind of pruning removes less im-\\nportant weights without maintaining any structure.\\nExisting\\nLLM pruning methods take advantage of the unique charac-\\nteristics of LLMs, uncommon for smaller models, where a\\nsmall subset of hidden states are activated with large magni-\\ntude [255]. Pruning by weights and activations (Wanda) [265]\\nprunes weights in every row based on importance, calculated\\nby multiplying the weights with the norm of input. The pruned\\nmodel does not require fine-tuning, thereby saving computa-\\ntional costs. Outlier weighed layerwise sparsity (OWL) [267]\\nextends Wanda with non-uniform layer pruning. It shows that\\nthe number of outliers varies for different layers; therefore, the\\nmodel should have variable pruning ratios for better perfor-\\nmance for every layer. Contrastive pruning (CAP) [43] itera-\\ntively prunes the model by training the sparse model using con-\\ntrastive loss between pre-trained, fine-tuned, and snapshots of\\nprevious sparse models to learn task-specific and task-agnostic\\nknowledge.\\nStructured Pruning: Here, the parameters are removed in\\ngroups, rows, columns, or matrices, which speeds up the\\ninference because of effective hardware tensor core utiliza-\\ntion [265].\\nLLM-Pruner [42] employs a 3-stage structured\\npruning strategy, identifying the groups of hidden states caus-\\ning each other to activate during the forward-pass, keeping im-\\nportant groups and removing less important ones, and fine-\\ntuning the pruned model with LoRA. Sparsity-induced mask\\nlearning (SIMPLE) [268] prunes the network using learnable\\nmasks. Similarly, another method prunes LLMs by learning\\nmasks and removing unimportant rank-1 components of the\\nfactorized weight matrix [266].\\n3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-\\ning applications, an increasing number of research works are\\nnow facilitating LLMs to perceive different modalities of infor-\\nmation like image [269, 270, 271], video [272, 273, 274], au-\\ndio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present\\nsubstantial benefits compared to standard LLMs that process\\nonly text. By incorporating information from various modal-\\nities, MLLMs can achieve a deeper understanding of context,\\nleading to more intelligent responses infused with a variety of\\nexpressions. Importantly, MLLMs align closely with human\\nperceptual experiences, leveraging the synergistic nature of our\\nmultisensory inputs to form a comprehensive understanding of\\nthe world [276, 26]. Coupled with a user-friendly interface,\\nMLLMs can offer intuitive, flexible, and adaptable interactions,\\nallowing users to engage with intelligent assistants through a\\nspectrum of input methods. According to the ways of construct-\\ning models, current MLLMs can be generally divided into three\\nstreams: pre-training, fine-tuning, and prompting. In this sec-\\ntion, we will discuss more details of these main streams, as well\\nas the important application of MLLMs in visual reasoning.\\nPre-training: This stream of MLLMs intends to support differ-\\nent modalities using unified end-to-end models. For instance,\\nFlamingo [269] applies gated cross-attention to fuse vision and\\nlanguage modalities, which are collected from pre-trained and\\nfrozen visual encoder and LLM, respectively. Moreover, BLIP-\\n2 [270] proposes a two-stage strategy to pre-train a Querying\\nTransformer (Q-Former) for the alignment between vision and\\nlanguage modalities: in the first stage, vision-language repre-\\nsentation learning is bootstrapped from a frozen visual encoder;\\nand in the second stage, a frozen LLM bootstraps vision-to-\\nlanguage generative learning for zero-shot image-to-text gen-\\neration. Similarly, MiniGPT-4 [277] deploys pre-trained and\\nfrozen ViT [278], Q-Former and Vicuna LLM [159], only train-\\ning the linear projection layer for vision and language modali-\\nties alignment.\\nFine-tuning: Derived from instruction tuning [16] for NLP\\ntasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\\nusing multimodal instructions. Following this method, LLMs\\ncan be easily and effectively extended as multimodal chat-\\nbots [277, 271, 29] and multimodal task solvers [279, 30, 280].\\nThe key issue of this stream of MLLMs is to collect multi-\\nmodal instruction-following data for fine-tuning [58]. To ad-\\ndress this issue, the solutions of benchmark adaptation [279,\\n281, 282], self-instruction [19, 31, 283], and hybrid composi-\\ntion [284, 280] are employed, respectively. To mitigate the gap\\nbetween the original language modality and additional modal-\\nities, the learnable interface is introduced to connect differ-\\nent modalities from frozen pre-trained models.\\nParticularly,\\nthe learnable interface is expected to work in a parameter-\\nefficient tuning manner: e.g., LLaMA-Adapter [285] applies\\nan efficient transformer-based adapter module for training,\\nand LaVIN [284] dynamically learns the multimodal feature\\nweights using a mixture-of-modality adapter. Different from\\nthe learnable interface, the expert models can directly convert\\nmultimodalities into language: e.g., VideoChat-Text [272] in-\\ncorporates Whisper [286], a speech recognition expert model,\\nto generate the captions of given videos for the understanding\\nof following LLMs.\\nPrompting:\\nDifferent from the fine-tuning technique that\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='directly updates the model parameters given task-specific\\ndatasets, the prompting technique provides certain context, ex-\\namples, or instructions to the model, fulfilling specialized tasks\\nwithout changing the model parameters. Since prompting can\\nsignificantly reduce the need for large-scale multimodal data,\\nthis technique is widely used to construct MLLMs. Particularly,\\nto solve multimodal Chain of Thought (CoT) problems [103],\\nLLMs are prompted to generate both the reasoning process and\\nthe answer given multimodal inputs [287]. On this front, differ-\\nent learning paradigms are exploited in practice: for example,\\nMultimodal-CoT [287] involves two stages of rationale genera-\\ntion and answer inference, where the input of the second stage\\nis a combination of the original input and the output of the first\\nstage; and CoT-PT [288] applies both prompt tuning and spe-\\ncific visual bias to generate a chain of reasoning implicitly. In\\naddition to CoT problems, LLMs can also be prompted with\\nmultimodal descriptions and tools, effectively dividing complex\\ntasks into sub-tasks [289, 290].\\nVisual Reasoning Application: Recent visual reasoning sys-\\ntems [291, 292, 216, 293] tend to apply LLMs for better visual\\ninformation analysis and visual-language integration. Differ-\\nent from previous works [294, 295] that rely on limited VQA\\ndatasets and small-scale neural networks, current LLM-aided\\nmethods offer benefits of stronger generalization ability, emer-\\ngent ability, and interactivity [58]. To realize visual reasoning\\nwith the help of LLMs, prompting and fine-tuning techniques\\ncan also be utilized: for example, PointClip V2 [292] applies\\nLLMs to generate 3D-specific prompts, which are encoded as\\ntextual features and then combined with visual features for\\n3D recognition; and GPT4Tools [31] employs LoRA [250] to\\nfine-tune LLMs following tool-related instructions.\\nServing\\nas a controller [293], decision maker [296], or semantics re-\\nfiner [291, 297], LLMs significantly facilitates the progress of\\nvisual reasoning research.\\n3.8. Summary and Discussion\\n3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-\\ntecture and training strategies have a big impact on performance\\nand stability. Here, we summarize key architectural modules\\nused in various LLMs, leading to better performance, reduced\\ntraining time and memory, and better training stability.\\nLayer Normalization: The performance and training stability\\nof LLMs are affected significantly by layer normalization. Pre-\\nnorm, that is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [6, 127, 108].\\nBLOOM [13] and AlexaTM [122] utilize an additional layer\\nnormalization before embedding layer to stabilize the training\\nof large-scale models, while the model’s zero-shot generaliza-\\ntion ability can be negatively impacted [13]. However, another\\nstudy [33] finds that pre-norm degrades fine-tuned model per-\\nformance as compared to post-norm, and there are no stability\\nbenefits of pre-norm beyond the 100B scale. Therefore, GLM-\\n130B [33] used deep-norm which is a variant of post-norm for\\nbetter downstream task performance after fine-tuning.\\nPositional Encoding: Like other building blocks of the model,\\npositional encoding also affects the performance and training\\nstability of LLMs.\\nBLOOM [13] finds ALiBi outperforms\\nlearned and rotary positional encodings.\\nContrary to this,\\nGLM-130B [33] identifies rotary positional encoding as being\\nbetter than ALiBi. So, there is no conclusion in the literature\\nabout positional encodings yet.\\nParallel Attention: In this type of attention, feed-forward and\\nattention layers are parallel to each other rather than sequen-\\ntial in a transformer block. It has been shown to reduce train-\\ning time by 15%. There is no evidence of performance drop\\ndue to this change in the literature and it is used by the models\\nPaLM [15], GPT-NeoX [118], and CodeGen [140].\\nMulti-Query Attention It has shared key and value attention\\nheads in a transformer block while query attention heads are\\nprojected as usual. This reduces memory usage and speeds up\\nsampling in autoregressive decoding. No performance degrada-\\ntion has been observed with this change and it makes the train-\\ning efficient allowing larger batch sizes. Multi-query attention\\nis used in [15, 142].\\nMixture of Experts: This type of architecture enables eas-\\nily scaling models to trillions of parameters [92, 91]. Only a\\nfew experts are activated during the computation making them\\ncompute-efficient. The performance of MoE models is better\\nthan dense models for the same amount of data and requires less\\ncomputation during fine-tuning to achieve performance similar\\nto dense models as discussed in [91]. MoE architectures are\\nless prone to catastrophic forgetting, therefore are more suited\\nfor continual learning [92]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [92].\\nSparse vs Dense Activated: GPT-3 [6] uses sparse transform-\\ners [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\\narchitectures to lower computational costs and increase the\\nmodel size and capacity. According to the literature, sparse\\nmodules do not degrade the model’s performance [67]. How-\\never, more experiments are required to verify this statement.\\n3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-\\ning costs, avoid loss divergence, and achieve better perfor-\\nmance. We summarize and discuss some of these key tricks\\nused in different LLMs.\\nMixed Precision: It is a famous method for LLMs to reduce\\nmemory usage and improve training efficiency. In mixed pre-\\ncision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept\\nin FP32 format [120]. A drawback associated with this for-\\nmat change is training instability due to a smaller value range\\nresulting in loss spikes [33]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs precision-\\nsensitive operations like gradient accumulation and softmax in\\nFP32 [13]. BF16 has better performance and training stability\\nbut uses more memory and is supported on specific hardware,\\nfor example, A100 GPUs. Therefore, its adoption in LLMs is\\nlimited.\\nTraining Instability: Loss divergence or spiking is a common\\nissue in LLMs that occurs multiple times during training. This\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='happens in the presence of gradient clipping [15]. To mitigate\\nthis problem, many approaches suggest restarting training from\\nan earlier checkpoint [15, 33, 91], skipping 200-500 earlier\\ndata batches at the point of divergence in [15] and re-shuffling\\nbatches in [91]. The embedding layer gradient shrink proves to\\nfurther stabilize the training as its gradient norm is significantly\\nlarger than the other layers [33]. Another suggestion to improve\\ntraining stability for larger models is not to use biases in dense\\nand norm layers as in [15].\\nWeight Initialization: It plays a significant role in model con-\\nvergence and training stability.\\nGPT-NeoX [118] initializes\\nfeed-forward layers before residuals with\\n2\\nL\\n√\\nd as in [153] and\\nother layers with the small initialization scheme [298]. This\\navoids activations growing exponentially with increasing depth.\\nMT-NLG [117] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [298]. Various models perform random weight initial-\\nization which can cause bad initialization, Galactica [148] sug-\\ngests a longer warmup to negate the effect.\\nLearning Rate: A suitable learning rate is important for sta-\\nble training. It is suggested to use a lower value [13, 15, 124]\\nwith warmup and decay (cosine or linear). Usually, the learn-\\ning rate is within the range 1e−4 to 8e−4. Moreover, MT-NLG\\n(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\\ning learning rates based on the model size using the GPT-3 [6]\\nmodels ranging between 13B and 175B. This avoids tuning the\\nlearning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,\\npipeline, and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\\nIn addition to 3D parallelism, BLOOM [13] uses a zero op-\\ntimizer [37] to shard optimizer states.\\nPanGu-α [108] and\\nPanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\\nlelism which additionally contains optimizer parallelism and\\nrematerialization.\\nMode Switching: It adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve downstream task performance\\nin [125, 124, 122]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation: Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-3 [6]\\nand other LLMs use in-context learning to control generated\\ntext. While in-context learning helps in controlling the gener-\\nated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\\nto rank its generated text for credibility and soft prompts such as\\ngenre, topic, keywords, sentiment, and length for better control\\non generated text.\\n3.8.3. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing di-\\nverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks by\\na large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance difference between zero-shot and few-shot is\\nlarge for pre-trained models [6, 15], naming LLMs as meta-\\nlearners [6]. LLMs zero-shot evaluations underperform unsu-\\npervised methods in neural machine translation [6]. The liter-\\nature shows pre-training is not enough for good zero-shot per-\\nformance [15, 16]. To improve the zero-shot performance the\\nliterature suggests using instruction fine-tuning that improves\\nthe zero-shot performance significantly and outperforms base-\\nlines. Instruction fine-tuning has also been shown to improve\\nzero-shot generalization to unseen tasks. Another model, Flan-\\nPaLM [16], unlocks zero-shot reasoning with CoT training.\\n3.8.5. Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for different\\ntasks, for example, encoder-only for NLU tasks, decoder-only\\nfor NLG, and encoder-decoder for sequence2sequence model-\\ning. Encoder-only models are famous for smaller models such\\nas Bert [7], RoBERTa [299], etc., whereas LLMs are either\\ndecoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\\nWhile decoder-only models are good at NLG tasks, various\\nLLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\\nLLaMA [156], are decoder-only models with significant per-\\nformance gains on both NLU and NLG tasks. In contradic-\\ntion to this, T5 [10] and UL2 [125] identify encoder-decoder\\nmodels out-performing decoder-only models. In another study,\\nPaLM [15] finds increasing the size of decoder-only models\\ncan reduce the performance gap between decoder-only and\\nencoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [125, 122] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5+ [34]\\nuses an encoder-decoder architecture with multiple training ob-\\njectives for different tasks, activating the encoder, decoder, or\\nboth according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\n4. Model Configurations\\nWe provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,\\nparallelism, etc in Table 3 and Table 4. Architecture details\\nof pre-trained LLMs are available in Table 5. Providing these\\ndetails for instruction-tuned models is unnecessary because it\\nfine-tunes pre-trained models for instruction datasets. Hence,\\narchitectural details are the same as the baselines. Moreover,\\noptimization settings for various LLMs are available in Table 6\\nand Table 7. We do not include details on precision, warmup,\\nand weight decay in Table 7. These details are not as important\\nas others to mention for instruction-tuned models, and are not\\nprovided by the papers.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Table 3: Summary of pre-trained LLMs (>10B). Only the LLMs discussed individually in the previous sections are summarized. “Data/Tokens” is the model’s\\npre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\\n(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs\\nhourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\\nre-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\\n(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,\\n“DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\\nModels\\nPublication\\nVenue\\nLicense\\nType\\nModel\\nCreators Purpose\\nNo. of\\nParams\\nCommercial\\nUse\\nSteps\\nTrained\\nData/\\nTokens\\nData\\nCleaning\\nNo. of\\nProcessing Units\\nProcessing\\nUnit Type\\nTraining\\nTime\\nCalculated\\nTrain. Cost\\nTraining\\nParallelism\\nLibrary\\nT5 [10]\\nJMLR'20\\nApache-2.0\\nGoogle\\nGeneral\\n11B\\n✓\\n1M\\n1T\\nHeur+Dedup\\n1024\\nTPU v3\\n-\\n-\\nD+M\\nMesh TensorFlow\\nGPT-3 [6]\\nNeurIPS'20\\n-\\nOpenAI\\nGeneral\\n175B\\n×\\n-\\n300B\\nDedup+QF\\n-\\nV100\\n-\\n-\\nM\\n-\\nmT5 [11]\\nNAACL'21\\nApache-2.0\\nGoogle\\nGeneral\\n13B\\n✓\\n1M\\n1T\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nPanGu-α [108]\\narXiv'21\\nApache-2.0\\nHuawei\\nGeneral\\n200B\\n✓\\n260k\\n1.1TB\\nHeur+Dedup\\n2048\\nAscend 910\\n-\\n-\\nD+OP+P+O+R\\nMindSpore\\nCPM-2 [12]\\nAI Open'21\\nMIT\\nTsinghua\\nGeneral\\n198B\\n✓\\n1M\\n2.6TB\\nDedup\\n-\\n-\\n-\\n-\\nD+M\\nJAXFormer\\nCodex [141]\\narXiv'21\\n-\\nOpenAI\\nCoding\\n12B\\n×\\n-\\n100B\\nHeur\\n-\\n-\\n-\\n-\\n-\\n-\\nERNIE 3.0 [110]\\narXiv'21\\n-\\nBaidu\\nGeneral\\n10B\\n×\\n120k∗\\n375B\\nHeur+Dedup\\n384\\nV100\\n-\\n-\\nM∗\\nPaddlePaddle\\nJurassic-1 [112]\\nWhite-Paper'21 Apache-2.0\\nAI21\\nGeneral\\n178B\\n✓\\n-\\n300B\\n-\\n800\\nGPU\\n-\\n-\\nD+M+P\\nMegatron+DS\\nHyperCLOVA [114]\\nEMNLP'21\\n-\\nNaver\\nGeneral\\n82B\\n×\\n-\\n300B\\nClf+Dedup+PF\\n1024\\nA100\\n321h\\n1.32 Mil\\nM\\nMegatron\\nYuan 1.0 [115]\\narXiv'21\\nApache-2.0\\n-\\nGeneral\\n245B\\n✓\\n26k∗\\n180B Heur+Clf+Dedup\\n2128\\nGPU\\n-\\n-\\nD+T+P\\n-\\nGopher [116]\\narXiv'21\\n-\\nGoogle\\nGeneral\\n280B\\n×\\n-\\n300B\\nQF+Dedup\\n4096\\nTPU v3\\n920h\\n13.19 Mil\\nD+M\\nJAX+Haiku\\nERNIE 3.0 Titan [35]\\narXiv'21\\n-\\nBaidu\\nGeneral\\n260B\\n×\\n-\\n300B\\nHeur+Dedup\\n-\\nAscend 910\\n-\\n-\\nD+M+P+D*\\nPaddlePaddle\\nGPT-NeoX-20B [118] BigScience'22\\nApache-2.0\\nEleutherAI\\nGeneral\\n20B\\n✓\\n150k\\n825GB\\nNone\\n96\\n40G A100\\n-\\n-\\nM\\nMegatron+DS+PyTorch\\nOPT [14]\\narXiv'22\\nMIT\\nMeta\\nGeneral\\n175B\\n✓\\n150k\\n180B\\nDedup\\n992\\n80G A100\\n-\\n-\\nD+T\\nMegatron\\nBLOOM [13]\\narXiv'22\\nRAIL-1.0\\nBigScience\\nGeneral\\n176B\\n✓\\n-\\n366B\\nDedup+PR\\n384\\n80G A100\\n2520h\\n3.87 Mil\\nD+T+P\\nMegatron+DS\\nGalactica [148]\\narXiv'22\\nApache-2.0\\nMeta\\nScience\\n120B\\n×\\n225k\\n106B\\nDedup\\n128\\n80GB A100\\n-\\n-\\n-\\nMetaseq\\nGLaM [91]\\nICML'22\\n-\\nGoogle\\nGeneral\\n1.2T\\n×\\n600k∗\\n600B\\nClf\\n1024\\nTPU v4\\n-\\n-\\nM\\nGSPMD\\nLaMDA [150]\\narXiv'22\\n-\\nGoogle\\nDialog\\n137B\\n×\\n3M\\n2.81T\\nFiltered\\n1024\\nTPU v3\\n1384h\\n4.96 Mil\\nD+M\\nLingvo\\nMT-NLG [117]\\narXiv'22\\nApache-v2.0 MS.+Nvidia General\\n530B\\n×\\n-\\n270B\\n-\\n4480\\n80G A100\\n-\\n-\\nD+T+P\\nMegatron+DS\\nAlphaCode [142]\\nScience'22\\nApache-v2.0\\nGoogle\\nCoding\\n41B\\n✓\\n205k\\n967B\\nHeur+Dedup\\n-\\nTPU v4\\n-\\n-\\nM\\nJAX+Haiku\\nChinchilla [96]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n70B\\n×\\n-\\n1.4T\\nQF+Dedup\\n-\\nTPUv4\\n-\\n-\\n-\\nJAX+Haiku\\nPaLM [15]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n255k\\n780B\\nHeur\\n6144\\nTPU v4\\n-\\n-\\nD+M\\nJAX+T5X\\nAlexaTM [122]\\narXiv'22\\nApache v2.0\\nAmazon\\nGeneral\\n20B\\n×\\n500k\\n1.1T\\nFiltered\\n128\\nA100\\n2880h\\n1.47 Mil\\nM\\nDS\\nU-PaLM [124]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n20k\\n-\\n-\\n512\\nTPU v4\\n120h\\n0.25 Mil\\n-\\n-\\nUL2 [125]\\nICLR'23\\nApache-2.0\\nGoogle\\nGeneral\\n20B\\n✓\\n2M\\n1T\\n-\\n512\\nTPU v4\\n-\\n-\\nM\\nJAX+T5X\\nGLM [33]\\nICLR'23\\nApache-2.0\\nMultiple\\nGeneral\\n130B\\n×\\n-\\n400B\\n-\\n768\\n40G A100\\n1440h\\n3.37 Mil\\nM\\n-\\nCodeGen [140]\\nICLR'23\\nApache-2.0\\nSalesforce\\nCoding\\n16B\\n✓\\n650k\\n577B\\nHeur+Dedup\\n-\\nTPU v4\\n-\\n-\\nD+M\\nJAXFormer\\nLLaMA [127]\\narXiv'23\\n-\\nMeta\\nGeneral\\n65B\\n×\\n350k\\n1.4T Clf+Heur+Dedup\\n2048\\n80G A100\\n504h\\n4.12 Mil\\nD+M\\nxFormers\\nPanGuΣ [92]\\narXiv'23\\n-\\nHuawei\\nGeneral 1.085T\\n×\\n-\\n329B\\n-\\n512\\nAscend 910\\n2400h\\n-\\nD+OP+P+O+R\\nMindSpore\\nBloombergGPT [151]\\narXiv23\\n-\\nBloomberg\\nFinance\\n50B\\n×\\n139k\\n569B\\nDedup\\n512\\n40G A100\\n1272h\\n1.97 Mil\\nM\\nPyTorch\\nXuan Yuan 2.0 [152]\\narXiv23\\nRAIL-1.0\\nDu Xiaoman Finance\\n176B\\n✓\\n-\\n366B\\nFiltered\\n-\\n80GB A100\\n-\\n-\\nP\\nDS\\nCodeT5+ [34]\\narXiv'23\\nBSD-3\\nSalesforce\\nCoding\\n16B\\n✓\\n110k\\n51.5B\\nDedup\\n16\\n40G A100\\n-\\n-\\n-\\nDS\\nStarCoder [147]\\narXiv'23\\nOpenRAIL-M BigCode\\nCoding\\n15.5B\\n✓\\n250k\\n1T\\nDedup+QF+PF\\n512\\n80G A100\\n624h\\n1.28 Mil\\nD+T+P\\nMegatron-LM\\nLLaMA-2 [21]\\narXiv'23\\nLLaMA-2.0\\nMeta\\nGeneral\\n70B\\n✓\\n500k\\n2T\\nMinimal Filtering\\n-\\n80G A100\\n1.7Mh\\n-\\n-\\n-\\nPaLM-2 [123]\\narXiv'23\\n-\\nGoogle\\nGeneral\\n-\\n×\\n-\\n-\\nDdedup+PF+QF\\n-\\n-\\n-\\n-\\n-\\n-\\nLLaMA-3.1 [130]\\narXiv'24\\nLLaMA-3.0\\nMeta\\nGeneral\\n405B\\n✓\\n1.2M\\n15T\\nDedup+QF\\n16k\\n80G H100 30.84Mh\\n-\\nD+T+P+C\\nPyTorch\\nMixtral 8x22B [131]\\nweb'24\\nApache-2.0\\nMistral AI\\nGeneral\\n141B\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSnowflake Arctic [132] web'24\\nApache-2.0\\nSnowflake\\nGeneral\\n480B\\n✓\\n-\\n3.5T\\n-\\n-\\n-\\n-\\nT+P\\nDS\\nNemotron-4 340B [137]web'24\\nNvidia\\nNvidia\\nGeneral\\n340B\\n✓\\n-\\n9T\\n-\\n6144\\n80G H100\\n-\\n-\\nD+T+P\\n-\\nDeepSeek [138]\\narXiv'24\\nMIT\\nDeepSeek\\nGeneral\\n67B\\n✓\\n-\\n2T\\nDedup+QF\\n-\\n-\\n300.6Kh\\n-\\nD+T+P\\nDS\\nDeepSeek-v2 [139]\\narXiv'24\\nMIT\\nDeepSeek\\nGeneral\\n67B\\n✓\\n-\\n8.1T\\nQF\\n-\\nH800\\n172.8Kh\\n-\\nD+P\\nHAI-LLM\\nTable 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number\\nof training samples.\\nModels\\nPublication\\nVenue\\nLicense\\nType\\nModel\\nCreators\\nPurpose\\nNo. of\\nParams\\nCommercial\\nUse\\nPre-trained\\nModels\\nSteps\\nTrained\\nData/\\nTokens\\nNo. of\\nProcessing Units\\nProcessing\\nUnit Type\\nTrain.\\nTime\\nCalculated\\nTrain. Cost\\nTrain.\\nParallelism\\nLibrary\\nWebGPT [166]\\narXiv'21\\n-\\nOpenAI\\nGeneral\\n175B\\n×\\nGPT-3\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nT0 [17]\\nICLR'22\\nApache-2.0\\nBigScience\\nGeneral\\n11B\\n✓\\nT5\\n-\\n250B\\n512\\nTPU v3\\n270h\\n0.48 Mil\\n-\\n-\\nTk-Instruct [18]\\nEMNLP'22\\nMIT\\nAI2+\\nGeneral\\n11B\\n✓\\nT5\\n1000\\n-\\n256\\nTPU v3\\n4h\\n0.0036 Mil\\n-\\nGoogle T5\\nOPT-IML [97]\\narXiv'22\\n-\\nMeta\\nGeneral\\n175B\\n×\\nOPT\\n8k\\n2B\\n128\\n40G A100\\n-\\n-\\nD+T\\nMegatron\\nFlan-U-PaLM [16] ICLR'22\\nApache-2.0\\nGoogle\\nGeneral\\n540B\\n✓\\nU-PaLM\\n30k\\n-\\n512\\nTPU v4\\n-\\n-\\n-\\nJAX+T5X\\nmT0 [154]\\nACL'23\\nApache-2.0 HuggingFace+ General\\n13B\\n✓\\nmT5\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSparrow [167]\\narXiv'22\\n-\\nGoogle\\nDialog\\n70B\\n×\\nChinchilla\\n-\\n-\\n64\\nTPU v3\\n-\\n-\\nM\\n-\\nWizardCoder [164] arXiv'23\\nApache-2.0\\nHK Bapt.\\nCoding\\n15B\\n×\\nStarCoder\\n200\\nS-78k\\n-\\n-\\n-\\n-\\n-\\n-\\nAlpaca [158]\\nGithub'23\\nApache-2.0\\nStanford\\nGeneral\\n13B\\n✓\\nLLaMA\\n3-Epoch\\nS-52k\\n8\\n80G A100\\n3h\\n600\\nFSDP\\nPyTorch\\nVicuna [159]\\nGithub'23\\nApache-2.0\\nLMSYS\\nGeneral\\n13B\\n✓\\nLLaMA\\n3-Epoch S-125k\\n-\\n-\\n-\\n-\\nFSDP\\nPyTorch\\nLIMA [185]\\narXiv'23\\n-\\nMeta+\\nGeneral\\n65B\\n-\\nLLaMA\\n15-Epoch S-1000\\n-\\n-\\n-\\n-\\n-\\n-\\nKoala [300]\\nGithub'23\\nApache-2.0\\nUC-Berkley\\nGeneral\\n13B\\n×\\nLLaMA\\n2-Epoch S-472k\\n8\\nA100\\n6h\\n100\\n-\\nJAX/FLAX\\n5. Datasets and Evaluation\\nGenerating training and evaluation datasets is expensive be-\\ncause of the large-scale data demand of LLMs. Hence, datasets\\nfor training and benchmarking these models are topics of key\\nimportance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\\nThe performance of LLMs largely depends on the training\\ndata’s quality, size, and diversity. Preparing training datasets\\nof high quality at a large scale is laborious. Researchers have\\nsuggested various pre-training and fine-tuning datasets to en-\\nhance LLMs capabilities. We summarize these efforts in Ta-\\nble 8. While numerous training datasets are available in the\\nliterature, we cover the most widely used ones in our summary.\\n25\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\\nsize of hidden states.\\nModels\\nType\\nTraining\\nObjective\\nAttention\\nVocab\\nTokenizer\\nNorm\\nPE\\nActivation\\nBias\\nnL\\nnH\\nHS\\nT5 (11B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n×\\n24\\n128\\n1024\\nGPT3 (175B)\\nCausal-Dec\\nNext Token\\nDense+Sparse\\n-\\n-\\nLayer\\nLearned\\nGeLU\\n✓\\n96\\n96\\n12288\\nmT5 (13B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n-\\n-\\n-\\n-\\nPanGu-α (200B)\\nCausal-Dec\\nNext Token\\nStandard\\n40k\\nBPE\\nLayer\\n-\\n-\\n-\\n64\\n128\\n16384\\nCPM-2 (198B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n-\\n24\\n64\\n-\\nCodex (12B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE+\\nPre-Layer\\nLearned\\nGeLU\\n-\\n96\\n96\\n12288\\nERNIE 3.0 (10B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n64\\n4096\\nJurassic-1 (178B)\\nCausal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece∗\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n76\\n96\\n13824\\nHyperCLOVA (82B)\\nCausal-Dec\\nNext Token\\nDense+Sparse\\n-\\nBPE*\\nPre-Layer\\nLearned\\nGeLU\\n-\\n64\\n80\\n10240\\nYuan 1.0 (245B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\n-\\n-\\n-\\n-\\n-\\n76\\n-\\n16384\\nGopher (280B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n128\\n16384\\nERNIE 3.0 Titan (260B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n192\\n12288\\nGPT-NeoX-20B\\nCausal-Dec\\nNext Token\\nParallel\\n50k\\nBPE\\nLayer\\nRotary\\nGeLU\\n✓\\n44\\n64\\n-\\nOPT (175B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE\\n-\\n-\\nReLU\\n✓\\n96\\n96\\n-\\nBLOOM (176B)\\nCausal-Dec\\nNext Token\\nStandard\\n250k\\nBPE\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n112\\n14336\\nGalactica (120B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE+custom\\nLayer\\nLearned\\nGeLU\\n×\\n96\\n80\\n10240\\nGLaM (1.2T)\\nMoE-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\nLayer\\nRelative\\nGeLU\\n✓\\n64\\n128\\n32768\\nLaMDA (137B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nLayer\\nRelative\\nGeGLU\\n-\\n64\\n128\\n8192\\nMT-NLG (530B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n105\\n128\\n20480\\nAlphaCode (41B)\\nEnc-Dec\\nNext Token\\nMulti-query\\n8k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n128\\n6144\\nChinchilla (70B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n64\\n8192\\nPaLM (540B)\\nCausal-Dec\\nNext Token\\nParallel+Multi-query\\n256k\\nSentencePiece\\nLayer\\nRoPE\\nSwiGLU\\n×\\n118\\n48\\n18432\\nAlexaTM (20B)\\nEnc-Dec\\nDenoising\\nStandard\\n150k\\nSentencePiece\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n78\\n32\\n4096\\nSparrow (70B)\\nCausal-Dec\\nPref.&Rule RM\\n-\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n16∗\\n64\\n8192\\nU-PaLM (540B)\\nNon-Causal-Dec\\nMoD\\nParallel+Multi-query\\n256k\\nSentencePiece\\nLayer\\nRoPE\\nSwiGLU\\n×\\n118\\n48\\n18432\\nUL2 (20B)\\nEnc-Dec\\nMoD\\nStandard\\n32k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n16\\n4096\\nGLM (130B)\\nNon-Causal-Dec\\nAR Blank Infilling\\nStandard\\n130k\\nSentencePiece\\nDeep\\nRoPE\\nGeGLU\\n✓\\n70\\n96\\n12288\\nCodeGen (16B)\\nCausal-Dec\\nNext Token\\nParallel\\n-\\nBPE\\nLayer\\nRoPE\\n-\\n-\\n34\\n24\\n-\\nLLaMA (65B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n80\\n64\\n8192\\nPanGu-Σ (1085B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE\\nFused Layer\\n-\\nFastGeLU\\n-\\n40\\n40\\n5120\\nBloombergGPT (50B)\\nCausal-Dec\\nNext Token\\nStandard\\n131k\\nUnigram\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n40\\n7680\\nXuan Yuan 2.0 (176B)\\nCausal-Dec\\nNext Token\\nSelf\\n250k\\nBPE\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n112\\n14336\\nCodeT5+ (16B)\\nEnc-Dec\\nSC+NT+Cont.+Match\\nStandard\\n-\\nCode-Specific\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nStarCoder (15.5B)\\nCausal-Dec\\nFIM\\nMulti-query\\n49k\\nBPE\\n-\\nLearned\\n-\\n-\\n40\\n48\\n6144\\nLLaMA-2 (70B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n32k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLUE\\n-\\n-\\n-\\n-\\nPaLM-2\\n-\\nMoD\\nParallel\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nLLaMA-3.1 (405B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n128k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n126\\n128\\n16384\\nNemotron-4 (340B)\\nCausal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\n-\\nRoPE\\nReLU\\n×\\n96\\n96\\n18432\\nDeepSeek (67B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n100k\\nBBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n95\\n64\\n8192\\nDeepSeek-v2 (67B)\\nMoE-Dec\\nNext Token\\nMulti-Head Latent\\n100k\\nBBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n60\\n128\\n5120\\n5.2. Evaluation Datasets and Tasks\\nThe evaluation of LLMs is important in gauging their profi-\\nciency and limitations. This process measures the model’s abil-\\nity to comprehend, generate, and interact with human language\\nacross a spectrum of tasks. Evaluating a language model (LM)\\nis divided into two broader categories: 1) natural language un-\\nderstanding (NLU) and 2) natural language generation (NLG).\\nIt is emphasized that tasks in NLU and NLG are softly catego-\\nrized and are often used interchangeably in the literature.\\nNatural Language Understanding: It measures the language\\nunderstanding capacity of LMs. It encompasses multiple tasks,\\nincluding sentiment analysis, text classification, natural lan-\\nguage inference (NLI), question answering (QA), common-\\nsense reasoning (CR), mathematical reasoning (MR), reading\\ncomprehension (RC), etc.\\nNatural Language Generation: It assesses the language gener-\\nation capabilities of LLMs by understanding the provided input\\ncontext. It includes tasks such as summarization, sentence com-\\npletion, machine translation (MT), dialogue generation, etc.\\nNumerous datasets are proposed for each task, evaluating\\nLLMs against different characteristics. To provide an overview\\nof evaluation datasets, we briefly discuss a few famous datasets\\nwithin each category and offer a comprehensive list of datasets\\nin Table 9. Moreover, we show a detailed overview of the train-\\ning datasets and evaluation tasks and benchmarks used by vari-\\nous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\\nble 11. We also compare the top-performing LLMs in various\\nNLP tasks in Table 12.\\n5.2.1. Multi-task\\nMMLU [307]: A benchmark that measures the knowledge\\nacquired by models during pretraining and evaluates models in\\nzero-shot and few-shot settings across 57 subjects, testing both\\nworld knowledge and problem-solving ability.\\nSuperGLUE [2]: A more challenging and diverse successor\\nto the GLUE [309] benchmark, SuperGLUE includes a variety\\nof language understanding tasks, such as question answering,\\nnatural language inference, and co-reference resolution. It is\\ndesigned to provide a rigorous test of language understanding\\nand requires significant progress in areas like sample-efficient,\\ntransfer, multi-task, and unsupervised or self-supervised learn-\\ning.\\nBIG-bench [308]: The BIG-bench (Behavior of Intelligent\\nGenerative Models Benchmark) is a large-scale benchmark de-\\nsigned to test the abilities of LLMs across a wide range of\\ntasks, including reasoning, creativity, ethics, and understanding\\nof specific domains.\\nGLUE [309]: The General Language Understanding Evalua-\\ntion (GLUE) benchmark is a collection of resources for train-\\ning, evaluating, and analyzing natural language understanding\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\\nfor most of the LLMs.\\nSequence\\nLR\\nOptimizers\\nPrecision\\nWeight\\nGrad\\nModels\\nBatch Size\\nLength\\nLR\\nWarmup\\nDecay\\nAdaFactorAdam AdamWFP16 BF16 Mixed Decay\\nClip\\nDropout\\nT5 (11B)\\n211\\n512\\n0.01\\n×\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nGPT3 (175B)\\n32K\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nmT5 (13B)\\n1024\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nPanGu-α (200B)\\n-\\n1024\\n2e-5\\n-\\n-\\n-\\n-\\n-\\n-\\n✓\\n-\\n-\\n-\\n-\\nCPM-2 (198B)\\n1024\\n1024\\n0.001\\n-\\n-\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nCodex (12B)\\n-\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n-\\n-\\nERNIE 3.0 (12B)\\n6144\\n512\\n1e-4\\n✓\\nlinear\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nJurassic-1 (178B)\\n3.2M\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nHyperCLOVA (82B)\\n1024\\n-\\n6e-5\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nYuan 1.0 (245B)\\n<10M\\n2048\\n1.6e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nGopher (280B)\\n3M\\n2048\\n4e-5\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n-\\n✓\\n-\\nERNIE 3.0 Titan (260B)\\n-\\n512\\n1e-4\\n✓\\nlinear\\n✓\\n✓\\n✓\\n✓\\n-\\nGPT-NeoX-20B\\n1538\\n2048\\n0.97e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nOPT (175B)\\n2M\\n2048\\n1.2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n✓\\n✓\\nBLOOM (176B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nGalactica (120B)\\n2M\\n2048\\n7e-6\\n✓\\nlinear decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n✓\\nGLaM (1.2T)\\n1M\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\nFP32 + ✓\\n-\\n✓\\n×\\nLaMDA (137B)\\n256K\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMT-NLG (530B)\\n1920\\n2048\\n5e-5\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n✓\\n✓\\n-\\nAlphaCode (41B)\\n2048\\n1536+768\\n1e-4\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n✓\\n✓\\n-\\nChinchilla (70B)\\n1.5M\\n2048\\n1e-4\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n-\\n-\\n-\\nPaLM (540B)\\n2048\\n2048\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n×\\nAlexaTM (20B)\\n2M\\n1024\\n1e-4\\n-\\nlinear decay to 5%\\n✓\\n✓\\n✓\\n-\\n✓\\nU-PaLM (540B)\\n32\\n2048\\n1e-4\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\nUL2 (20B)\\n1024\\n1024\\n-\\n-\\ninverse square root\\n-\\n-\\n-\\n-\\n-\\n-\\n×\\n-\\n-\\nGLM (130B)\\n4224\\n2048\\n8e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeGen (16B)\\n2M\\n2048\\n5e-5\\n✓\\ncosine\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nLLaMA (65B)\\n4M Tokens\\n2048\\n1.5e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nPanGu-Σ (1.085T)\\n512\\n1024\\n2e-5\\n✓\\n-\\n✓\\n✓\\n-\\n-\\n-\\nBloombergGPT (50B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nXuan Yuan 2.0 (176B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nCodeT5+ (16B)\\n2048\\n1024\\n2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n-\\n-\\nStarCoder (15.5B)\\n512\\n8k\\n3e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n-\\n-\\nLLaMA-2 (70B)\\n4M Tokens\\n4k\\n1.5e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nLLaMA-3.1 (405B)\\n16M\\n8192\\n8e-5\\n✓\\nlinear+cosine\\n✓\\n✓\\n-\\n-\\n-\\nNemotron-4 (340B)\\n2304\\n4096\\n-\\n-\\nlinear\\n-\\n-\\n-\\n✓\\n-\\n-\\n×\\nDeepSeek (67B)\\n4608\\n4096\\n3.2e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nDeepSeek-v2 (67B)\\n9216\\n4k\\n2.4e-4\\n✓\\nstep-decay\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\\nno model uses weight decay for instruction tuning.\\nSequence\\nOptimizers\\nGrad\\nModels\\nBatch Size\\nLength\\nLR\\nWarmup\\nLR_Decay\\nAdaFactor\\nAdam\\nAdamW\\nClip\\nDropout\\nWebGPT (175B)\\nBC:512, RM:32\\n-\\n6e-5\\n-\\n-\\n✓\\n-\\n-\\nT0 (11B)\\n1024\\n1280\\n1e-3\\n-\\n-\\n✓\\n-\\n✓\\nTk-Instruct (11B)\\n1024\\n-\\n1e-5\\n-\\nconstant\\n-\\n-\\n-\\n-\\n-\\nOPT-IML (175B)\\n128\\n2048\\n5e-5\\n×\\nlinear\\n✓\\n✓\\n✓\\nFlan-U-PaLM (540B)\\n32\\n-\\n1e-3\\n-\\nconstant\\n✓\\n-\\n✓\\nSparrow (70B)\\nRM: 8+16, RL:16\\n-\\n2e-6\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n×\\nWizardCoder (15B)\\n512\\n2048\\n2e-5\\n✓\\ncosine\\n-\\n-\\n-\\n-\\n-\\nAlpaca (13B)\\n128\\n512\\n1e-5\\n✓\\ncosine\\n-\\n-\\n✓\\n✓\\n×\\nVicuna (13B)\\n128\\n-2048\\n2e-5\\n✓\\ncosine\\n✓\\n-\\n×\\nLIMA (65B)\\n32\\n2048\\n1e-5\\n×\\nlinear\\n✓\\n-\\n✓\\nsystems. It includes a variety of tasks that test a wide range of\\nlinguistic phenomena, making it a comprehensive tool for eval-\\nuating language understanding in AI.\\n5.2.2. Language Understanding\\nWinoGrande [354]: A large-scale dataset inspired by the orig-\\ninal Winograd [357] Schema Challenge tests models on their\\nability to resolve pronoun ambiguity and encourages the devel-\\nopment of models that understand the broad context in natural\\nlanguage text.\\nCoQA [316]: A conversational question-answering dataset,\\nCoQA challenges models with questions that rely on conver-\\nsation history and require free-form text answers. Its diverse\\ncontent from seven domains makes it a rigorous test for mod-\\nels’ ability to handle a wide range of topics and conversational\\ncontexts.\\nWiC [317]: This dataset assesses a model’s ability to dis-\\ncern word meanings based on context, aiding in tasks related\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\nDataset\\nType\\nSize/Samples\\nTasks\\nSource\\nCreation\\nComments\\nC4 [10]\\nPretrain\\n806GB\\n-\\nCommon Crawl\\nAutomated\\nA clean, multilingual dataset with billions\\nof tokens\\nmC4 [11]\\nPretrain\\n38.49TB\\n-\\nCommon Crawl\\nAutomated\\nA multilingual extension of the C4\\ndataset, mC4 identifies over 100 lan-\\nguages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301]\\nPretrain\\n825GB\\n-\\nCommon Crawl, PubMed Central,\\nOpenWebText2, ArXiv, GitHub,\\nBooks3, and others\\nAutomated\\nA massive dataset comprised of 22 con-\\nstituent sub-datasets\\nROOTs [302]\\nPretrain\\n1.61TB\\n-\\n498 Hugging Face datasets\\nAutomated\\n46 natural and 13 programming lan-\\nguages\\nMassiveText [116]\\nPretrain\\n10.5TB\\n-\\nMassiveWeb, Books, News,\\nWikipedia, Github, C4\\nAutomated\\n99% of the data is in English\\nWikipedia [303]\\nPretrain\\n-\\n-\\nWikipedia\\nAutomated\\nDump of wikipedia\\nRedPajama [304]\\nPretrain\\n5TB\\n-\\nCommonCrawl, C4, Wikipedia,\\nGithub, Books, StackExchange\\nAutomated\\nOpen-source replica of LLaMA dataset\\nPushShift.io Reddit\\nPretrain\\n21.1GB\\n-\\nReddit\\nAutomated\\nSubmissions and comments on Reddit\\nfrom 2005 to 2019\\nBigPython [140]\\nPretrain\\n5.5TB\\nCoding\\nGitHub\\nAutomated\\n-\\nPool of Prompt (P3) [17]\\nInstructions\\n12M\\n62\\nPromptSource\\nManual\\nA Subset of PromptSource, created from\\n177 datasets including summarization,\\nQA, classification, etc.\\nxP3 [154]\\nInstructions\\n81M\\n71\\nP3+Multilingual datasets\\nManual\\nExtending P3 to total 46 languages\\nSuper-NaturalInstructions (SNI) [18]\\nInstructions\\n12.4M\\n1616\\nMultiple datasets\\nManual\\nExtending P3 with additional multi-\\nlingual datasets, total 46 languages\\nFlan [16]\\nInstructions\\n15M\\n1836\\nMuffin+T0-SF+NIV2\\nManual\\nTotal 60 languages\\nOPT-IML [97]\\nInstructions\\n18.1M\\n1667\\n-\\nManual\\n-\\nSelf-Instruct [19]\\nInstructions\\n82k\\n175\\n-\\nAutomated\\nGenerated 52k instructions with 82k sam-\\nples from 175 seed tasks using GPT-3\\nAlpaca [158]\\nInstructions\\n52k\\n-\\n-\\nAutomated\\nEmployed self-instruct method to gener-\\nate data from text-davinci-003\\nVicuna [159]\\nInstructions\\n125k\\n-\\nShareGPT\\nAutomated\\nConversations\\nshared\\nby\\nusers\\non\\nShareGPT using public APIs\\nLLaMA-GPT-4 [160]\\nInstructions\\n52k\\n-\\nAlpaca\\nAutomated\\nRecreated Alpaca dataset with GPT-4 in\\nEnglish and Chinese\\nUnnatural Instructions [305]\\nInstructions\\n68k\\n-\\n15-Seeds (SNI)\\nAutomated\\n-\\nLIMA [185]\\nInstructions\\n1k\\n-\\nMultiple datasets\\nManual\\nCarefully created samples to test perfor-\\nmance with fine-tuning on less data\\nAnthropic-HH-RLHF [306]\\nAlignment\\n142k\\n-\\n-\\nManual\\nAnthropic-HH-RLHF-2 [178]\\nAlignment\\n39k\\n-\\n-\\nManual\\nto Word Sense Disambiguation.\\nWikitext103 [318]:\\nWith over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as lan-\\nguage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from\\nProject Gutenberg. It is specifically designed to facilitate re-\\nsearch in unsupervised learning and language modeling, with a\\nspecial focus on long-form content.\\nC4 [10]: A clean, multilingual dataset, C4 offers billions of to-\\nkens from web-crawled data. It is a comprehensive resource for\\ntraining advanced Transformer models on various languages.\\nLCQMC [320]: The Large-scale Chinese Question Matching\\nCorpus (LCQMC) is a dataset for evaluating the performance\\nof models in semantic matching tasks. It contains pairs of ques-\\ntions in Chinese and their matching status, making it a valuable\\nresource for research in Chinese language understanding.\\n5.2.3. Story Cloze and Sentence Completion\\nStoryCloze [334]: It introduces a new “StoryCloze Test”, a\\ncommonsense reasoning framework for evaluating story under-\\nstanding, generation, and script learning. It considers a model’s\\nability to understand and generate coherent and sensible stories.\\nLAMBADA [335]: This dataset evaluates contextual text un-\\nderstanding through a word prediction task. Models must pre-\\ndict the last word of a passage, which is easy for humans when\\ngiven the whole passage, but not when given only the last sen-\\ntence.\\n5.2.4. Physical Knowledge and World Understanding\\nPIQA [340]: A dataset that probes the physical knowledge of\\nmodels, aiming to understand how well they are learning about\\nthe real world.\\nTriviaQA [341]: A dataset that tests models on reading com-\\nprehension and open domain question answering (QA) tasks,\\nwith a focus on Information Retrieval (IR)-style QA.\\nARC [342]: A larger version of the ARC-Challenge, this\\ndataset contains both easy and challenging grade-school level,\\nmultiple-choice science questions. It is a comprehensive test of\\na model’s ability to understand and answer complex questions.\\nARC-Easy [342]:\\nA subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Table 9: Categorized evaluation datasets used in evaluating LLMs.\\nType\\nDatasets/Benchmarks\\nMulti-Task\\nMMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-\\nCLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]\\nLanguage Understanding\\nCoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],\\nCB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\\nCLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]\\nStory Cloze and\\nSentence Completion\\nStoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-\\nFC [312]\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\\nBookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]\\nContextual Language\\nUnderstanding\\nRACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],\\ncMedQA [351],cMedQA2 [352], MATINF-QA [353]\\nCommonsense Reasoning\\nWinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\\nCLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]\\nReading Comprehension\\nSQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],\\nCMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-\\ntiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\\nDuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD\\n1.0 [380], CAIL2018-Task1 & Task2 [381]\\nMathematical Reasoning\\nMATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-\\nDiv [388], MAWPS [389], SVAMP [390]\\nProblem Solving\\nHumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]\\nNatural Language Inference\\n& Logical Reasoning\\nANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],\\nANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-\\ngyQA [349]\\nCross-Lingual Understanding\\nMLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-\\nGoldP [403], MLSum [404]\\nTruthfulness and Fact Checking\\nTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\\nBiases and Ethics in AI\\nETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]\\nToxicity\\nRealToxicityPrompts [413], CivilComments toxicity classification [414]\\nLanguage Translation\\nWMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]\\nScientific Knowledge\\nAminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral\\nGroups [148]\\nDialogue\\nWizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],\\nKdConv [421]\\nTopic Classification\\nTNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]\\nIt is a great starting point for models beginning to explore ad-\\nvanced question-answering.\\nARC-Challenge [342]:\\nA rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval, test-\\ning the true comprehension capabilities of models.\\n5.2.5. Contextual Language Understanding\\nRACE [347]: The RACE dataset is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge\\nof a real-world examination.\\nRACE-Middle [347]: Another subset of the RACE [347]\\ndataset, RACE-Middle, contains middle school-level English\\nexam questions. It offers a slightly less challenging but academ-\\nically oriented evaluation of a model’s comprehension skills.\\nRACE-High [347]: A subset of the RACE [347] dataset,\\nRACE-High consists of high school-level English exam ques-\\ntions. It is designed to evaluate the comprehension ability of\\nmodels in a more academic and challenging context.\\nQuAC [348]: This dataset simulates an information-seeking\\ndialog between students and teachers using hidden Wikipedia\\ntext. It introduces unique challenges not found in machine com-\\nprehension datasets, making it a valuable resource for advanc-\\ning dialog systems.\\n5.2.6. Commonsense Reasoning\\nHellaSwag [355]: A dataset that challenges models to pick the\\nbest ending to a context uses Adversarial Filtering to create a\\n‘Goldilocks’ zone of complexity, where generated text is absurd\\nto humans but often misclassified by models.\\nCOPA [401]: This dataset evaluates a model’s progress in\\nopen-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability to\\nunderstand and reason about cause and effect.\\nWSC [357]: The Winograd Schema Challenge (WSC) is a\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\\nis natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\\n“Mem.” is memorization.\\nBenchmark\\nModels\\nTraining Dataset\\nBIG-\\nbench\\nMMLU\\nSuper\\nGLUE\\nQA\\nClf\\nNLI\\nMT\\nCloze/\\nCompletion\\nRC\\nCR\\nMR\\nCoding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5\\nC4 [10]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGPT-3\\nCommon Crawl, WebText, Books Cor-\\npora, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nmT5\\nmC4 [11]\\n✓\\n✓\\n✓\\nPanGu-α\\n1.1TB Chinese Text Corpus\\n✓\\n✓\\n✓\\n✓\\n✓\\nCPM-2\\nWuDaoCorpus [109]\\n✓\\n✓\\nCodex\\n54 million public repositories from Github\\n✓\\nERNIE-3.0\\nChinese text corpora, Baidu Search, Web\\ntext, QA-long, QA-short, Poetry and Cou-\\nplet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge\\ngraph with more than 50 million facts\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nJurassic-1\\nWikipedia, OWT, Books, C4, Pile [301],\\narXiv, GitHub\\n✓\\n✓\\n✓\\n✓\\nHyperCLOVA\\nKorean blogs, Community sites, News,\\nKiN Korean Wikipedia, Wikipedia (En-\\nglish and Japanese), Modu-Corpus: Mes-\\nsenger, News, Spoken and written lan-\\nguage corpus, Web corpus\\n✓\\nYuan 1.0\\nCommon Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓\\n✓\\n✓\\n✓\\nGopher\\nsubsets of MassiveWeb Books, C4, News,\\nGitHub and Wikipedia samples from Mas-\\nsiveText\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nERNIE-3.0 TITAN\\nSame as ERNIE 3.0 and ERNIE 3.0 ad-\\nversarial dataset, ERNIE 3.0 controllable\\ndataset\\n✓\\n✓\\n✓\\n✓\\n✓\\nGPT-NeoX-20B\\nPile [301]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nOPT\\nRoBERTa [299], Pile [301], PushShift.io\\nReddit [423]\\n✓\\n✓\\n✓\\n✓\\nBLOOM\\nROOTs [13]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGalactica\\narXiv, PMC, Semantic Scholar, Wikipedia,\\nStackExchange, LibreText, Open Text-\\nbooks, RefSeq Genome, OEIS, LIPID\\nMAPS, NASAExoplanet, Common Crawl,\\nScientificCC, AcademicCC, GitHub repos-\\nitories Khan Problems, GSM8K, OneS-\\nmallStep\\n✓\\n✓\\n✓\\n✓\\n✓\\nGLaM\\nFiltered Webpages, Social media conversa-\\ntions Wikipedia, Forums, Books, News\\n✓\\n✓\\n✓\\n✓\\n✓\\nLaMDA\\nInfiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG\\nTwo snapshots of Common Crawl and\\nBooks3, OpenWebText2, Stack Exchange,\\nPubMed Abstracts,\\nWikipedia,\\nPG-19\\n[242], BookCorpus2, NIH ExPorter, Pile,\\nCC-Stories, RealNews\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlphaCode\\nSelected GitHub repositories, CodeCon-\\ntests: Codeforces, Description2Code, Co-\\ndeNet\\n✓\\nChinchilla\\nMassiveWeb,\\nMassiveText Books,\\nC4,\\nNews, GitHub, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM\\nwebpages, books, Wikipedia, news, arti-\\ncles, source code, social media conversa-\\ntions\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlexaTM\\nWikipedia, mC4\\n✓\\n✓\\n✓\\n✓\\n✓\\nU-PaLM\\nSame as PaLM\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nUL2\\n-\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGLM-130B\\n-\\n✓\\n✓\\n✓\\nCodeGen\\nPile, BigQuery, BigPython\\n✓\\nLLaMA\\nCommonCrawl, C4, Github, Wikipedia,\\nBooks, arXiv, StackExchange\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPanGu-Σ\\nWuDaoCorpora, CLUE, Pile, C4, Python\\ncode\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nBloombergGPT\\ninPile, Pile, C4, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeT5+\\nCodeSearchNet, Github Code\\n✓\\n✓\\nStarCoder\\nThe Stack v1.2\\n✓\\n✓\\n✓\\n✓\\nLLaMA-2\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM-2\\nWeb documents, Code, Books, Maths,\\nConversation\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\nModels\\nTraining Dataset\\nBIG-\\nbench\\nMMLU\\nBBH\\nRAFT\\nFLAN\\nSNI\\nPromptSource\\nTyDiQA\\nHumanEval\\nMBPP\\nTruthful/\\nBias/\\nToxicity\\nT0\\nPool of Prompts\\n✓\\nWebGPT\\nELI5\\n[424],\\nELI5\\nfact-\\ncheck\\n[166],\\nTriviaQA\\n[341],\\nARC-Challenge\\n[342],\\nARC-\\nEasy\\n[342],\\nHand-written\\ndata,\\nDemonstrations of humans, Com-\\nparisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCT\\nSNI [18]\\n✓\\nmT0\\nxP3 [154]\\nOPT-IML\\nPromptSource [17], FLAN [16],\\nSNI\\n[425],\\nUnifiedSKG\\n[426],\\nCrossFit\\n[427],\\nExMix\\n[428],\\nT5 [10], Reasoning\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nFlan\\nMuffin, T0-SF, NIv2, CoT\\n✓\\n✓\\n✓\\nWizardCoder\\nCode Alpaca\\n✓\\n✓\\nreading comprehension task in which a system must resolve\\nreferences in a text, often requiring world knowledge and rea-\\nsoning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering\\ndataset that requires commonsense knowledge to evaluate the\\nability of AI models to understand and answer questions.\\n5.2.7. Reading Comprehension\\nBoolQ [363]: A dataset derived from Google search queries,\\nBoolQ challenges models to answer binary (yes/no) questions.\\nThe questions are naturally occurring and are paired with a\\nparagraph from a Wikipedia article containing the answer. It\\nis a test of reading comprehension and reasoning.\\nSQUADv2 [364]: The Stanford Question Answering Dataset\\n(SQuAD) [362] is a collection of questions posed by crowd\\nworkers on a set of Wikipedia articles, where the answer to ev-\\nery question is a segment of text from the corresponding reading\\npassage. SQuADv2 combines the original SQuAD1.1 dataset\\nwith over 50,000 unanswerable questions. The aim is to evalu-\\nate a model’s ability to understand and answer questions based\\non a given context and to determine when a question is unan-\\nswerable.\\nDROP [365]: DROP, or Discrete Reasoning Over the con-\\ntent of Paragraphs, is designed to test a model’s ability to un-\\nderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]:\\nThe Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textual\\nentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\nWebQA [367]: A dataset for open-domain question answering,\\nWebQA offers a large collection of web-based question-answer\\npairs. It is designed to assess the ability of AI models to under-\\nstand and answer questions based on web content.\\nCMRC2018 [369]: This dataset is a test of Chinese language\\nmodels’ ability to reason comprehensively and is designed with\\na challenging span-extraction format that pushes the boundaries\\nof machine performance.\\n5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the\\nmathematical problem-solving abilities of AI models. It con-\\ntains a diverse set of math problems, ranging from arithmetic\\nto calculus, and is designed to test the model’s ability to under-\\nstand and solve complex mathematical problems.\\nMath23k [383]: This one challenges a model’s ability to un-\\nderstand and solve mathematical word problems. It contains\\n23,000 Chinese arithmetic word problems that require models\\nto perform reasoning and computation based on the problem\\ndescription.\\nGSM8K [384]: A dataset of diverse grade school math word\\nproblems, testing a model’s ability to perform multi-step math-\\nematical reasoning.\\n5.2.9. Problem Solving and Logical Reasoning\\nANLI [393]: A large-scale dataset designed to test the robust-\\nness of machine learning models in Natural Language Inference\\n(NLI) is created through an iterative, adversarial process where\\nhumans try to generate examples that models cannot correctly\\nclassify.\\nHumanEval [141]: A dataset for evaluating the problem-\\nsolving ability of AI models, which includes a diverse set of\\ntasks that require various cognitive abilities, making it a com-\\nprehensive tool for assessing general intelligence in AI.\\nStrategyQA [349]: A question-answering dataset that re-\\nquires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the bound-\\naries of what machines can understand and answer.\\n5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the\\nMultiNLI [429] corpus to 15 languages, including low-resource\\nones like Urdu. It tests models on cross-lingual sentence under-\\nstanding, with 112,500 annotated pairs across three categories:\\nentailment, contradiction, and neutral.\\nPAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver-\\nsaries from Word Scrambling, is a multilingual version of the\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='PAWS [430] dataset for paraphrase identification. It includes\\nexamples in seven languages and is designed to evaluate the\\nperformance of cross-lingual paraphrase identification models.\\n5.2.11. Truthfulness\\nTruthful-QA [405]: A unique benchmark that measures a\\nlanguage model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some designed to test the model against com-\\nmon human misconceptions.\\n5.2.12. Biases and Ethics in AI\\nETHOS [408]: ETHOS is a hate speech detection dataset\\nbuilt from YouTube and Reddit comments. It is a tool in the\\nfight against online hate speech, offering binary and multi-label\\nvariants for robust content moderation.\\nStereoSet [409]: StereoSet is a comprehensive dataset de-\\nsigned to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. Contrasting stereotypi-\\ncal bias against language modeling ability provides a valuable\\ntool for understanding and mitigating biases in large language\\nmodels.\\n6. Applications\\nApplying Large Language Models (LLMs) to a variety of\\ndownstream tasks has become a popular trend in both AI-\\nrelated research communities and industries, with many emerg-\\ning uses being discovered and explored daily. LLMs, which are\\ncapable of understanding and generating human-like text, have\\nfound meaningful applications across a variety of fields. This\\nsection provides an overview of LLM applications in medicine,\\neducation, science, mathematics, law, finance, robotics, and\\ncoding. While each of these domains pose different challenges,\\nLLMs open up opportunities to make significant contributions\\nto these domains through their generalizability.\\nGeneral Purpose:\\nLLMs are being widely considered as\\ngeneral-purpose tools for a wide variety of tasks [431]. This\\nis due to their inherent ability to understand, generate, and\\nmanipulate human-like text in a contextually relevant man-\\nner. This allows them to perform tasks ranging from simple\\nlanguage translation and question-answering to more complex\\ntasks like summarization, text generation, and even program-\\nming help [432]. The utility of LLMs is further enhanced by\\ntheir ability to adapt to the specific style and tone of the text\\nthey are processing, making the outputs more user-friendly and\\ncontext-aware. In everyday applications, LLMs can be used\\nas personal assistants, helping users draft emails or schedule\\nappointments [433]; they can also be deployed in customer ser-\\nvice to handle common questions or applied to generate content\\nfor digital platforms like websites by creating human-like text\\nbased on given prompts [434]. Moreover, LLMs play a cru-\\ncial role in data analysis, where they can filter large volumes of\\ntext data, summarize key points, and find patterns that would\\ntake humans much longer to identify [435]. Despite their wide-\\nranging applications, it is essential to remember that LLMs,\\nsimilar to any AI system, are only as good as the data they have\\nbeen trained on.\\nMedicine: The application of LLMs in the field of medicine is\\nreshaping healthcare delivery and research. For example, LLMs\\nare increasingly used in clinical decision support systems to\\nprovide physicians with evidence-based treatment recommen-\\ndations [436, 437, 438]. By analyzing patient data and medical\\nliterature, they can help identify potential diagnoses, suggest\\nappropriate tests, and recommend optimal treatment strategies.\\nMoreover, LLMs can also enhance patient interactions with\\nhealthcare systems; e.g., they can be used in chatbot applica-\\ntions [439, 440, 441] to answer patient queries about symptoms\\nor medications, schedule appointments, and even provide es-\\nsential health advice. For medical research, LLMs are used to\\nextract and filter information from a considerable amount of\\nmedical literature, identify relevant studies, summarize find-\\nings, and even predict future research trends [442, 443, 444].\\nFor medical education, LLMs can help create training mate-\\nrials, generate exam questions, provide detailed explanations\\nof complex medical topics, and offer personalized feedback to\\nstudents [445, 446, 447, 448]. They can also simulate patient\\ninteractions, enabling students to practice and improve their\\nclinical skills. At a broader level, LLMs can assist in public\\nhealth initiatives by analyzing media data to detect disease out-\\nbreaks, monitor public sentiment towards health policies, and\\ndisseminate health information in a clear and understandable\\nmanner [449]. LLMs can be employed to support public health\\ninitiatives, addressing related issues such as data privacy, the\\nnecessity for explainability, and the potential risk of propagat-\\ning biases [450, 451].\\nEducation: The integration of LLMs into the educational sec-\\ntor offers opportunities to enhance learning experiences, teacher\\nsupport, and educational content development. For students, by\\nanalyzing their learning styles, performance, and preferences,\\nLLMs can provide customized study materials and practice\\nquestions to develop personalized learning experiences [452].\\nFor teachers, LLMs can help to create lesson plans and grade\\nassignments and generate diverse and inclusive educational\\ncontent, significantly saving more time for teaching and student\\ninteraction [453, 454]. In language learning, LLMs serve as\\nadvanced conversational partners capable of simulating conver-\\nsations in multiple languages, correcting grammar, enhancing\\nvocabulary, and aiding pronunciation for the needs of fluency\\nin practice [455]. Furthermore, LLMs improve accessibility\\nin education by providing support for students with disabili-\\nties. They can generate real-time transcriptions for the hear-\\ning impaired, offer reading assistance for the visually impaired,\\nand simplify complex texts for those with learning disabili-\\nties [451]. As LLMs continue to evolve, their applications in\\neducation can benefit more students and teachers from different\\nperspectives in practice.\\nScience: Similar to medical applications, LLMs can expedite\\nthe research process by quickly analyzing and summarizing sci-\\nentific literature. By briefing comprehensible and accessible re-\\nsearch summaries, LLMs can assist researchers in staying up-\\nto-date with the latest findings, even in fields outside their area\\nof expertise [456, 457]. In addition, LLMs can aid scientists\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\\nto the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\\nbenchmark.\\nTask\\nDataset/Benchmark\\nTop-1\\nTop-2\\nTop-3\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nMulti-Task\\nBIG-bench (B)\\nChinchilla (70B)\\n65.1 (5-shot)\\nGopher (280B)\\n53.97 (5-shot)\\nPaLM (540B)\\n53.7 (5-shot)\\nMMLU (B)\\nGPT-4 (-)\\n86.4 (5-shot)\\nGemini (Ultra)\\n83.7 (5-shot)\\nFlan-PaLM-2( f) (Large)\\n81.2 (5-shot)\\nLanguage Understanding\\nSuperGLUE (B)\\nERNIE 3.0 (12B)\\n90.6 (-)\\nPaLM(f) (540B)\\n90.4 (-)\\nT5 (11B)\\n88.9 (-)\\nStory Comprehension and\\nGeneration\\nHellaSwag\\nGPT-4 (-)\\n95.3 (10-shot)\\nGemini (Ultra)\\n87.8 (10-shot)\\nPaLM-2 (Large)\\n86.8 (one shot)\\nStoryCloze\\nGPT3 (175B)\\n87.7 (few shot)\\nPaLM-2 (Large)\\n87.4 (one shot)\\nOPT (175B)\\n79.82 (-)\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA\\nPaLM-2 (Large)\\n85.0 (one shot)\\nLLaMa (65B)\\n82.8 (zero shot)\\nMT-NLG (530B)\\n81.99 (zero shot)\\nTriviaQA\\nPaLM-2 (Large)\\n86.1 (one shot)\\nLLaMA-2 (70B)\\n85.0 (one shot)\\nPaLM (540B)\\n81.4 (one shot)\\nContextual Language\\nUnderstanding\\nLAMBADA\\nPaLM (540B)\\n89.7 (few shot)\\nMT-NLG (530B)\\n87.15 (few shot)\\nPaLM-2 (Large)\\n86.9 (one shot)\\nCommonsense Reasoning\\nWinoGrande\\nGPT-4 (-)\\n87.5 (5-shot)\\nPaLM-2 (Large)\\n83.0 (one shot)\\nPaLM (540B)\\n81.1 (zero shot)\\nSIQA\\nLLaMA (65B)\\n52.3 (zero shot)\\nChinchilla (70B)\\n51.3 (zero shot)\\nGopher (280B)\\n50.6 (zero shot)\\nReading Comprehension\\nBoolQ\\nPaLM(f) (540B)\\n92.2 (-)\\nT5 (11B)\\n91.2 (-)\\nPaLM-2 (Large)\\n90.9 (one shot)\\nTruthfulness\\nTruthful-QA\\nLLaMA (65B)\\n57 (-)\\nMathematical Reasoning\\nMATH\\nGemini (Ultra)\\n53.2 (4-shot)\\nPaLM-2 (Large)\\n34.3 (4-shot)\\nLLaMa-2 (65B)\\n13.5 (4-shot)\\nGSM8K\\nGPT-4 (-)\\n92.0 (5-shot)\\nPaLM-2 (Large)\\n80.7 (8-shot)\\nU-PaLM (540B)\\n58.5 (-)\\nProblem Solving and\\nLogical Reasoning\\nHumanEval\\nGemini( f) (Ultra)\\n74.4 (zero shot)\\nGPT-4 (-)\\n67.0 (zero shot)\\nCode Llama (34B)\\n48.8 (zero shot)\\nin formulating new hypotheses and research questions since\\ntheir ability to process large-scale datasets allows them to un-\\nveil insights that might not be immediately apparent to human\\nresearchers [458]. Moreover, for scientific writing, LLMs can\\nhelp researchers draft documents, suggest improvements, and\\nensure adherence to specific formatting guidelines [459, 460].\\nThis not only saves time but also improves the clarity of scien-\\ntific communication, enabling interdisciplinary teams to work\\ntogether more effectively.\\nMaths: In addition to providing mathematical research and\\neducation support, LLMs can assist in solving mathematical\\nproblems by giving step-by-step explanations and guiding users\\nthrough complex proofs and calculations. They can help iden-\\ntify errors in reasoning or computation and suggest corrections,\\nserving as an invaluable tool for both learning and verification\\npurposes [461, 462]. LLMs can be employed to check the valid-\\nity of mathematical proofs, offering a preliminary filter before\\nhuman review. While they are not a substitute for the meticu-\\nlous work of mathematicians, they can help simplify the process\\nof proof verification [463, 464]. Moreover, LLMs enhance ac-\\ncessibility to mathematics by translating complex concepts and\\nfindings into understandable language for non-specialists [465],\\nwhere the gap between theoretical mathematics and applied\\ncontexts such as physics, engineering, and economics can be\\nbridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-\\numents, including generating initial coding for datasets, iden-\\ntifying themes, and classifying data according to these themes.\\nThis collaborative effort between legal experts and LLMs has\\nproved to be effective in analyzing legal texts such as court\\nopinions on theft, improving both the efficiency and quality of\\nthe research [466]. Additionally, LLMs have been evaluated for\\ntheir ability to generate explanations of legal terms, focusing\\non improving factual accuracy and relevance by incorporating\\nsentences from case law. By feeding relevant case law into the\\nLLM, the augmented models can generate higher-quality expla-\\nnations with less factually incorrect information [467]. More-\\nover, LLMs can be trained with specialized domain knowledge\\nto perform legal reasoning tasks [468] and answer legal ques-\\ntions [469].\\nFinance: LLMs like BloombergGPT [151], trained on exten-\\nsive proprietary financial datasets, exhibit superior performance\\non financial tasks. This indicates the value of domain-specific\\ntraining in creating LLMs that can more accurately understand\\nand process industry-specific language and concepts. The intro-\\nduction of FinGPT [470] as an open-source model offers trans-\\nparent and accessible resources to develop novel applications\\nsuch as robo-advising, algorithmic trading, and low-code so-\\nlutions, ultimately expanding the capabilities of financial ser-\\nvices. Both BloombergGPT and FinGPT show the adaptabil-\\nity of LLMs to the financial domain, with the former showing\\nthe power of custom datasets and the latter emphasizing a data-\\ncentric approach and low-rank adaptation techniques for cus-\\ntomization. Moreover, LLMs demonstrate an ability to break\\ndown complex financial tasks into actionable plans, enabling\\nend-to-end solutions that were previously unfeasible with a sin-\\ngle model [471].\\nRobotics: In robotics research, LLMs have promising appli-\\ncations, such as enhancing human-robot interaction [28, 472,\\n473, 474], task planning [237], motion planning [246], nav-\\nigation [246, 475], object manipulation [236], personalized\\nrobots [476], etc. LLMs enable robots to understand the en-\\nvironment effectively and generate plans to complete tasks col-\\nlaboratively [240, 26]. They can facilitate continuous learning\\nby allowing robots to access and integrate information from a\\nwide range of sources, helping robots acquire new skills, adapt\\nto changes, and refine their paths [224, 233, 234].\\n7. Challenges and Future Directions\\nLLMs such as GPT-4 and its predecessors have significantly\\nadvanced natural language processing. Nevertheless, they also\\nbring along a set of challenges. The computational cost, ad-\\nversarial robustness, and interpretability are among the tech-\\nnical challenges that are intrinsic to these models.\\nFurther-\\nmore, as these models are scaled up to handle more complex\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tasks or to operate in more complex or dynamic environments,\\nnew challenges in scalability, privacy, and real-time processing\\nemerge. On the frontier of foundational research, integrating\\nmulti-modality and the effectiveness of transfer learning are be-\\ning keenly explored. Additionally, the continuous learning as-\\npect of these models, which aims to have models that can adapt\\nto new information over time, presents a fresh set of challenges.\\nThese challenges not only underscore the technical intricacies\\ninvolved but also highlight the broader impact and the future\\ntrajectory of LLMs in real-world applications. The following\\nsections delve into these challenges, shedding light on the on-\\ngoing and potential efforts to address them.\\nComputational Cost: Training LLMs require extensive compu-\\ntational resources, which increases production costs and raises\\nenvironmental concerns due to substantial energy consump-\\ntion during large-scale training. Improved performance occurs\\nas computational resources increase, but the rate of improve-\\nment gradually decreases when both the model and dataset\\nsize remain fixed, following the power law of diminishing re-\\nturns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-\\nases in their training data. These biases can manifest in the\\nmodel’s outputs, leading to potential ethical and fairness is-\\nsues [478].\\nOverfitting: Although LLMs possess substantial learning ca-\\npabilities, they are susceptible to overfitting noisy and peculiar\\npatterns within their extensive training data. Consequently, this\\nmay cause them to generate illogical responses [479]. The de-\\nbate about Memorization vs. Generalization in LLMs is about\\nfinding the right balance. Memorization allows the model to\\nremember specific details from its training data, ensuring it can\\nprovide accurate answers to precise questions. However, gen-\\neralization enables the model to make inferences and produce\\nresponses for inputs it has not seen before, which is essential\\nfor handling various real-world tasks. Striking the right bal-\\nance is the challenge: too much memorization can lead to over-\\nfitting, making the model inflexible and struggling with new\\ninputs [480].\\nEconomic and Research Inequality: The high cost of train-\\ning and deploying LLMs may make their development concen-\\ntrated within well-funded organizations, potentially worsening\\neconomic and research inequalities in AI [481].\\nReasoning and Planning: Some reasoning and planning tasks,\\neven as seemingly simple as common-sense planning, which\\nhumans find easy, remain well beyond the current capabilities\\nof LLMs evaluated using an assessment framework. This is not\\nentirely unexpected, considering that LLMs primarily generate\\ntext completions based on likelihood and offer no solid guaran-\\ntees in terms of reasoning abilities [482].\\nHallucinations: LLMs exhibit “hallucinations\", where they\\ngenerate responses that, while sounding plausible, are incorrect\\nor do not align with the provided information [483]. Hallucina-\\ntions can be categorized into three categories.\\n• Input-conflicting hallucination, wherein LLMs produce\\ncontent that diverges from the input given by users.\\n• Context-conflicting hallucination, where LLMs generate\\ncontent that contradicts information they have generated\\nearlier.\\n• Fact-conflicting hallucination involves LLM’s generation\\nof content that does not align with established world\\nknowledge.\\nPrompt Engineering: Prompts serve as inputs to LLMs, and\\ntheir syntax and semantics play a crucial role in determining\\nthe model’s output. The prompt variations, sometimes counter-\\nintuitive to humans, can result in significant changes in model\\noutput and are addressed through prompt engineering, which\\ninvolves designing natural language queries to guide LLMs\\nresponses effectively [484, 32].\\nLimited Knowledge: Information acquired during pretraining\\nis limited and may become obsolete after some time.\\nRe-\\ntraining the model using updated data is costly. To generate\\nfactually accurate responses, people use a retrieval augmen-\\ntation pipeline [198].\\nHowever, pre-trained models are not\\ntrained with retrieval augmentation generation (RAG) [6, 21];\\nhence, adapting the training pipeline is necessary [193, 25].\\nSafety and Controllability: Using LLMs comes with the risk\\nof generating harmful, misleading, or inappropriate content,\\nwhether by accident or when given specific prompts. Ensuring\\nthese models are safely utilized is a significant concern [485].\\nSecurity and Privacy: LLMs are prone to leaking personal\\ninformation and generating false, unethical, misaligned re-\\nsponses. Researchers have explored various security attacks,\\ni.e., backdoor attacks, jailbreaking, prompt injection, and data\\npoisoning, that lead to breaking LLMs security.\\nTherefore,\\ndeveloping better defense mechanisms is essential to ensure\\nLLMs are safe, reliable, and trustworthy for complex AI\\napplications [486].\\nMulti-Modality:\\nMulti-modal learning, where LLMs are\\ntrained on diverse data like text, images, and videos, aims to\\ncreate models with richer understanding but faces challenges\\nin data alignment, fusion strategies, and higher computational\\ndemands.\\nCatastrophic Forgetting:\\nLLMs are often pre-trained on\\nlarge datasets and then fine-tuned on domain-specific data,\\nreducing training resources.\\nHowever, they face issues like\\ndomain adaptation and catastrophic forgetting, which hinder\\nthe retention of original knowledge when learning new tasks.\\nAdversarial Robustness:\\nLarge Language Models (LLMs)\\nhave shown great capabilities in various tasks but are vul-\\nnerable to adversarial attacks, where slight, deliberate input\\nalterations can mislead them.\\nEspecially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-\\nthough it sometimes compromises generalization [487].\\nAs\\nLLMs integrate more into complex systems, examining their\\nsecurity properties becomes crucial, given the emerging field\\nof adversarial attacks on LLMs within trustworthy ML [488].\\nThis vulnerability is notable in safety-critical domains, ne-\\ncessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].\\nInterpretability and Explainability: The “black-box” nature\\nof LLMs poses challenges in understanding their decision-\\nmaking, which is crucial for broader acceptance and trust,\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='especially in sensitive domains.\\nDespite their advanced\\ncapabilities, the lack of insight into their operation limits their\\neffectiveness and trustworthiness [490, 491]. Efforts are being\\nmade to make LLMs more explainable to promote user trust\\nand to ensure responsible AI usage. Understanding the logic\\nbehind LLMs’ responses is essential for fostering trust and\\nensuring they align with human values and legal standards.\\nPrivacy Concerns:\\nPrivacy concerns in Large Language\\nModels (LLMs) have escalated with their growth in complexity\\nand size, particularly around data sharing and potential misuse.\\nThere is a risk of malicious content creation, filter bypass,\\nand data privacy issues, especially in e-commerce, where\\nprotecting customer privacy is crucial. If models are trained\\non private data, additional concerns arise if such models are\\nmade publicly available. LLMs tend to memorize phrases from\\ntheir training sets, which an adversary could exploit to extract\\nsensitive data, posing a threat to personal privacy [492, 493].\\nReal-Time Processing: Real-time processing in Large Lan-\\nguage Models (LLMs) is pivotal for various applications,\\nespecially with the rising popularity of mobile AI applications\\nand concerns regarding information security and privacy.\\nHowever, LLMs often have hundreds of layers and millions\\nof parameters, which impede real-time processing due to the\\nhigh computational demands and limited weight storage on\\nhardware platforms, particularly in edge computing environ-\\nments [494].\\nWhile certain efforts like MobileBERT aim\\nto reduce memory requirements, they still face substantial\\nexecution overhead due to the large number of model layers,\\nleading to high inference latency.\\nLong-Term Dependencies:\\nLarge Language Models have\\nshown considerable progress in understanding and generating\\ntext, yet they often struggle with preserving context and\\nhandling long-term dependencies, particularly in complex,\\nmulti-turn conversations or long documents. This limitation\\ncan lead to incoherent or irrelevant responses.\\nHardware Acceleration: The growth of LLMs presents signif-\\nicant hardware challenges due to the increasing computational\\nand memory demands associated with training and deploying\\nthese models. GPUs have played a crucial role in meeting the\\nhardware requirements for training LLMs, with the networking\\nindustry also evolving to optimize hardware for training\\nworkloads. However, the growing size of LLMs, which has\\nbeen outpacing hardware progress, makes model inference in-\\ncreasingly costly. Model quantization is a promising approach\\nto bridge the widening gap between LLM size and hardware\\ncapacity [495].\\nAlthough specialized hardware acceleration\\nlike GPUs or TPUs can significantly reduce the computational\\ncost, making real-time applications more feasible, they may not\\nfully resolve all limitations, necessitating further advancements\\nin hardware technology.\\nRegulatory and Ethical Frameworks: The rapid advancements\\nin artificial intelligence have given rise to sophisticated Large\\nLanguage Models (LLMs) like OpenAI’s GPT-4 [157] and\\nGoogle’s Bard. These developments underscore the imperative\\nfor regulatory oversight to manage the ethical and social\\nchallenges accompanying LLMs’ widespread use [496]. For\\ninstance, LLMs can generate content that can be used posi-\\ntively or negatively, emphasizing the need for proactive ethical\\nframeworks and policy measures to guide their responsible\\nuse and assign accountability for their outputs [497]. Auditing\\nis identified as a promising governance mechanism to ensure\\nthat AI systems, including LLMs, are designed and deployed\\nethically, legally, and technically robust [498].\\n8. Conclusion\\nThis article has comprehensively reviewed the develop-\\nments in LLMs.\\nIt contributes to summarizing significant\\nfindings of LLMs in the existing literature and provides a\\ndetailed analysis of the design aspects, including architec-\\ntures, datasets, and training pipelines.\\nWe identified crucial\\narchitectural components and training strategies employed by\\ndifferent LLMs.\\nThese aspects are presented as summaries\\nand discussions throughout the article.\\nMoreover, we have\\ndiscussed the performance differences of LLMs in zero-shot\\nand few-shot settings, explored the impact of fine-tuning, and\\ncompared supervised and generalized models and encoder vs.\\ndecoder vs. encoder-decoder architectures. A comprehensive\\nreview of multi-modal LLMs, retrieval augmented LLMs,\\nLLMs-powered agents, efficient LLMs, datasets, evaluation,\\napplications, and challenges is also provided. This article is\\nanticipated to serve as a valuable resource for researchers,\\noffering insights into the recent advancements in LLMs and\\nproviding fundamental concepts and details to develop better\\nLLMs.\\nAcknowledgement:\\nThe author/s would like to acknowl-\\nedge the support received from Saudi Data and AI Authority\\n(SDAIA) and King Fahd University of Petroleum and Miner-\\nals (KFUPM) under SDAIA-KFUPM Joint Research Center for\\nArtificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\\ntory” for natural language processing?, in:\\nMachine Learning and\\nKnowledge Discovery in Databases. Research Track: European Con-\\nference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\\nProceedings, Part III 21, Springer, 2021, pp. 677–693. 1\\n[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, S. Bowman, Superglue: A stickier benchmark for general-\\npurpose language understanding systems, Advances in neural informa-\\ntion processing systems 32 (2019). 1, 26, 29\\n[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\\nZ. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human-\\nlike open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\\n[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n151 (2) (2022) 183–197. 2\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\\nLanguage models are unsupervised multitask learners, OpenAI blog\\n1 (8) (2019) 9. 2, 7\\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\\nare few-shot learners, Advances in neural information processing sys-\\ntems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018). 2, 18, 24\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nL. Zettlemoyer, Deep contextualized word representations, in: NAACL-\\nHLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\\n2\\n[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV. Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehen-\\nsion, arXiv preprint arXiv:1910.13461 (2019). 2\\n[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\\na unified text-to-text transformer, The Journal of Machine Learning Re-\\nsearch 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31\\n[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-\\ntext transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\\n25, 28, 30\\n[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,\\nJ. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\\nguage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\\nparameter open-access multilingual language model, arXiv preprint\\narXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30\\n[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\\nM. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer\\nlanguage models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24,\\n25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\\ning language modeling with pathways, arXiv preprint arXiv:2204.02311\\n(2022). 2, 6, 9, 11, 23, 24, 25\\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\\nlanguage models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31\\n[17] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\\nprompted training enables zero-shot task generalization, arXiv preprint\\narXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31\\n[18] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\\nSuper-naturalinstructions: Generalization via declarative instructions on\\n1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\\n11, 16, 17, 24, 25, 28, 31\\n[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\\njishirzi, Self-instruct: Aligning language model with self generated in-\\nstructions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28\\n[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\\nels to follow instructions with human feedback, Advances in Neural In-\\nformation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\\n22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\\nfoundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\\n(2023). 2, 7, 10, 16, 25, 34\\n[22] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\\ngatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\\nlarge language models, arXiv preprint arXiv:2206.07682 (2022). 2\\n[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\\nlanguage models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\\n[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\\nentific research capabilities of large language models, arXiv preprint\\narXiv:2304.05332 (2023). 2\\n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\\nretrieval augmented language models, arXiv preprint arXiv:2208.03299\\n(2022). 2, 18, 19, 34\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33\\n[27] A. Parisi, Y. Zhao, N. Fiedel, Talm: Tool augmented language models,\\narXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models\\nfor human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\\n33\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi,\\nY. Shi, et al., mplug-owl: Modularization empowers large language\\nmodels with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\\n22\\n[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\\nT. Lu, J. Zhou, Y. Qiao, et al., Visionllm: Large language model\\nis also an open-ended decoder for vision-centric tasks, arXiv preprint\\narXiv:2305.11175 (2023). 2, 22\\n[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:\\nTeaching large language model to use tools via self-instruction, arXiv\\npreprint arXiv:2305.18752 (2023). 2, 19, 22, 23\\n[32] E.\\nSaravia,\\nPrompt\\nEngineering\\nGuide,\\nhttps://github.com/dair-\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\\nW. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\\nmodel, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25\\n[34] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5+:\\nOpen code large language models for code understanding and genera-\\ntion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25\\n[35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\\nY. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\\nedge enhanced pre-training for language understanding and generation,\\narXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25\\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: System op-\\ntimizations enable training deep learning models with over 100 billion\\nparameters, in: Proceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5\\n[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimiza-\\ntions toward training trillion parameter models, in: SC20: International\\nConference for High Performance Computing, Networking, Storage and\\nAnalysis, IEEE, 2020, pp. 1–16. 2, 4, 24\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\\na unified view of parameter-efficient transfer learning, arXiv preprint\\narXiv:2110.04366 (2021). 2, 20, 21\\n[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\\nria, Llm-adapters: An adapter family for parameter-efficient fine-tuning\\nof large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\\n20\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\\nefficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\\nFrom dense to sparse: Contrastive pruning for better pre-trained lan-\\nguage model compression, in: Proceedings of the AAAI Conference on\\nArtificial Intelligence, Vol. 36, 2022, pp. 11547–11555. 2, 22\\n[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\\nAccurate and efficient post-training quantization for large language\\nmodels, in: ICML, Vol. 202 of Proceedings of Machine Learning Re-\\nsearch, PMLR, 2023, pp. 38087–38099. 2, 21\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nCompression of generative pre-trained language models via quantiza-\\ntion, arXiv preprint arXiv:2203.10705 (2022). 2, 21\\n[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\\nGiraffe: Adventures in expanding context lengths in llms, arXiv preprint\\narXiv:2308.10882 (2023). 2, 17\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\\nEfficient con-\\ntext window extension of large language models, arXiv preprint\\narXiv:2309.00071 (2023). 2, 17\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Longt5: Efficient text-to-text transformer for long sequences, arXiv\\npreprint arXiv:2112.07916 (2021). 2, 18\\n[49] S. Chen, S. Wong, L. Chen, Y. Tian, Extending context window\\nof large language models via positional interpolation, arXiv preprint\\narXiv:2306.15595 (2023). 2, 17\\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\\nJ. Zhang, Z. Dong, et al., A survey of large language models, arXiv\\npreprint arXiv:2303.18223 (2023). 2, 3, 7\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\\nvey on word representation models: From classical to state-of-the-art\\nword representation language models, Transactions on Asian and Low-\\nResource Language Information Processing 20 (5) (2021) 1–35. 2, 3\\n[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\\ncessing via large pre-trained language models: A survey, arXiv preprint\\narXiv:2111.01243 (2021). 2, 3\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He, et al., A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\\n2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\\narXiv:2301.00234 (2022). 2, 7, 18\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\\n[56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\\nQ. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long\\nPapers), 2016, pp. 1715–1725. 4\\n[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\\nIEEE international conference on acoustics, speech and signal process-\\ning (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization in\\nnlp, arXiv preprint arXiv:2112.10508 (2021). 4\\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017). 4, 7\\n[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\\nlinear biases enables input length extrapolation, in: International Con-\\nference on Learning Representations, 2022.\\nURL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\\n[66] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: En-\\nhanced transformer with rotary position embedding, arXiv preprint\\narXiv:2104.09864 (2021). 4, 9, 17\\n[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\\nwith sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\\n23\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness, Advances in Neural\\nInformation Processing Systems 35 (2022) 16344–16359. 4\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n[70] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\\nmann machines, in: Proceedings of the 27th international conference on\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\npreprint arXiv:1606.08415 (2016). 4\\n[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overfitting, The\\njournal of machine learning research 15 (1) (2014) 1929–1958. 4\\n[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\\nKe, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regular-\\nizing rnns by randomly preserving hidden activations, arXiv preprint\\narXiv:1606.01305 (2016). 4\\n[74] N.\\nShazeer,\\nGlu\\nvariants\\nimprove\\ntransformer,\\narXiv\\npreprint\\narXiv:2002.05202 (2020). 4\\n[75] Y. N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\\ngated convolutional networks, in: International conference on machine\\nlearning, PMLR, 2017, pp. 933–941. 4\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\narXiv:1607.06450 (2016). 4\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\nin Neural Information Processing Systems 32 (2019). 4\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\\ntransformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\\n[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\\nMegatron-lm: Training multi-billion parameter language models using\\nmodel parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\\n[81] \"bmtrain: Efficient training for big models.\".\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n[82] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\\ntac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\\nart natural language processing, in: Proceedings of the 2020 conference\\non empirical methods in natural language processing: system demon-\\nstrations, 2020, pp. 38–45. 5\\n[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\\nrin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\\nJax: composable transformations of python+ numpy programs (2018).\\n5\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You,\\nColossal-ai: A unified deep learning system for large-scale parallel train-\\ning, arXiv preprint arXiv:2110.14883 (2021). 5\\n[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe:\\nA\\nfast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\\n(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\\nwork, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\\n162. 5\\n[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\\native style, high-performance deep learning library, Advances in neural\\ninformation processing systems 32 (2019). 5\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\\nscale machine learning., in: Osdi, Vol. 16, Savannah, GA, USA, 2016,\\npp. 265–283. 5\\n[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,\\nB. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and efficient machine\\nlearning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\\nlion parameter models with simple and efficient sparsity, The Journal of\\nMachine Learning Research 23 (1) (2022) 5232–5270. 5, 9\\n[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\\nY. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\\nmodels with mixture-of-experts, in: International Conference on Ma-\\nchine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25\\n[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-P: Towards trillion\\nparameter language model with sparse heterogeneous computing, arXiv\\npreprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, C. Raffel, What language model architecture and pretrain-\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='ing objective works best for zero-shot generalization?, in: International\\nConference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou,\\nH.-W. Hon, Unified language model pre-training for natural language\\nunderstanding and generation, Advances in neural information process-\\ning systems 32 (2019). 6\\n[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\\nS. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\\nmodels, arXiv preprint arXiv:2001.08361 (2020). 6\\n[96] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\\net al., Training compute-optimal large language models, arXiv preprint\\narXiv:2203.15556 (2022). 6, 9, 25, 29\\n[97] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\\nT. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\\nstruction meta learning through the lens of generalization, arXiv preprint\\narXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\\n[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan,\\nPrinciple-driven self-alignment of language models from scratch with\\nminimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\\n7, 17\\n[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\\nN. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\\nas a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\\n7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\\nP. Christiano, G. Irving, Fine-tuning language models from human pref-\\nerences, arXiv preprint arXiv:1909.08593 (2019). 7\\n[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\\ntion: Improving zero-shot and few-shot learning of language models via\\nchain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\\nining the power of symbolic tasks in instruction tuning, arXiv preprint\\narXiv:2304.07995 (2023). 7, 16\\n[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\\nD. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\\nlanguage models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23\\n[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022). 7, 20\\n[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. Narasimhan,\\nTree of thoughts: Deliberate problem solving with large language mod-\\nels, arXiv preprint arXiv:2305.10601 (2023). 7, 20\\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\\ning for nlp, in: International Conference on Machine Learning, PMLR,\\n2019, pp. 2790–2799. 7, 20\\n[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\\nof large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\\n[108] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang, et al., Pangu-α : Large-scale autoregressive pre-\\ntrained chinese language models with auto-parallel computation, arXiv\\npreprint arXiv:2104.12369 (2021). 8, 23, 24, 25\\n[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\\nJ. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\\ntraining language models, AI Open 2 (2021) 65–68. 8, 30\\n[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY. Zhao, Y. Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation, arXiv preprint\\narXiv:2107.02137 (2021). 8, 25\\n[111] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov,\\nTransformer-xl: Attentive language models beyond a fixed-length con-\\ntext, arXiv preprint arXiv:1901.02860 (2019). 8\\n[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\\n[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\\nficiencies of self-attention, Advances in Neural Information Processing\\nSystems 33 (2020) 22640–22651. 8, 11\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\\nmodels bring?\\nintensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25\\n[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\\nL. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\\nshot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\\n24, 25\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\\nguage models: Methods, analysis & insights from training gopher, arXiv\\npreprint arXiv:2112.11446 (2021). 8, 9, 25, 28\\n[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al.,\\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a\\nlarge-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25\\n[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\\nH. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\\nsource autoregressive language model, arXiv preprint arXiv:2204.06745\\n(2022). 9, 23, 24, 25\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9\\n[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\\ncision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\\n[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\\nton, J. Dean, Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\\n[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\\natm 20b: Few-shot learning using a large-scale multilingual seq2seq\\nmodel, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25\\n[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\\narXiv preprint arXiv:2305.10403 (2023). 9, 25\\n[124] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia,\\nH. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\\nwith 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\\n24, 25\\n[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\\nguage learning paradigms, in: The Eleventh International Conference\\non Learning Representations, 2022. 9, 10, 24, 25\\n[126] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\\neral language model pretraining with autoregressive blank infilling, in:\\nProceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. 10\\n[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and efficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023). 10, 23, 25\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\npreprint arXiv:2112.05682 (2021). 10\\n[129] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\nM. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\\ntransformer models, Proceedings of Machine Learning and Systems 5\\n(2023). 10\\n[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of\\nmodels, arXiv preprint arXiv:2407.21783 (2024). 10, 25\\n[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25\\n[132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n25\\n[133] https://github.com/xai-org/grok-1. 10\\n[134] https://x.ai/blog/grok-1.5. 10\\n[135] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly\\ncapable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10\\n[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem-\\nini 1.5: Unlocking multimodal understanding across millions of tokens\\nof context, arXiv preprint arXiv:2403.05530 (2024). 10\\n[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun-\\ndyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b\\ntechnical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25\\n[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\\nQ. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\\nwith longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25\\n[139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,\\nC. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li,\\nF. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang,\\nH. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\\nJ. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao,\\nK. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li,\\nM. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang,\\nP. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge,\\nR. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye,\\nS. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\\nT. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao,\\nW. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen,\\nX. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical,\\nand efficient mixture-of-experts language model, CoRR abs/2405.04434\\n(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,\\nC. Xiong, Codegen: An open large language model for code with multi-\\nturn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11,\\n23, 25, 28\\n[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\\nwards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31\\n[142] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\\ncode generation with alphacode, Science 378 (6624) (2022) 1092–1097.\\n11, 23, 25, 29\\n[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\narXiv preprint arXiv:1911.02150 (2019). 11\\n[144] R. Y. Pang, H. He, Text generation by learning from demonstrations,\\narXiv preprint arXiv:2009.07839 (2020). 11\\n[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine\\ntranslation models, arXiv preprint arXiv:2009.09372 (2020). 11\\n[146] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and genera-\\ntion, arXiv preprint arXiv:2109.00859 (2021). 11\\n[147] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\\nwith you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25\\n[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\\nA. Poulton, V. Kerkez, R. Stojnic, Galactica: A large language model for\\nscience, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\\n[149] FairScale authors, Fairscale: A general purpose modular pytorch library\\nfor high performance and large scale training, https://github.com/\\nfacebookresearch/fairscale (2021). 11\\n[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models\\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\\n[151] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33\\n[152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\\ncial chat model with hundreds of billions parameters, arXiv preprint\\narXiv:2305.12002 (2023). 11, 17, 25\\n[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\nformer language model with jax (2021). 12, 24\\n[154] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\\nCrosslingual generalization through multitask finetuning, arXiv preprint\\narXiv:2211.01786 (2022). 16, 25, 28, 31\\n[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\\nDynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue, et al., Llama-adapter v2: Parameter-efficient visual in-\\nstruction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\\n[157] Openai. gpt-4 technical report (2023). 16, 35\\n[158] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\\nT. B. Hashimoto, Stanford alpaca:\\nAn instruction-following llama\\nmodel,\\nhttps://github.com/tatsu-lab/stanford_alpaca\\n(2023). 16, 25, 28\\n[159] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\\nS. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).\\nURL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\\n28\\n[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprint\\narXiv:2304.06975 (2023). 16\\n[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\\nWizardlm: Empowering large language models to follow complex in-\\nstructions, arXiv preprint arXiv:2304.12244 (2023). 16\\n[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nD. Jiang, Wizardcoder: Empowering code large language models with\\nevol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\\n[165] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\\ning language models to support answers with verified quotes, arXiv\\npreprint arXiv:2203.11147 (2022). 17\\n[166] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-\\nassisted question-answering with human feedback, arXiv preprint\\narXiv:2112.09332 (2021). 17, 19, 20, 25, 31\\n[167] A. Glaese, N. McAleese, M. Tr˛ebacz, J. Aslanides, V. Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\\nalignment of dialogue agents via targeted human judgements, arXiv\\npreprint arXiv:2209.14375 (2022). 17, 20, 25\\n[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\\nDirect preference optimization: Your language model is secretly a re-\\nward model, arXiv preprint arXiv:2305.18290 (2023). 17\\n[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\\nT. Zhang, Raft: Reward ranked finetuning for generative foundation\\nmodel alignment, arXiv preprint arXiv:2304.06767 (2023). 17\\n[170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\\nresponses to align language models with human feedback without tears,\\narXiv preprint arXiv:2304.05302 (2023). 17\\n[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, H. Wang, Preference rank-\\ning optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17\\n[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\\ntuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\\n17\\n[173] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\\nA. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\\nlessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\\n[174] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang,\\nT. B. Hashimoto,\\nAlpacafarm:\\nA simulation frame-\\nwork for methods that learn from human feedback, arXiv preprint\\narXiv:2305.14387 (2023). 17\\n[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\\nPrompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17\\n[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯ut˙e, A. Chen,\\nA. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\\nity for moral self-correction in large language models, arXiv preprint\\narXiv:2302.07459 (2023). 17\\n[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 17\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath,\\nB. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\\nguage models to reduce harms: Methods, scaling behaviors, and lessons\\nlearned, arXiv preprint arXiv:2209.07858 (2022). 17, 28\\n[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\\nlish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17\\n[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\\nN. McAleese, G. Irving, Red teaming language models with language\\nmodels, arXiv preprint arXiv:2202.03286 (2022). 17\\n[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\\ncontinual learners, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 6107–6122. 17\\n[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\\nC. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17\\n[184] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong,\\nJ. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\\nof low training data instruction tuning, arXiv preprint arXiv:2305.09246\\n(2023). 17\\n[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\\nP. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\\narXiv:2305.11206 (2023). 17, 25, 28\\n[186] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm-infinite: Sim-\\nple on-the-fly length generalization for large language models, arXiv\\npreprint arXiv:2308.16137 (2023). 17, 18\\n[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan-\\nskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay, et al., Colt5: Faster\\nlong-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18\\n[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\\nLongnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\\narXiv:2307.02486 (2023). 18\\n[189] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\\ncient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18\\n[190] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend,\\nE. Karpas, A. Shashua, K. Leyton-Brown, Y. Shoham, Parallel context\\nwindows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank:\\nEnhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18\\n[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\\nReflexion: Language agents with verbal reinforcement learning, arXiv\\npreprint arXiv:2303.11366 14 (2023). 18, 20\\n[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\\ning llms with databases as their symbolic memory, arXiv preprint\\narXiv:2306.03901 (2023). 18\\n[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, G. Neubig, Active retrieval augmented generation, arXiv\\npreprint arXiv:2305.06983 (2023). 18\\n[198] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, Y. Shoham, In-context retrieval-augmented language models,\\narXiv preprint arXiv:2302.00083 (2023). 18, 34\\n[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18\\n[200] D. Schuurmans, Memory augmented large language models are compu-\\ntationally universal, arXiv preprint arXiv:2301.04589 (2023). 18\\n[201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\\ngeneral read-write memory for large language models, arXiv preprint\\narXiv:2305.14322 (2023). 18\\n[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\\nwork: Bm25 and beyond, Foundations and Trends® in Information Re-\\ntrieval 3 (4) (2009) 333–389. 18\\n[203] X. Wang,\\nJ. Wei,\\nD. Schuurmans,\\nQ. Le,\\nE. Chi,\\nD. Zhou,\\nRationale-augmented ensembles in language models, arXiv preprint\\narXiv:2207.00747 (2022). 18\\n[204] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-G. Lou, W. Chen,\\nRepocoder: Repository-level code completion through iterative retrieval\\nand generation, arXiv preprint arXiv:2303.12570 (2023). 18\\n[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\\nO. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study, arXiv preprint\\narXiv:2304.06762 (2023). 19\\n[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19\\n[207] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What makes\\ngood in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\\n(2021). 19\\n[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19\\n[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\\nmodels, arXiv preprint arXiv:2301.12652 (2023). 19\\n[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\narXiv preprint arXiv:2306.13421 (2023). 19\\n[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\\nlanguage model pre-training, in: International conference on machine\\nlearning, PMLR, 2020, pp. 3929–3938. 19\\n[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: Efficient and ef-\\nfective retrieval-augmented text generation, in: Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2023, pp. 1437–1447. 19\\n[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\nation, arXiv preprint arXiv:2107.07566 (2021). 19\\n[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\\naugmented language models through few-shot prompting for open-\\ndomain question answering, arXiv preprint arXiv:2203.05115 (2022).\\n19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\\ngpt: A general multi-modal assistant that can plan, execute, inspect, and\\nlearn, arXiv preprint arXiv:2306.08640 (2023). 19\\n[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\\nJ. Gao, Chameleon: Plug-and-play compositional reasoning with large\\nlanguage models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23\\n[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\\nRibeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\\nguage models, arXiv preprint arXiv:2303.09014 (2023). 19\\n[218] C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y. Fujii, A. Ratner, C.-Y. Lee, R. Kr-\\nishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\\nlarge language models, arXiv preprint arXiv:2308.00675 (2023). 19\\n[219] Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt:\\nConnecting large language models with real-world applications via rest-\\nful apis, arXiv preprint arXiv:2306.06624 (2023). 19\\n[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\\nguage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='20\\n[224] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv-\\ning ai tasks with chatgpt and its friends in huggingface, arXiv preprint\\narXiv:2303.17580 (2023). 19, 20, 33\\n[225] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji,\\nS. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\\ndation models with millions of apis, arXiv preprint arXiv:2303.16434\\n(2023). 19\\n[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\\n[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\\nuser assistance systems, Business & Information Systems Engineering\\n58 (2016) 367–370. 20\\n[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\\nmulti-agent collaborative framework, arXiv preprint arXiv:2308.00352\\n(2023). 20\\n[230] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou, et al., The rise and potential of large language model\\nbased agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\\n[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\\nX. Chen, Y. Lin, et al., A survey on large language model based au-\\ntonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20\\n[232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\\nshot planners: Extracting actionable knowledge for embodied agents,\\nin: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20\\n[233] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\\ning with language model is planning with world model, arXiv preprint\\narXiv:2305.14992 (2023). 20, 33\\n[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy,\\nZ. Chen, J. Zhang, D. Arpit, et al., Retroformer:\\nRetrospective\\nlarge language agents with policy gradient optimization, arXiv preprint\\narXiv:2308.02151 (2023). 20, 33\\n[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\\nlogue: Embodied reasoning through planning with language models, in:\\n6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20\\n[236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\\nEmbodied finetuning for vision-language reasoning in robot manipula-\\ntion, arXiv preprint arXiv:2305.18898 (2023). 20, 33\\n[237] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, A. Garg, Progprompt: Generating situated robot task plans\\nusing large language models, in: 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20,\\n33\\n[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\\nChiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\\nfor robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\\n[239] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein,\\nMedagents: Large language models as collaborators for zero-shot med-\\nical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20\\n[240] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\\nJ. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\\nGrounding language in robotic affordances, in: Conference on Robot\\nLearning, PMLR, 2023, pp. 287–318. 20, 33\\n[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\\nguided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\\n20\\n[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\\nnav: Grounding large language models for dynamic planning to navi-\\ngation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20\\n[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su,\\nLlm-planner: Few-shot grounded planning for embodied agents with\\nlarge language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n[244] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\\npreprint arXiv:2303.03480 (2023). 20\\n[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\\nrobot navigation, in: 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\\n[246] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\\nwith large language models for object rearrangement, arXiv preprint\\narXiv:2303.06247 (2023). 20, 33\\n[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\\n[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-efficient tun-\\ning: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\\n20\\n[249] Y. Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\\nAdamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\\nguage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\\n[250] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\\nW. Chen, Lora: Low-rank adaptation of large language models, arXiv\\npreprint arXiv:2106.09685 (2021). 21, 22, 23\\n[251] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks, in: Pro-\\nceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 2: Short Papers), 2022, pp. 61–68. 21\\n[252] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\\nProgressive prompts: Continual learning for language models, arXiv\\npreprint arXiv:2301.12314 (2023). 21\\n[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\\nwards adaptive prefix tuning for parameter-efficient language model\\nfine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\\n[254] E. B. Zaken, S. Ravfogel, Y. Goldberg, Bitfit:\\nSimple parameter-\\nefficient fine-tuning for transformer-based masked language-models,\\narXiv preprint arXiv:2106.10199 (2021). 21\\n[255] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8 ():\\n8-bit matrix multiplication for transformers at scale, arXiv preprint\\narXiv:2208.07339 (2022). 21, 22\\n[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq:\\nAccurate\\npost-training quantization for generative pre-trained transformers, arXiv\\npreprint arXiv:2210.17323 (2022). 21\\n[257] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\\npression+: Accurate quantization of large language models by equiva-\\nlent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21\\n[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\\naccurate post-training quantization and pruning, Advances in Neural In-\\nformation Processing Systems 35 (2022) 4475–4488. 21\\n[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, arXiv\\npreprint arXiv:2306.02272 (2023). 21\\n[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\\nW. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained language models, arXiv\\npreprint arXiv:2210.03858 (2022). 21\\n[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient\\nfinetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22\\n[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr-\\nishnamoorthi, V. Chandra, Llm-qat: Data-free quantization aware train-\\ning for large language models, arXiv preprint arXiv:2305.17888 (2023).\\n21, 22\\n[263] Y. Guo, A. Yao, H. Zhao, Y. Chen, Network sketching: Exploiting bi-\\nnary structure in deep cnns, in: Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\\nMemory-efficient fine-tuning of compressed large language models via\\nsub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\\n22\\n[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and effective pruning\\napproach for large language models, arXiv preprint arXiv:2306.11695\\n(2023). 22\\n[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[267] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,\\nY. Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\\nmissing secret sauce for pruning llms to high sparsity, arXiv preprint\\narXiv:2310.05175 (2023). 22\\n[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nStructured pruning for efficient generative pre-trained language models,\\nin: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22\\n[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\\nguage model for few-shot learning, Advances in Neural Information Pro-\\ncessing Systems 35 (2022) 23716–23736. 22\\n[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models,\\narXiv preprint arXiv:2301.12597 (2023). 22\\n[271] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv preprint\\narXiv:2304.08485 (2023). 22\\n[272] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang,\\nY. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22\\n[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\\ntailed video understanding via large vision and language models, arXiv\\npreprint arXiv:2306.05424 (2023). 22\\n[274] H. Zhang,\\nX. Li,\\nL. Bing,\\nVideo-llama:\\nAn instruction-tuned\\naudio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22\\n[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\\nY. Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\\ndio captioning dataset for audio-language multimodal research, arXiv\\npreprint arXiv:2303.17395 (2023). 22\\n[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\\nllm: Multi-modal language modeling with image, audio, video, and text\\nintegration, arXiv preprint arXiv:2306.09093 (2023). 22\\n[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models,\\narXiv preprint arXiv:2304.10592 (2023). 22\\n[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020). 22\\n[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\\nS. Hoi, Instructblip: Towards general-purpose vision-language models\\nwith instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\\n[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\\nshot learning via instruction tuning, arXiv preprint arXiv:2212.10773\\n(2022). 22\\n[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\\nChatbridge: Bridging modalities with large language model as a lan-\\nguage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\\n[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,\\nX. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\\nlingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\\n[283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\\nH. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\\narXiv preprint arXiv:2305.14167 (2023). 22\\n[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\\nEfficient vision-language instruction tuning for large language models,\\narXiv preprint arXiv:2305.15023 (2023). 22\\n[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y. Qiao,\\nLlama-adapter: Efficient fine-tuning of language models with zero-init\\nattention, arXiv preprint arXiv:2303.16199 (2023). 22\\n[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\\nRobust speech recognition via large-scale weak supervision, in: Inter-\\nnational Conference on Machine Learning, PMLR, 2023, pp. 28492–\\n28518. 22\\n[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\\nmodal chain-of-thought reasoning in language models, arXiv preprint\\narXiv:2302.00923 (2023). 23\\n[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt\\ntuning in vision language models, arXiv preprint arXiv:2304.07919\\n(2023). 23\\n[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\\ning, drawing and editing with visual foundation models, arXiv preprint\\narXiv:2303.04671 (2023). 23\\n[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\\nM. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\\nsoning and action, arXiv preprint arXiv:2303.11381 (2023). 23\\n[291] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao,\\nS. Zhao, Y. Shan, et al., Caption anything: Interactive image descrip-\\ntion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\\n(2023). 23\\n[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\\nAdapting clip for powerful 3d open-world learning, arXiv preprint\\narXiv:2211.11682 (2022). 23\\n[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\\nsoning without training, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\\n23\\n[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\\nfusion with intra-and inter-modality attention flow for visual question\\nanswering, in: Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition, 2019, pp. 6639–6648. 23\\n[295] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Deep modular co-attention net-\\nworks for visual question answering, in: Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition, 2019, pp. 6281–\\n6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\\nW. Chang, S.-F. Chang, Idealgpt:\\nIteratively decomposing vision\\nand language reasoning via large language models, arXiv preprint\\narXiv:2305.14985 (2023). 23\\n[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li,\\nPrompt, generate, then cache: Cascade of foundation models makes\\nstrong few-shot learners, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\\n23\\n[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\\nnormalization of self-attention, CoRR abs/1910.05895 (2019). 24\\n[299] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-\\ntraining approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\\n[300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nD. Song, Koala: A dialogue model for academic research, Blog post\\n(April 2023).\\nURL\\nhttps://bair.berkeley.edu/blog/2023/04/03/koala/\\n25\\n[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile:\\nAn\\n800gb dataset of diverse text for language modeling, arXiv preprint\\narXiv:2101.00027 (2020). 28, 30\\n[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen,\\net al., The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset, Advances in Neural Information Processing Systems 35 (2022)\\n31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n[304] Together Computer, Redpajama: An open source recipe to reproduce\\nllama training dataset (Apr. 2023).\\nURL\\nhttps://github.com/togethercomputer/\\nRedPajama-Data 28\\n[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\\nTuning language models with (almost) no human labor, arXiv preprint\\narXiv:2212.09689 (2022). 28\\n[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\\nD. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\\nharmless assistant with reinforcement learning from human feedback,\\narXiv preprint arXiv:2204.05862 (2022). 28\\n[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\\nJ. Steinhardt, Measuring massive multitask language understanding,\\narXiv preprint arXiv:2009.03300 (2020). 26, 29\\n[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='the imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models, arXiv preprint arXiv:2206.04615 (2022). 26, 29\\n[309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\\nA multi-task benchmark and analysis platform for natural language un-\\nderstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29\\n[310] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\\neration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\\n29\\n[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu,\\nC. Yu, et al., Clue: A chinese language understanding evaluation bench-\\nmark, arXiv preprint arXiv:2004.05986 (2020). 29\\n[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\\nbenchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y.-L. Boureau, Can\\nyou put it all together: Evaluating conversational agents’ ability to blend\\nskills, arXiv preprint arXiv:2004.08449 (2020). 29\\n[314] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of\\nlanguage models, arXiv preprint arXiv:2211.09110 (2022). 29\\n[315] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\\nY. Song, T. Oh, et al., Klue: Korean language understanding evaluation,\\narXiv preprint arXiv:2105.09680 (2021). 29\\n[316] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\\nanswering challenge, Transactions of the Association for Computational\\nLinguistics 7 (2019) 249–266. 27, 29\\n[317] M.\\nT.\\nPilehvar,\\nJ.\\nCamacho-Collados,\\nWic:\\n10,000\\nexample\\npairs for evaluating context-sensitive representations, arXiv preprint\\narXiv:1808.09121 6 (2018). 27, 29\\n[318] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\\nmodels, arXiv preprint arXiv:1609.07843 (2016). 28, 29\\n[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\\nsive transformers for long-range sequence modelling, arXiv preprint\\narXiv:1911.05507 (2019). 28, 29\\n[320] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\\nlarge-scale chinese question matching corpus, in: Proceedings of the\\n27th international conference on computational linguistics, 2018, pp.\\n1952–1962. 28, 29\\n[321] S.\\nIyer,\\nN.\\nDandekar,\\nK.\\nCsernai,\\nFirst\\nquora\\ndataset\\nre-\\nlease:\\nQuestion\\npairs,\\nhttps://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs. 29\\n[322] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\\ncoreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\\n[323] M.-C. De Marneffe, M. Simons, J. Tonhauser, The commitmentbank: In-\\nvestigating projection in naturally occurring discourse, in: proceedings\\nof Sinn und Bedeutung, Vol. 23, 2019, pp. 107–124. 29\\n[324] Z. Li, N. Ding, Z. Liu, H. Zheng, Y. Shen, Chinese relation extraction\\nwith multi-grained information and external linguistic knowledge, in:\\nProceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics, 2019, pp. 4377–4386. 29\\n[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\\nand relation extraction dataset for chinese literature text, arXiv preprint\\narXiv:1711.07010 (2017). 29\\n[326] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\\nlarge-scale domain-specific chinese corpus for sentence semantic equiv-\\nalence identification, in: Proceedings of the 2018 conference on empiri-\\ncal methods in natural language processing, 2018, pp. 4946–4951. 29\\n[327] B. Liu, D. Niu, H. Wei, J. Lin, Y. He, K. Lai, Y. Xu, Matching arti-\\ncle pairs with graphical decomposition and convolutions, arXiv preprint\\narXiv:1802.07459 (2018). 29\\n[328] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Dataset and neu-\\nral recurrent sequence labeling model for open-domain factoid question\\nanswering, arXiv preprint arXiv:1607.06275 (2016). 29\\n[329] N. Peng, M. Dredze, Named entity recognition for chinese social media\\nwith jointly trained embeddings, in: Proceedings of the 2015 conference\\non empirical methods in natural language processing, 2015, pp. 548–\\n554. 29\\n[330] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\\nnale generation: Learning to solve and explain algebraic word problems,\\narXiv preprint arXiv:1705.04146 (2017). 29\\n[331] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\\nlease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\\ntium (2011). 29\\n[332] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\\ncomplex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\\n[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\\nin social media: A case study of african-american english, arXiv preprint\\narXiv:1608.08868 (2016). 29\\n[334] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, J. Allen, A corpus and evaluation framework\\nfor deeper understanding of commonsense stories, arXiv preprint\\narXiv:1604.01696 (2016). 28, 29\\n[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\\nWord prediction requiring a broad discourse context, arXiv preprint\\narXiv:1606.06031 (2016). 28, 29\\n[336] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\\nrization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\\n[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\\nation with planning-based hierarchical variational model, arXiv preprint\\narXiv:1908.06605 (2019). 29\\n[338] J. Novikova, O. Dušek, V. Rieser, The e2e dataset: New challenges for\\nend-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\\n[339] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\\nfor cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\\n[340] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about phys-\\nical commonsense in natural language, in: Proceedings of the AAAI\\nconference on artificial intelligence, Vol. 34, 2020, pp. 7432–7439. 28,\\n29\\n[341] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\\ndistantly supervised challenge dataset for reading comprehension, arXiv\\npreprint arXiv:1705.03551 (2017). 28, 29, 31\\n[342] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nO. Tafjord, Think you have solved question answering? try arc, the ai2\\nreasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 28, 29,\\n31\\n[343] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost:\\nPhys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29\\n[344] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering, arXiv\\npreprint arXiv:1809.02789 (2018). 29\\n[345] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\\nwebnlg+ shared task overview and evaluation results (webnlg+ 2020),\\nin: Proceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG+), 2020. 29\\n[346] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\\nA chinese dataset for cant understanding with common sense and world\\nknowledge, arXiv preprint arXiv:2104.02704 (2021). 29\\n[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\\nLarge-scale\\nreading comprehension dataset from examinations, arXiv preprint\\narXiv:1704.04683 (2017). 29\\n[348] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\\nL. Zettlemoyer, Quac: Question answering in context, arXiv preprint\\narXiv:1808.07036 (2018). 29\\n[349] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\\ntle use a laptop? a question answering benchmark with implicit reason-\\ning strategies, Transactions of the Association for Computational Lin-\\nguistics 9 (2021) 346–361. 29, 31\\n[350] J. Boyd-Graber, B. Satinoff, H. He, H. Daumé III, Besting the quiz mas-\\nter: Crowdsourcing incremental classification games, in: Proceedings of\\nthe 2012 joint conference on empirical methods in natural language pro-\\ncessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29\\n[351] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\\nquestion answer matching using end-to-end character-level multi-scale\\ncnns, Applied Sciences 7 (8) (2017) 767. 29\\n[352] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\\nteraction networks for chinese medical question answer selection, IEEE\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Access 6 (2018) 74061–74071. 29\\n[353] C. Xu, J. Pei, H. Wu, Y. Liu, C. Li, Matinf: A jointly labeled large-scale\\ndataset for classification, question answering and summarization, arXiv\\npreprint arXiv:2004.12302 (2020). 29\\n[354] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y. Choi, Winogrande: An\\nadversarial winograd schema challenge at scale, Communications of the\\nACM 64 (9) (2021) 99–106. 27, 29\\n[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a\\nmachine really finish your sentence?, arXiv preprint arXiv:1905.07830\\n(2019). 29\\n[356] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\\nnatives: An evaluation of commonsense causal reasoning., in: AAAI\\nspring symposium: logical formalizations of commonsense reasoning,\\n2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\\nlenge, in: Thirteenth international conference on the principles of knowl-\\nedge representation and reasoning, 2012. 27, 29\\n[358] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, arXiv preprint\\narXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:\\nCommonsense reasoning about social interactions, arXiv preprint\\narXiv:1904.09728 (2019). 29\\n[360] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\\nlenging chinese machine reading comprehension, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 141–155. 29\\n[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\\ning the gap between human and machine commonsense reading compre-\\nhension, arXiv preprint arXiv:1810.12885 (2018). 29\\n[362] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\\nfor machine comprehension of text, arXiv preprint arXiv:1606.05250\\n(2016). 29, 31\\n[363] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\\nK. Toutanova, Boolq: Exploring the surprising difficulty of natural\\nyes/no questions, arXiv preprint arXiv:1905.10044 (2019). 29, 31\\n[364] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\\nable questions for squad, arXiv preprint arXiv:1806.03822 (2018). 29,\\n31\\n[365] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\\nA reading comprehension benchmark requiring discrete reasoning over\\nparagraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\\n[366] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\\ntailment challenge, in: Machine learning challenges workshop, Springer,\\n2005, pp. 177–190. 29, 31\\n[367] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y. Bisk, Webqa: Mul-\\ntihop and multimodal qa, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31\\n[368] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\\nevaluation on chinese machine reading comprehension, arXiv preprint\\narXiv:1709.08299 (2017). 29\\n[369] Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\\nA span-extraction dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:1810.07366 (2018). 29, 31\\n[370] Y. Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\\nA sentence cloze dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:2004.03116 (2020). 29\\n[371] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, Y. Wang, Character-based bilstm-crf\\nincorporating pos and dictionaries for chinese opinion target extraction,\\nin: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29\\n[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\\ning beyond the surface: A challenge set for reading comprehension\\nover multiple sentences, in: Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, Volume 1 (Long Papers), 2018,\\npp. 252–262. 29\\n[373] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\\nberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\\ntions: a benchmark for question answering research, Transactions of the\\nAssociation for Computational Linguistics 7 (2019) 453–466. 29\\n[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-\\nchine reading comprehension dataset, arXiv preprint arXiv:1806.00920\\n(2018). 29\\n[375] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu,\\nQ. She, et al., Dureader: a chinese machine reading comprehension\\ndataset from real-world applications, arXiv preprint arXiv:1711.05073\\n(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A\\nchinese dataset towards evaluating the robustness of machine reading\\ncomprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\\n[377] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\\nquestions, arXiv preprint arXiv:1707.06209 (2017). 29\\n[378] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\\nranking with kernel pooling, in: Proceedings of the 40th International\\nACM SIGIR conference on research and development in information\\nretrieval, 2017, pp. 55–64. 29\\n[379] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, R. Morante,\\nQa4mre 2011-2013: Overview of question answering for machine read-\\ning evaluation, in: Information Access Evaluation. Multilinguality, Mul-\\ntimodality, and Visualization: 4th International Conference of the CLEF\\nInitiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\\nceedings 4, Springer, 2013, pp. 303–320. 29\\n[380] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\\nreading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\\n[381] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y. Feng, X. Han,\\nZ. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\\nment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\\n[382] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\\ncompetence with apps, arXiv preprint arXiv:2105.09938 (2021). 29, 31\\n[383] Y. Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\\nin: Proceedings of the 2017 conference on empirical methods in natural\\nlanguage processing, 2017, pp. 845–854. 29, 31\\n[384] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\\nto solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\\n29, 31\\n[385] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with\\nlarge language models, CoRR abs/2108.07732 (2021). 29\\n[386] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W.\\nChung, Y. Tay, S. Ruder, D. Zhou, et al., Language models are mul-\\ntilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\\n(2022). 29\\n[387] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\\npreprint arXiv:1608.01413 (2016). 29\\n[388] S.-Y. Miao, C.-C. Liang, K.-Y. Su, A diverse corpus for evaluating\\nand developing english math word problem solvers, arXiv preprint\\narXiv:2106.15772 (2021). 29\\n[389] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\\nMawps: A math word problem repository, in: Proceedings of the 2016\\nconference of the north american chapter of the association for computa-\\ntional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29\\n[390] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\\nsimple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\\n29\\n[391] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\\nD. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\\ndata science code generation, in: International Conference on Machine\\nLearning, PMLR, 2023, pp. 18319–18345. 29\\n[392] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\\nlanguage models, arXiv preprint arXiv:2108.07732 (2021). 29\\n[393] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\\nsarial nli: A new benchmark for natural language understanding, arXiv\\npreprint arXiv:1910.14599 (2019). 29, 31\\n[394] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\\ncorpus for sentence understanding through inference, arXiv preprint\\narXiv:1704.05426 (2017). 29\\n[395] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='nosing syntactic heuristics in natural language inference, arXiv preprint\\narXiv:1902.01007 (2019). 29\\n[396] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, Y. Zhang, Logiqa: A chal-\\nlenge dataset for machine reading comprehension with logical reason-\\ning, arXiv preprint arXiv:2007.08124 (2020). 29\\n[397] P. Lewis, B. O˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\\nuating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29\\n[398] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, V. Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\\nresentations, arXiv preprint arXiv:1809.05053 (2018). 29, 31\\n[399] Y. Yang,\\nY. Zhang,\\nC. Tar,\\nJ. Baldridge,\\nPaws-x:\\nA cross-\\nlingual adversarial dataset for paraphrase identification, arXiv preprint\\narXiv:1908.11828 (2019). 29, 31\\n[400] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\\nsummary!, Topic-Aware Convolutional Neural Networks for Extreme\\nSummarization. ArXiv, abs (1808). 29\\n[401] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli´c, A. Korhonen,\\nXcopa: A multilingual dataset for causal commonsense reasoning, arXiv\\npreprint arXiv:2005.00333 (2020). 29\\n[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\\nas a baseline for cross-lingual transfer in commonsense reasoning, arXiv\\npreprint arXiv:2106.12066 (2021). 29\\n[403] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Niko-\\nlaev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\\ntion answering in typologically diverse languages, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 454–470. 29\\n[404] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\\nMlsum:\\nThe multilingual summarization corpus,\\narXiv preprint\\narXiv:2004.14900 (2020). 29\\n[405] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\\nhuman falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29, 32\\n[406] I. Augenstein,\\nC. Lioma,\\nD. Wang,\\nL. C. Lima,\\nC. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc:\\nA real-world multi-domain\\ndataset for evidence-based fact checking of claims, arXiv preprint\\narXiv:1909.03242 (2019). 29\\n[407] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\\nlarge-scale dataset for fact extraction and verification, arXiv preprint\\narXiv:1803.05355 (2018). 29\\n[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\\nhate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\\n29, 32\\n[409] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\\nbias in pretrained language models, arXiv preprint arXiv:2004.09456\\n(2020). 29, 32\\n[410] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\\nquestion answering, arXiv preprint arXiv:2110.08193 (2021). 29\\n[411] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, K.-W. Chang, Gender bias\\nin coreference resolution: Evaluation and debiasing methods, arXiv\\npreprint arXiv:1804.06876 (2018). 29\\n[412] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\\nlenge dataset for measuring social biases in masked language models,\\narXiv preprint arXiv:2010.00133 (2020). 29\\n[413] S. Gehman, S. Gururangan, M. Sap, Y. Choi, N. A. Smith, Realtoxic-\\nityprompts: Evaluating neural toxic degeneration in language models,\\narXiv preprint arXiv:2009.11462 (2020). 29\\n[414] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\\nmetrics for measuring unintended bias with real data for text classifica-\\ntion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29\\n[415] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V. Logacheva, C. Monz, et al., Find-\\nings of the 2016 conference on machine translation, in: Proceedings of\\nthe First Conference on Machine Translation: Volume 2, Shared Task\\nPapers, 2016, pp. 131–198. 29\\n[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-\\nman, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\\n2020 conference on machine translation (wmt20), in: Proceedings of\\nthe Fifth Conference on Machine Translation, Association for Compu-\\ntational Linguistics„ 2020, pp. 1–55. 29\\n[417] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\\nmatching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\\n[418] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\\nwikipedia: Knowledge-powered conversational agents, arXiv preprint\\narXiv:1811.01241 (2018). 29\\n[419] H. Rashkin, E. M. Smith, M. Li, Y.-L. Boureau, Towards empathetic\\nopen-domain conversation models: A new benchmark and dataset, arXiv\\npreprint arXiv:1811.00207 (2018). 29\\n[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,\\nD. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\\ntional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\\ntion: From Machine Learning to Intelligent Conversations, Springer,\\n2020, pp. 187–208. 29\\n[421] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\\nmulti-domain dialogue dataset towards multi-turn knowledge-driven\\nconversation, arXiv preprint arXiv:2004.04100 (2020). 29\\n[422] L. CO, Iflytek: a multiple categories chinese text classifier. competition\\nofficial website (2019). 29\\n[423] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\\npushshift reddit dataset, in: Proceedings of the international AAAI con-\\nference on web and social media, Vol. 14, 2020, pp. 830–839. 30\\n[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\\nform question answering, arXiv preprint arXiv:1907.09190 (2019). 31\\n[425] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\\nBenchmarking generalization via in-context instructions on 1,600+ lan-\\nguage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\\n[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\\nM. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\\ntasking structured knowledge grounding with text-to-text language mod-\\nels, arXiv preprint arXiv:2201.05966 (2022). 31\\n[427] Q. Ye, B. Y. Lin, X. Ren, Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\\n(2021). 31\\n[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,\\nH. Zhuang, V. Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\\nmulti-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\\n(2021). 31\\n[429] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\\npus for sentence understanding through inference, in: Proceedings of\\nthe 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), Association for Computational Linguistics,\\nNew Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\\nN18-1101.\\nURL https://aclanthology.org/N18-1101 31\\n[430] Y. Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\\nscrambling, in: Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), Associa-\\ntion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\\n1298–1308. doi:10.18653/v1/N19-1131.\\nURL https://aclanthology.org/N19-1131 32\\n[431] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\\nGPT a general-purpose natural language processing task solver?, in: The\\n2023 Conference on Empirical Methods in Natural Language Process-\\ning, 2023.\\nURL https://openreview.net/forum?id=u03xn1COsO 32\\n[432] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\\nprehensive survey of its applications, challenges, limitations, and future\\nprospects, TechRxiv (2023). 32\\n[433] X. L. Dong, S. Moon, Y. E. Xu, K. Malik, Z. Yu, Towards next-\\ngeneration intelligent assistants leveraging llm techniques, in: Proceed-\\nings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, 2023, pp. 5792–5793. 32\\n[434] K. Pandya, M. Holia, Automating customer service using langchain:\\nBuilding custom open-source gpt chatbot for organizations, arXiv\\npreprint arXiv:2310.05421 (2023). 32\\n[435] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\\nR. Geng, et al., Can llm already serve as a database interface?\\na\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='big bench for large-scale database grounded text-to-sqls, arXiv preprint\\narXiv:2305.03111 (2023). 32\\n[436] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\\nchatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\\n2023–02. 32\\n[437] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\\nsir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\\nlanguage models for decision support in personalized oncology, JAMA\\nNetwork Open 6 (11) (2023) e2343689–e2343689. 32\\n[438] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\\nmaroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\\nploring the potential of chat-gpt as a supportive tool for sialendoscopy\\nclinical decision making and patient information support, European\\nArchives of Oto-Rhino-Laryngology (2023) 1–6. 32\\n[439] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\\nData decentralisation of llm-based chatbot systems in chronic disease\\nself-management, in: Proceedings of the 2023 ACM Conference on In-\\nformation Technology for Social Good, 2023, pp. 205–212. 32\\n[440] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\\nhuman feedback for a therapy chatbot application (2023). 32\\n[441] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\\nagents: A personalized llm-powered agent framework, arXiv preprint\\narXiv:2310.02374 (2023). 32\\n[442] K. V. Lemley, Does chatgpt help us understand the medical literature?,\\nJournal of the American Society of Nephrology (2023) 10–1681. 32\\n[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\\nnext-generation large language model (llm) or chatgpt is required for\\nbiomedical engineering and research, Annals of Biomedical Engineering\\n(2023) 1–4. 32\\n[444] Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\\ncalla dataset: Probing llms’ interactive knowledge acquisition from chi-\\nnese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\\n[445] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\\nS. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\\nlanguage models in medical education: Opportunities, challenges, and\\nfuture directions, JMIR Medical Education 9 (1) (2023) e48291. 32\\n[446] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\\nChatgpt passing usmle shines a spotlight on the flaws of medical educa-\\ntion (2023). 32\\n[447] S. Ahn, The impending impacts of large language models on medical\\neducation, Korean Journal of Medical Education 35 (1) (2023) 103. 32\\n[448] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\\n(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\\n(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\\nArtificial intelligence and public health: Evaluating chatgpt responses to\\nvaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 32\\n[450] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\\nTozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\\nai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32\\n[451] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\\nformance of chatgpt and other large language models (llm) for scientific\\nand research advancements: a double-edged sword, International Re-\\nsearch Journal of Modernization in Engineering Technology and Science\\n5 (10) (2023) 875–899. 32\\n[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large\\nlanguage models provide feedback to students? a case study on chatgpt,\\nin: 2023 IEEE International Conference on Advanced Learning Tech-\\nnologies (ICALT), IEEE, 2023, pp. 323–325. 32\\n[453] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\\nChatgpt for good? on opportunities and challenges of large language\\nmodels for education, Learning and individual differences 103 (2023)\\n102274. 32\\n[454] N. Rane, Enhancing the quality of teaching and learning through chat-\\ngpt and similar large language models: Challenges, future prospects,\\nand ethical considerations in education, Future Prospects, and Ethical\\nConsiderations in Education (September 15, 2023) (2023). 32\\n[455] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\\ngenerating chatbot’s dialogue for english as a foreign language learning,\\nInternational Journal of Advanced Computer Science and Applications\\n14 (6) (2023). 32\\n[456] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\\nthe impacts of chatgpt on future scientific work, SocArXiv (2023). 32\\n[457] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\\nscholarly writing: Is the integrity of the scientific discourse in jeopardy?,\\narXiv preprint arXiv:2311.06981 (2023). 32\\n[458] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\\nLarge language models for scientific synthesis, inference and explana-\\ntion, arXiv preprint arXiv:2310.07984 (2023). 33\\n[459] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\\nin scientific writing, PsyArXiv (2023). 33\\n[460] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\\nentific writing: a friend or a foe?, Reproductive BioMedicine Online\\n(2023). 33\\n[461] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\\nusing large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33\\n[462] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\\non learning mathematical reasoning with large language models, arXiv\\npreprint arXiv:2308.01825 (2023). 33\\n[463] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\\nR. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\\naugmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33\\n[464] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\\nT. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\\nlanguage models for mathematics through interactions, arXiv preprint\\narXiv:2306.01694 (2023). 33\\n[465] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He,\\nZ. Liu, et al., Summary of chatgpt-related research and perspective\\ntowards the future of large language models, Meta-Radiology (2023)\\n100017. 33\\n[466] J. Drápal, H. Westermann, J. Savelka, Using large language models\\nto support thematic analysis in empirical legal studies, arXiv preprint\\narXiv:2310.18729 (2023). 33\\n[467] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\\ning legal concepts with augmented large language models (gpt-4), arXiv\\npreprint arXiv:2306.09525 (2023). 33\\n[468] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\\nA. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\\nbench: A collaboratively built benchmark for measuring legal reasoning\\nin large language models, arXiv preprint arXiv:2308.11462 (2023). 33\\n[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases, arXiv\\npreprint arXiv:2306.16092 (2023). 33\\n[470] H. Yang, X.-Y. Liu, C. D. Wang, Fingpt: Open-source financial large\\nlanguage models, arXiv preprint arXiv:2306.06031 (2023). 33\\n[471] Y. Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\\nsurvey, in: Proceedings of the Fourth ACM International Conference on\\nAI in Finance, 2023, pp. 374–382. 33\\n[472] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model, arXiv preprint\\narXiv:2305.19352 (2023). 33\\n[473] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\\naction, in: ACM/IEEE International Conference on Human-Robot Inter-\\naction, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33\\n[474] Y. Ye, H. You, J. Du, Improved trust in human-robot collaboration with\\nchatgpt, IEEE Access (2023). 33\\n[475] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\\nknowledge from large language models for task and motion planning,\\nin: RSS 2023 Workshop on Learning for Task and Motion Planning,\\n2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\\nwith large language models, arXiv preprint arXiv:2305.05658 (2023).\\n33\\n[477] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\\nfor deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 34\\n[478] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='gers of stochastic parrots: Can language models be too big?, in: Pro-\\nceedings of the 2021 ACM conference on fairness, accountability, and\\ntransparency, 2021, pp. 610–623. 34\\n[479] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\\ndeep learning (still) requires rethinking generalization, Communications\\nof the ACM 64 (3) (2021) 107–115. 34\\n[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\\ntrained language models, arXiv preprint arXiv:2105.00828 (2021). 34\\n[481] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\\nNow (2019) 1–33. 34\\n[482] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\\nguage models still can’t plan (a benchmark for llms on planning and\\nreasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\\n[483] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen, et al., Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models, arXiv preprint arXiv:2309.01219\\n(2023). 34\\n[484] A. Webson, E. Pavlick, Do prompt-based models really understand the\\nmeaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\\n[485] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\\nthought, let’s not think step by step! bias and toxicity in zero-shot rea-\\nsoning, arXiv preprint arXiv:2212.08061 (2022). 34\\n[486] B. C. Das, M. H. Amini, Y. Wu, Security and privacy challenges of large\\nlanguage models: A survey, arXiv preprint arXiv:2402.00888 (2024). 34\\n[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-\\nial training for large neural language models, ArXiv (April 2020).\\nURL\\nhttps://www.microsoft.com/en-us/research/\\npublication/adversarial-training-for-large-neural-language-models/\\n34\\n[488] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu-\\nGhazaleh, Survey of vulnerabilities in large language models revealed\\nby adversarial attacks (2023). arXiv:2310.10844. 34\\n[489] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\\nllm can fool itself: A prompt-based adversarial attack (2023). arXiv:\\n2310.13345. 34\\n[490] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nM. Du, Explainability for large language models: A survey (2023).\\narXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large\\nlanguage models explain themselves? a study of llm-generated self-\\nexplanations (2023). arXiv:2310.11207. 35\\n[492] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\\nmean for a language model to preserve privacy?, in: Proceedings of the\\n2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35\\n[493] R. Plant, V. Giuffrida, D. Gkatzia, You are what you write:\\nPre-\\nserving privacy in the era of large language models, arXiv preprint\\narXiv:2204.09391 (2022). 35\\n[494] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\\nB. Ren, Y. Wang, Real-time execution of large-scale language models\\non mobile (2020). arXiv:2009.06823. 35\\n[495] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo,\\nY. Zhu, Olive:\\nAccelerating large language models via hardware-\\nfriendly outlier-victim pair quantization, in: Proceedings of the 50th\\nAnnual International Symposium on Computer Architecture, 2023, pp.\\n1–15. 35\\n[496] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\\nlanguage models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35\\n[497] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\\nand policy implications for large language models: Guiding responsible\\ndevelopment and deployment, arXiv preprint arXiv:2308.02678 (2023).\\n35\\n[498] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\\nmodels: a three-layered approach, AI and Ethics (2023) 1–31. 35\\n47')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "## Load all the pdf files from dir\n",
    "dir_loader=DirectoryLoader( \n",
    "    \"../data/pdf\", \n",
    "    glob=\"**/*.pdf\", \n",
    "    loader_cls= PyMuPDFLoader, \n",
    "    show_progress=False \n",
    ") \n",
    "\n",
    "pdf_documents=dir_loader.load() \n",
    "pdf_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "144c1b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3a458c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked 47 documents into 703 chunks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='dUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='jThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='nication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='capabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='Language Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\nWizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020\\n2021\\n2022\\n2023\\n2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='Codex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\nPaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='Mixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='and open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs.\\nWhile'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='from pre-trained language models (PLMs) to LLMs.\\nWhile\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='The larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='proposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot.\\nFine-tuning them with task instruc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='tings than in few-shot.\\nFine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='planning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.\\nThese abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='Various improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='opportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41].\\nParam-\\neter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='research literature has recently experienced a large influx of\\nLLM-related contributions.\\nResearchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='cle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='providing a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners effec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='tively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='lowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='uration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='bols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other.\\nMoreover, the attention mod-\\nule in the transformer does not capture positional information.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='embeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='which decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='weight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='ity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs.\\nTo speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='and the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0, x)\\n(1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='GLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product (⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\\n(2)\\nwhere X is the input of layer and l, W, b, V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\\nS wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='Pre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='vices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='allelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='large-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can differenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write efficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='training code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='a pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based.\\nClassifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='for filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can affect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources.\\nThis data contains private\\ninformation; therefore, many LLMs employ heuristics-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The difference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='Figure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output.\\nHere, the encoder sees the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='decoder to generate the output.\\nHere, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='steps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an efficient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='tention block [90]. Mixture-of-Experts (MoE) is an efficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives.\\nFor\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='For\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='Masked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='attention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='dataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of different training stages and in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='and utilization. An example of different training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='kens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with different\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='prompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='asking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='model on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='responses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='will discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='model to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning.\\nGenerating\\nreasons is possible only by using different prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='whereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='able in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='prompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='and are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='tional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu-α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='ulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='non-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nhWk\\nhTHT\\nL\\n(3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='to initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='at a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='jointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model.\\nThe training vocabulary of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='parameter J1-Jumbo model.\\nThe training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='based on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='Filtering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='tensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the effect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='on the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='facts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='NeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='is difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF(LN2(x))\\n(4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='employs dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with differences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='bedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90].\\nTo gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='are sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7× larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3× GPT-3 model parameters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='2 architecture that has roughly 3× GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little different data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='ture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='lion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='trained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='PaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='from a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='training and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='significantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R, S, X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='mance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='unidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='the most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='safer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-α and extended to a trillion scale with Ran-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='copied from PanGu-α and extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='tivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128×3.66B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='mixture-of-experts (MoE) architecture. The MoE (128×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='eters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='context length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='context window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='ducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='used to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='batch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector.\\nMLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='3.1.2. Coding\\nCodeGen [140]:\\nCodeGen has a similar architecture to\\nPaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\\nbeddings. The model is trained on both natural language and\\nprogramming language data sequentially (trained on the first\\ndataset, then the second, and so on) on the following datasets\\n1) PILE, 2) BIGQUERY, and 3) BIGPYTHON. CodeGen pro-\\nposed a multi-step approach to synthesizing code. The purpose\\nis to simplify the generation of long sequences where the previ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='is to simplify the generation of long sequences where the previ-\\nous prompt and generated code are given as input with the next\\nprompt to generate the next code sequence. CodeGen open-\\nsource a Multi-Turn Programming Benchmark (MTPB) to eval-\\nuate multi-step program synthesis.\\nCodex [141]: This LLM is trained on a subset of public Python\\nGithub repositories to generate code from docstrings. Com-\\nputer programming is an iterative process where the programs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='puter programming is an iterative process where the programs\\nare often debugged and updated before fulfilling the require-\\nments. Similarly, Codex generates 100 versions of a program\\nby repetitive sampling for a given description, which produces\\na working solution for 77.5% of the problems passing unit tests.\\nIts powerful version powers Github Copilot2.\\nAlphaCode [142]: A set of large language models, ranging\\nfrom 300M to 41B parameters, designed for competition-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='from 300M to 41B parameters, designed for competition-level\\ncode generation tasks. It uses the multi-query attention [143] to\\nreduce memory and cache costs. Since competitive program-\\nming problems highly require deep reasoning and an under-\\nstanding of complex natural language algorithms, the Alpha-\\nCode models are pre-trained on filtered GitHub code in popular\\nlanguages and then fine-tuned on a new competitive program-\\nming dataset named CodeContests. The CodeContests dataset'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='ming dataset named CodeContests. The CodeContests dataset\\nmainly contains problems, solutions, and test cases collected\\nfrom the Codeforces platform3. The pre-training employs stan-\\ndard language modeling objectives, while GOLD [144] with\\ntempering [145] serves as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='on the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\nCodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with\\nshallow encoder and deep decoder, trained in multiple stages\\ninitially unimodal data (code) and later bimodal data (text-code\\npairs). Each training stage has different training objectives and\\nactivates different model blocks encoder, decoder, or both ac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='activates different model blocks encoder, decoder, or both ac-\\ncording to the task. The unimodal pre-training includes span\\ndenoising and CLM objectives, whereas bimodal pre-training\\nobjectives contain contrastive learning, matching, and CLM for\\ntext-code pairs. CodeT5+ adds special tokens with the text to\\nenable task modes, for example, [CLS ] for contrastive loss,\\n[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder\\narchitecture, employing Flash attention to scale up the context\\nlength to 8k. The StarCoder trains an encoder to filter names,\\n2https://github.com/features/copilot\\n3https://codeforces.com/\\nemails, and other personal data from the training data. Its fine-\\ntuned variant outperforms PaLM, LLaMA, and LAMDA on\\nHumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='HumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge\\nGalactica [148]: A large curated corpus of human scientific\\nknowledge with 48 million papers, textbooks, lecture notes,\\nmillions of compounds and proteins, scientific websites, en-\\ncyclopedias, and more are trained using the metaseq library3,\\nwhich is built on PyTorch and fairscale [149]. The model wraps\\nreasoning datasets with the < work > token to provide step-by-\\nstep reasoning context to the model, which has been shown to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='step reasoning context to the model, which has been shown to\\nimprove the performance on reasoning tasks.\\n3.1.4. Dialog\\nLaMDA [150]: A decoder-only model pre-trained on pub-\\nlic dialog data, public dialog utterances, and public web doc-\\numents, where more than 90% of the pre-training data is in\\nEnglish. LaMDA is trained with the objective of producing re-\\nsponses that exhibit high levels of quality, safety, and grounded-\\nness. To achieve this, discriminative and generative fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='ness. To achieve this, discriminative and generative fine-tuning\\ntechniques are incorporated to enhance the model’s safety and\\nquality aspects. As a result, the LaMDA models can be utilized\\nas a general language model performing various tasks.\\n3.1.5. Finance\\nBloombergGPT [151]: A non-causal decoder model trained\\nusing both financial (“FINPILE” from the Bloomberg archive)\\nand general-purpose datasets. The model’s architecture is sim-\\nilar to the BLOOM [13] and OPT [14]. It allocates 50B param-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='ilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\\neters to different blocks of the model using the approach [113].\\nFor effective training, BloombergGPT packs documents to-\\ngether with < |endoftext| > to use the maximum sequence\\nlength, uses warmup batch size starting from 1024 to 2048, and\\nmanually reduces the learning rate multiple times during the\\ntraining.\\nXuan Yuan 2.0 [152]: A Chinese financial chat model with\\nBLOOM’s [13] architecture trained on a combination of general'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='BLOOM’s [13] architecture trained on a combination of general\\npurpose, financial, general purpose instructions, and financial\\ninstitutions datasets. Xuan Yuan 2.0 combined the pre-training\\nand fine-tuning stages to avoid catastrophic forgetting.\\n3.2. Fine-Tuned LLMs\\nPre-trained LLMs have excellent generalization abilities to\\nunseen tasks. However, because they are generally trained with\\nthe objective of next token prediction, LLMs have limited ca-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='the objective of next token prediction, LLMs have limited ca-\\npacity to follow user intent and are prone to generate unethical,\\ntoxic or inaccurate responses [20]. For their effective utiliza-\\ntion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\\ngenerate safe responses [20], which also results in increasing\\nzero-shot, few-shot, and cross-task generalization [97, 16, 18],\\nwith minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='with minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].\\nWe review various fine-tuned LLMs and strategies for effective\\nfine-tuning in this section.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\\nModels\\nFindings & Insights\\nT5\\n• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\\nclassification layers\\nGPT-3\\n• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\\nlearners\\nmT5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='learners\\nmT5\\n• Large multi-lingual models perform equivalently to single language models on downstream tasks.\\nHowever, smaller multi-lingual models perform worse\\nPanGu-α\\n• LLMs have good few shot capabilities\\nCPM-2\\n• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\\nble to full model fine-tuning\\n• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n• Inserting prompt tokens in-between sentences can allow the model to understand relations between\\nsentences and long sequences\\n• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\\n(aggregate information with the input text) for the model\\nERNIE 3.0\\n• A modular LLM architecture with a universal representation module and task-specific representa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• A modular LLM architecture with a universal representation module and task-specific representa-\\ntion module helps in the finetuning phase\\n• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\\nan efficient way to take advantage of the powerful pre-trained model\\nJurassic-1\\n• The performance of LLM is highly related to the network size\\n• To improve runtime performance, more operations can be performed in parallel (width) rather than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• To improve runtime performance, more operations can be performed in parallel (width) rather than\\nsequential (depth)\\n• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\\ncabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\\nbenefits in few-shot learning tasks\\nHyperCLOVA\\n• By employing prompt-based tuning, the performances of models can be improved, often surpassing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• By employing prompt-based tuning, the performances of models can be improved, often surpassing\\nthose of state-of-the-art models when the backward gradients of inputs are accessible\\nYuan 1.0\\n• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\\nbehavior in zero-shot and few-shot learning\\nGopher\\n• Relative encodings enable the model to evaluate for longer sequences than training.\\nERNIE 3.0 Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='ERNIE 3.0 Titan\\n• Additional self-supervised adversarial loss to distinguish between real and generated text improves\\nthe model performance as compared to ERNIE 3.0\\nGPT-NeoX-20B\\n• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\\nlayers\\n• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations\\nfrom growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='from growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot\\nTable Continued on Next Page\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='Models\\nFindings & Insights\\nOPT\\n• Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n• Model is prone to generate repetitive text and stuck in a loop\\nGalactica\\n• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\\ndomain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\\nresearch on LLMs\\n• A working memory token approach can achieve strong performance over existing methods on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='• A working memory token approach can achieve strong performance over existing methods on\\nmathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\\ntasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\nGLaM\\n• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\\nin each transformer layer with a mixture-of-experts (MoE)\\n• The model trained on filtered data shows consistently better performances on both NLG and NLU'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='• The model trained on filtered data shows consistently better performances on both NLG and NLU\\ntasks, where the effect of filtering is more significant on the former tasks\\n• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\\nthe downstream tasks\\n• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\\nthe MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\\nmance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='mance\\nLaMDA\\n• The model can be fine-tuned to learn to call different external information resources and tools\\nAlphaCode\\n• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed\\nwith a shallower encoder and a deeper decoder\\n• To achieve better performances, it is necessary to employ strategies such as massively scaling\\nupsampling, followed by the filtering and clustering of samples into a compact set'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='upsampling, followed by the filtering and clustering of samples into a compact set\\n• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-\\nscale sampling is crucial\\n• Simplifying problem descriptions can effectively improve the model’s performance\\nChinchilla\\n• The model size and the number of training tokens should be scaled proportionately: for each dou-\\nbling of the model size, the number of training tokens should be doubled as well\\nPaLM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='bling of the model size, the number of training tokens should be doubled as well\\nPaLM\\n• English-centric models produce better translations when translating to English as compared to non-\\nEnglish\\n• Generalized models can have equivalent performance for language translation to specialized small\\nmodels\\n• Larger models have a higher percentage of training data memorization\\n• Performance has not yet saturated even at 540B scale, which means larger models are likely to\\nperform better\\nAlexaTM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='perform better\\nAlexaTM\\n• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\\ncontext than decoder-only\\n• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\\nlearning\\n• Placing layer norm at the beginning of each transformer layer improves the training stability\\nTable Continued on Next Page\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='Models\\nFindings & Insights\\nU-PaLM\\n• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\\ndiversity\\nUL2\\n• Mode switching training enables better performance on downstream tasks\\n• CoT prompting outperforms standard prompting for UL2\\nGLM-130B\\n• Pre-training data with a small proportion of multi-task instruction data improves the overall model\\nperformance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='performance\\nCodeGen\\n• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\\neration\\nLLaMA\\n• A constant performance improvement is observed when scaling the model\\n• Smaller models can achieve good performances with more training data and computing time\\nPanGu-Σ\\n• Sparse models provide the benefits of large models at a lower computation cost\\n• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for\\ncontinual learning\\n• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\\ncost-efficient while maintaining a performance similar to the original\\nBloombergGPT\\n• Pre-training with general-purpose and task-specific data improves task performance without hurt-\\ning other model capabilities\\nXuanYuan 2.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='ing other model capabilities\\nXuanYuan 2.0\\n• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5+\\n• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\\nfor better performance\\nStarCoder\\n• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2\\n• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\\nfine-tuning\\n• Model trained on unfiltered data requires fewer samples for safety alignment\\nPaLM-2\\n• Data quality is important to train better models\\n• Model and data size should be scaled with 1:1 proportions\\n• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1\\n• Increasing batch size gradually stabilizes the training without loss spikes\\n• High-quality data at the final stages of training improves the model performance\\n• Increasing model context length windows step-wise allows it to better adapt to various sequence\\nlengths\\nNemotron-40B\\n• Model aligned iteratively on synthetic data with data generated from the previously aligned model\\nachieves competitive performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='achieves competitive performance\\nDeepSeek\\n• Batch size should increase with the increase in compute budget while decreasing the learning rate\\nDeepSeek-v2\\n• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring\\na significantly smaller KV cache, therefore achieving faster data generation\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels\\nFindings & Insights\\nT0\\n• Multi-task prompting enables zero-shot generalization and outperforms baselines\\n• Even a single prompt per dataset task is enough to improve performance\\nWebGPT\\n• To aid the model in effectively filtering and utilizing relevant information, human labelers play a\\ncrucial role in answering questions regarding the usefulness of the retrieved documents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='crucial role in answering questions regarding the usefulness of the retrieved documents\\n• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\\nend-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n• Generating answers with references can make labelers easily judge the factual accuracy of answers\\nTk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='Tk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks\\n• More tasks improve generalization whereas only increasing task instances does not help\\n• Supervised trained models are better than generalized models\\n• Models pre-trained with instructions and examples perform well for different types of inputs\\nmT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='mT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before\\n• Multi-lingual training leads to even better zero-shot generalization for both English and non-\\nEnglish\\n• Training on machine-translated prompts improves performance for held-out tasks with non-English\\nprompts\\n• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\\nother pre-trained language tasks\\nOPT-IML'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='other pre-trained language tasks\\nOPT-IML\\n• Creating a batch with multiple task examples is important for better performance\\n• Only example proportional sampling is not enough, training datasets should also be proportional\\nfor better generalization/performance\\n• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\\nwhereas fully supervised tasks have no effect\\n• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\\n• Only 1% reasoning data improves the performance, adding more deteriorates performance\\n• Adding dialogue data makes the performance worse\\nSparrow\\n• Labelers’ judgment and well-defined alignment rules help the model generate better responses\\n• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\\nraters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='raters\\n• The combination of reinforcement learning (RL) with reranking yields optimal performance in\\nterms of preference win rates and resilience against adversarial probing\\nFlan\\n• Finetuning with CoT improves performance on held-out tasks\\n• Fine-tuning along with CoT data improves reasoning abilities\\n• CoT tuning improves zero-shot reasoning\\n• Performance improves with more tasks\\n• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n• Improving the model’s performance with instruction tuning is compute-efficient\\n• Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder\\n• Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\nLLaMA-2-Chat\\n• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\\nRLHF step further improves model safety and make it less prone to jailbreak attacks\\nLIMA\\n• Less high quality data is enough for fine-tuned model generalization\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Figure 10: This example illustrates the PanGu-P architecture, as depicted in\\nthe image sourced from [92].\\n3.2.1. Instruction-Tuning with Manually Created Datasets\\nNumerous hand-crafted instruction-tuning datasets with\\ndifferent design choices are proposed in the literature to\\ninstruction-tune LLMs. The performance of fine-tuned LLMs\\ndepends on multiple factors, such as dataset, instruction diver-\\nsity, prompting templates, model size, and training objectives.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='sity, prompting templates, model size, and training objectives.\\nKeeping this in view, diverse fine-tuned models have emerged\\nin the literature using manually created datasets.\\nThe models T0 [17] and mT0 (multi-lingual) [154] employ\\ntemplates to convert existing datasets into prompt datasets.\\nThey have shown improvements in generalization to zero-shot\\nand held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\\nwith in-context instructions to study generalization on unseen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='with in-context instructions to study generalization on unseen\\ntasks when given in-context instructions during test time. The\\nmodel outperformed Instruct-GPT, despite being smaller in\\nsize, i.e., 11B parameters as compared to 175B of GPT-3.\\nIncreasing Tasks and Prompt Setups: Zero-shot and few-shot\\nperformance improves significantly by expanding task collec-\\ntion and prompt styles. OPT-IML [97] and Flan [16] curated\\nlarger 2k and 1.8k task datasets, respectively. While increasing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='larger 2k and 1.8k task datasets, respectively. While increasing\\ntask size alone is not enough, OPT-IML and Flan add more\\nprompting setups in their datasets, zero-shot, few-shot, and\\nCoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\\nfurther on 1.88M CoT samples. Another method [102] uses\\nsymbolic tasks with tasks in T0, Flan, etc.\\n3.2.2. Instruction-Tuning with LLMs Generated Datasets\\nGenerating an instruction-tuning dataset requires carefully'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Generating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are often\\nwritten by humans, smaller in size, and less diverse. To over-\\ncome this, self-instruct [19] proposed an approach to prompt\\navailable LLMs to generate instruction-tuning datasets. Self-\\ninstruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\\ninstruction, and 1 sample per task and iteratively generates new\\ninstructions (52k) and instances (82k input-output pairs) using\\nFigure 11: An example image shows an instance of the Flan training paradigm,\\ntaken from [16].\\nGPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data\\nof datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='of datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.\\nLLaMA Tuned: Various models in the literature instruction-\\ntune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-\\nated datasets.\\nAmong these, Alpaca [158], Vicuna [159],\\nand LLaMA-GPT-4 [160] are a few general-purpose fine-tuned\\nmodels, where Alpaca is trained on 52k samples from text-\\ndavinci-003, Vicuna on 70k samples from ShareGPT.com, and\\nLLaMA-GPT-4 by re-creating Alpaca instructions from GPT-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\\n4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million\\nsamples) by generating data from ChatGPT and outperforms\\nGPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the\\nLLaMA’s consistent tokenization of numbers. HuaTuo [162] is\\na medical knowledge model, fine-tuned with a generated QA\\ndataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='dataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs\\nto convert given instructions into a more complex set. The in-\\nstructions are iteratively evolved with re-writing instructions in\\ncomplex wording and creating new instructions. With this style\\nof automated instruction generation, WizardLM [163] (fine-\\ntuned LLaMA on 250k instructions), outperforms Vicuna and\\nAlpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.\\n3.2.3. Aligning with Human Preferences\\nIncorporating human preferences into LLMs presents a\\nsignificant advantage in mitigating undesirable behaviors and\\nensuring accurate outputs. The initial work on alignment, such\\nas InstructGPT [20] aligns GPT-3 using a 3-step approach,\\ninstruction-tuning, reward modeling, and fine-tuning with\\nreinforcement learning (RL). The supervised fine-tuned GPT-3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='reinforcement learning (RL). The supervised fine-tuned GPT-3\\non demonstrations is queried to generate responses, which\\nhuman labelers rank according to human values, and a reward\\nmodel is trained on the ranked data. Lastly, the GPT-3 is trained\\nwith proximal policy optimization (PPO) using rewards on the\\ngenerated data from the reward model. LLaMA 2-Chat [21]\\nimproves alignment by dividing reward modeling into help-\\nfulness and safety rewards and using rejection sampling in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='fulness and safety rewards and using rejection sampling in\\naddition to PPO. The initial four versions of LLaMA 2-Chat\\nare fine-tuned with rejection sampling and then with PPO on\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='top of rejection sampling.\\nAligning with Supported Evidence: This style of alignment\\nallows the model to generate responses with proofs and facts,\\nreduces hallucination, and assists humans more effectively,\\nwhich increases trust in the model’s output.\\nSimilar to\\nthe RLHF training style, a reward model is trained to rank\\ngenerated responses containing web citations in answers\\nto questions, which is later used to train the model, as in\\nGopherCite [165], WebGPT [166], and Sparrow [167]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='GopherCite [165], WebGPT [166], and Sparrow [167]. The\\nranking model in Sparrow [167] is divided into two branches,\\npreference reward and rule reward, where human annotators\\nadversarial probe the model to break a rule. These two rewards\\ntogether rank a response to train with RL.\\nAligning Directly with SFT: The PPO in the RLHF pipeline\\nis complex, memory-intensive, and unstable, requiring mul-\\ntiple models, reward, value, policy, and reference models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='tiple models, reward, value, policy, and reference models.\\nAvoiding this sophisticated alignment pipeline is possible by\\nincorporating minimal changes in the supervised fine-tuning\\n(SFT) pipeline as in [168, 169, 170], with better or compa-\\nrable performance to PPO. Direct preference optimization\\n(DPO) [168] trains a model directly on the human-preferred\\nresponses to maximize the likelihood of preferred against\\nunpreferred responses, with per-sample importance weight.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='unpreferred responses, with per-sample importance weight.\\nReward ranked fine-tuning RAFT [169] fine-tunes the model\\non ranked responses by the reward model. Preference ranking\\noptimization (PRO) [171] and RRHF [170] penalize the model\\nto rank responses with human preferences and supervised loss.\\nOn the other hand, chain-of-hindsight (CoH) [172] provides\\nfeedback to the model in language rather than reward, to learn\\ngood versus bad responses.\\nAligning with Synthetic Feedback:\\nAligning LLMs with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='good versus bad responses.\\nAligning with Synthetic Feedback:\\nAligning LLMs with\\nhuman feedback is slow and costly. The literature suggests a\\nsemi-automated process to align LLMs by prompting LLMs to\\ngenerate helpful, honest, and ethical responses to the queries,\\nand fine-tuning using the newly created dataset. Constitutional\\nAI [173] replaces human feedback in RLHF with AI, calling\\nit RL from AI feedback (RLAIF). AlpacaFarm [174] designs\\nprompts to imitate human feedback using LLMs APIs. Oppo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='prompts to imitate human feedback using LLMs APIs. Oppo-\\nsite to constitutional AI, AlpacaFarm injects noise in feedback\\nto replicate human mistakes.\\nSelf-Align [98] prompts the\\nLLM with ICL examples, instructing the LLM about what the\\nresponse should contain to be considered useful and ethical.\\nThe same LLM is later fine-tuned with the new dataset.\\nAligning with Prompts: LLMs can be steered with prompts to\\ngenerate desirable responses without training [175, 176]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='generate desirable responses without training [175, 176]. The\\nself-correction prompting in [176] concatenates instructions\\nand CoT with questions, guiding the model to answer its\\ninstruction following a strategy to ensure moral safety before\\nthe actual answer. This strategy is shown to reduce the harm in\\ngenerated responses significantly.\\nRed-Teaming/Jailbreaking/Adversarial\\nAttacks:\\nLLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='Attacks:\\nLLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-\\nformation, and other shortcomings through adversarial probing.\\nThe models are susceptible to generating harmful responses\\neven though they are aligned for safety [177, 178].\\nRed-\\nteaming is a common approach to address illicit outputs, where\\nthe LLMs are prompted to generate harmful outputs [178, 179].\\nThe dataset collected through red-teaming is used to fine-tune'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='The dataset collected through red-teaming is used to fine-tune\\nmodels for safety. While red-teaming largely relies on human\\nannotators, another work [180] red-team LLMs to find prompts\\nthat lead to harmful outputs for other LLMs.\\n3.2.4. Continue Pre-Training\\nAlthough fine-tuning boosts a model’s performance, it leads\\nto catastrophic forgetting of previously learned information.\\nConcatenating fine-tuning data with a few randomly selected'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='Concatenating fine-tuning data with a few randomly selected\\npre-training samples in every iteration avoids network forget-\\nting [181, 152]. This is also effective in adapting LLMs for\\ncases where fine-tuning data is small and the original capac-\\nity is to be maintained. Prompt-based continued pre-training\\n(PCP) [182] trains the model with text and instructions related\\nto tasks and then finally instruction-tunes the model for down-\\nstream tasks.\\n3.2.5. Sample Efficiency'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='stream tasks.\\n3.2.5. Sample Efficiency\\nWhile fine-tuning data is generally many-fold smaller than\\nthe pre-training data, it still has to be large enough for accept-\\nable performance [16, 97, 18] and requires proportional com-\\nputing resources. Studying the effects on performance with less\\ndata, existing literature [183, 184] finds that models trained\\non less data can outperform models trained with more data.\\nIn [183], 25% of the total downstream data is found enough'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='In [183], 25% of the total downstream data is found enough\\nfor state-of-the-art performance. Selecting coreset-based 0.5%\\nof the total instruction-tuning data improves the model perfor-\\nmance by 2% in [184], as compared to the complete data tun-\\ning. Less is more for alignment (LIMA) [185] uses only 1000\\ncarefully created demonstrations to fine-tune the model and has\\nachieved comparable performance to GPT-4.\\n3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-\\npensive attention and high memory requirements.\\nA model\\ntrained on limited sequence lengths fails to generalize to unseen\\nlengths at inference time [186, 49]. Alternatively, LLMs with\\nALiBi [65] positional encodings can perform zero-shot length\\nextrapolation. However, ALiBi has less expressive power [66]\\nand inferior performance on multiple benchmarks [46], and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='and inferior performance on multiple benchmarks [46], and\\nmany LLMs use RoPE positional embedding that is unable to\\nperform zero-shot extrapolation. A larger context length has\\nbenefits such as a better understanding of longer documents,\\nmore samples in in-context learning, execution of bigger rea-\\nsoning processes, etc. Expanding context length during fine-\\ntuning is slow, inefficient, and computationally expensive [49].\\nTherefore, researchers employ various context window extrap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='Therefore, researchers employ various context window extrap-\\nolation techniques discussed below.\\nPosition Interpolation: Rather than extrapolating, [49] shows\\nthat interpolating position encodings within the pre-trained con-\\ntext window are more effective. The work demonstrates that\\nonly 1000 steps of fine-tuning are enough to achieve better re-\\nsults on larger windows without reducing performance com-\\npared to the original context size. Giraffe [46] uses power scal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='pared to the original context size. Giraffe [46] uses power scal-\\ning in RoPE, and YaRN [47] proposed NTK-aware interpola-\\ntion.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='Efficient Attention Mechanism:\\nDense global attention is\\none of the major constraints in training larger context win-\\ndow LLMs.\\nUsing efficient attention variants, such as lo-\\ncal, sparse, and dilated attention, reduces the computation cost\\nsignificantly.\\nLongT5 [48] proposes transient global atten-\\ntion (TGlobal), applying attention to local and global tokens\\n(windowed token averaging).\\nThe model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='The model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on\\n4098 sequence length, fine-tunes on larger window sizes, as\\nlarge as 16k, and improves task performance on longer inputs.\\nThis shows the extrapolation ability of TGlobal attention with\\nonly fine-tuning. COLT5 [187] uses two branches, one with\\nlightweight and the other with heavyweight attention and feed-\\nforward layers. All tokens are processed from the lightweight'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='forward layers. All tokens are processed from the lightweight\\nbranch, and only important tokens are routed to the heavy-\\nweight branch. LongNet [188] replaces standard attention with\\ndilated attention, expanding sequence length to 1 billion tokens.\\nLongLoRA [189] proposes shift-short attention, used during\\nfine-tuning to reduce dense attention costs. However, the model\\nduring inference uses dense attention and achieves similar per-\\nformance as full attention fine-tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='formance as full attention fine-tuning.\\nExtrapolation without Training: LM-Infinite [186] and par-\\nallel context windows (PCW) [190] show length extrapolation\\nis possible using pre-trained LLMs. LM-Infinite suggested Λ-\\nshaped attention applied within the original context window\\nlimits. Likewise, PCW chunks larger inputs into the pre-trained\\ncontext lengths and applies the same positional encodings to\\neach chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='each chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-\\nnated with the input, known as context augmentation, in-\\ncontext learning (ICL), or few-shot prompting. They show ex-\\ncellent generalization to unseen tasks with few-shot prompt-\\ning, enabling LLMs to answer queries beyond the capacity ac-\\nquired during training [6, 55]. These emergent abilities allow\\nfor adapting the model without fine-tuning—a costly process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='for adapting the model without fine-tuning—a costly process.\\nAside from this, hallucination, producing inaccurate, unsafe,\\nor factually incorrect responses, is common for LLMs, which is\\navoided by augmenting contextual data. While the user can pro-\\nvide in-context samples in the query [54, 32], here we specifi-\\ncally refer to the methods that access external storage program-\\nmatically, calling them augmented LLMs.\\nThe literature suggests various external memory designs to aug-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='The literature suggests various external memory designs to aug-\\nment LLMs, long-term [191, 192, 193, 194], short-term [195],\\nsymbolic [196], and non-symbolic [197, 198]. The memory\\ncan be maintained in different formats such as documents, vec-\\ntors, or databases. A few systems maintain intermediate mem-\\nory representations to retain information across multiple iter-\\nations [194, 192], while others extract important information\\nfrom the datasets and save it in memory for recall [199]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='from the datasets and save it in memory for recall [199]. The\\nmemory read and write operations are performed either with\\nor without LLMs cooperation [192, 200, 194, 201], acting as\\na feedback signal in [195]. We discuss different types of aug-\\nmented LLMs below.\\nFigure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\\ntracts a similar context to the input and forwards it to the LLM either in simple\\nlanguage or encoded through Fusion-in-Decoder (FiD). Depending on the task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='language or encoded through Fusion-in-Decoder (FiD). Depending on the task,\\nretrieval and generation may repeat multiple times.\\n3.4.1. Retrieval Augmented LLMs\\nLLMs may have limited memory and outdated information,\\nleading to inaccurate responses. Retrieving relevant informa-\\ntion from external up-to-date storage enables the LLMs to\\naccurately answer with references and utilize more informa-\\ntion. With retrieval augmentation, smaller models have been'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='tion. With retrieval augmentation, smaller models have been\\nshown to perform at par with larger models. For instance, the\\n11B model can become competitive to 540B PaLM in [25] and\\n7.5B to 280B Gopher in [193]. Retrieval augmented language\\nmodeling (RALM) has two major components, shown in\\nFigure 12, namely: 1) retriever and 2) language model.\\nIn\\nRALM, the retriever plays a crucial role in driving LLM\\nresponse, where incorrect information can steer LLMs to false'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='response, where incorrect information can steer LLMs to false\\nbehavior. This leads to the development of various methods to\\nretrieve accurate information and fuse with the query for better\\nperformance.\\nZero-Shot Retrieval Augmentation: This kind of augmen-\\ntation keeps the original LLM architecture and weights\\nunchanged and uses BM25 [202], nearest neighbors, or frozen\\npre-trained models like Bert [7] as a retriever. The retrieved\\ninformation is provided as input to the model for response'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='information is provided as input to the model for response\\ngeneration, shown to improve performance over LLMs without\\nretrieval [198, 203].\\nIn some scenarios, multiple retrieval\\niterations are required to complete the task.\\nThe output\\ngenerated in the first iteration is forwarded to the retriever\\nto fetch similar documents. Forward-looking active retrieval\\n(FLARE) [197] initially generates the response and corrects\\nthe output by retrieving relevant documents if the response'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='the output by retrieving relevant documents if the response\\ncontains low-confidence tokens. Similarly, RepoCoder [204]\\nfetches code snippets recursively for code completion.\\nTraining with Retrieval Augmentation: To reduce failures in\\nretrieval augmentation generation (RAG), researchers train or\\nfine-tune retrievers and LLMs with a retrieval augmentation\\npipeline. We discuss the literature below based on their focus\\non the respective training processes of the pipeline.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='on the respective training processes of the pipeline.\\nTraining LLM: Retrieval-enhanced transformer (RETRO) [193]\\nshows pre-training smaller LLMs with RAG pipeline outper-\\nforms larger LLMs, such as GPT-3 trained without RAG.\\nRETRO uses a 2-trillion token subset of MassiveText as\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='a database.\\nThe retrieval pipeline divides the input query\\ninto subsets and retrieves relevant chunks from the database\\nfor each subset, encoded together with input intermediate\\nrepresentations for generating tokens. It uses cross-chunked\\nattention to attend to previous chunks auto-regressively.\\nA\\nstudy on RETRO [205] shows models pre-trained without RAG\\nbut fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='but fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.\\nTraining Retriever: Quality of responses generated by LLMs\\nis highly dependent on the in-context examples.\\nThere-\\nfore, [206, 207, 208, 209] train retrievers to retrieve accurate\\nfew-shot samples while keeping the LLM frozen for gener-\\nation.\\nRetrieved samples are ranked to build ground-truth\\ndata to train retrievers with contrastive learning in [206, 208].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='data to train retrievers with contrastive learning in [206, 208].\\nRoBERTa is trained for downstream tasks in [207] for ICL\\nsamples retrieval.\\nREPLUG [209] trains the retriever with\\nsupervised signals from the frozen LLM-generated outputs.\\nTraining Retriever and LLM: Further benefits are achieved by\\ntraining both the retriever and the model in [25, 210, 211]. In\\nthis case, the error propagates back to the retriever, updating\\nboth the language model and the retriever.\\nWhile masked'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='both the language model and the retriever.\\nWhile masked\\nlanguage modeling (MLM) is a common pre-training objec-\\ntive [25, 211], retrieval pre-trained transformer (RPT) [210]\\nused document chunk prediction as a pre-training objective for\\nlong text modeling.\\nEncoded Context Augmentation:\\nConcatenating retrieved\\ndocuments with the query becomes infeasible as the sequence\\nlength and sample size grow. Encoding the context and fusing\\nit with the decoder (Fusion-in-Decoder) using cross-attention'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='it with the decoder (Fusion-in-Decoder) using cross-attention\\nmakes it possible to augment more samples without increasing\\ncomputation costs significantly [212, 193, 210, 25].\\nWeb Augmented:\\nLocally stored memory, but external to\\nLLM, has limited information. However, a large amount of\\ninformation is available on the internet, which gets updated\\nregularly.\\nRather than storing information locally, various\\nmethods retrieve query-related context through a web search'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='methods retrieve query-related context through a web search\\nand forward it to LLMs [213, 214, 166].\\n3.4.2. Tool Augmented LLMs\\nWhile RAG relies on the retriever to provide context to the\\nLLM to answer queries, tool augmented LLMs capitalize on the\\nreasoning abilities of LLMs to iteratively plan by dividing tasks\\ninto sub-tasks, selecting necessary tools, and taking actions to\\ncomplete the task [215, 216, 217, 27]. A generic pipeline of\\ntool-augmented LLMs is shown in Figure 13, where different'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='tool-augmented LLMs is shown in Figure 13, where different\\nmodules in Figure 13 are selected in a loop until the task com-\\npletion.\\nZero-Shot Tool Augmentation: LLMs in-context learning and\\nreasoning abilities enable them to interact with tools with-\\nout training. Automatic reasoning and tool-use (ART) [217]\\nbuilds a task library with demonstrations of reasoning steps and\\ncalling external tools. It retrieves similar task examples and\\nprovides the context to the LLM for inference. Aside from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='provides the context to the LLM for inference. Aside from\\nthis, [218] shows tool documentation is enough to teach LLMs\\nto use tools without demonstrations. RestGPT [219] integrates\\nLLMs with RESTful APIs by decomposing tasks into planning\\nFigure 13: A basic flow diagram of tool augmented LLMs. Given an input and\\na set of available tools, the model generates a plan to complete the task. The\\ntool augmented LLMs utilize different modules iteratively, such as retriever,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='tool augmented LLMs utilize different modules iteratively, such as retriever,\\ntool execution, read-write to memory, feedback, etc., depending on the task.\\nand API selection steps. The API selector understands the API\\ndocumentation to select a suitable API for the task and plan the\\nexecution. ToolkenGPT [220] uses tools as tokens by concate-\\nnating tool embeddings with other token embeddings. During\\ninference, the LLM generates the tool tokens representing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='inference, the LLM generates the tool tokens representing the\\ntool call, stops text generation, and restarts using the tool exe-\\ncution output.\\nTraining with Tool Augmentation: LLMs are trained to inter-\\nact with diverse tools, enhancing planning abilities to overcome\\nthe limitations of zero-shot tool augmentation [221, 27, 222,\\n223]. Gorilla [221] instruction-tunes LLaMA with information\\nretrieval from API documentation. It uses the self-instruct [19]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='retrieval from API documentation. It uses the self-instruct [19]\\ndata generation pipeline with GPT-4 by providing in-context\\nexamples retrieved from API documentation. Tool augmented\\nlanguage model (TALM) [27] fine-tunes T5 [10] for tool use\\nwith a self-play approach, where it iteratively completes tool\\nmanipulation tasks and includes them back in the training set.\\nToolLLM [223] collects 16k APIs from RapidAPI. It samples\\nAPIs from the list to generate an instruction-tuning dataset us-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='APIs from the list to generate an instruction-tuning dataset us-\\ning ChatGPT in single-tool and multi-tool scenarios. For high-\\nquality datasets, ToolLLM suggested a depth-first search-based\\ndecision tree (DFSDT) method to generate ground-truths with\\ndiverse reasoning and planning.\\nMultimodal Tool Augmentation: The compositional reasoning\\ncapacity of LLMs allows them to manipulate tools in multi-\\nmodal settings [215, 216, 224]. Following the pipeline shown'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='modal settings [215, 216, 224]. Following the pipeline shown\\nin Figure 13, the LLM outlines a plan, generally executing in a\\nsequence: Plan →Tool selection →Execute →Inspect →\\nGenerate, to respond to the user query. Here, the database of\\ntools is rich in modalities, including text, images, etc. Many of\\nthe multimodal tool augmentation systems employ multimodal\\nLLMs [31, 225, 224, 216], while others utilize single modality\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='LLMs and generate a plan on using different modality tools to\\nsolve multimodal queries [226].\\n3.5. LLMs-Powered Agents\\nAI agents are autonomous entities, capable of planning,\\ndecision-making, and performing actions to achieve complex\\ngoals.\\nIn the early days, AI agents were rule-based, de-\\nsigned for narrow tasks, and had limited capabilities, such\\nas Clippy [227] and Deep Blue [228].\\nIn contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='In contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it\\npossible to incorporate them in diverse applications, includ-\\ning LLMs-powered agents [224, 216], where LLMs behave\\nas the brain of agents. LLMs have been incorporated in web\\nagents [166, 167], coding agents [229], tool agents [27, 223],\\nembodied agents [26], and conversational agents [195], requir-\\ning minimal to no fine-tuning\". Below we summarize the re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='ing minimal to no fine-tuning\". Below we summarize the re-\\nsearch in LLMs-based autonomous agents. For a more detailed\\ndiscussion, please refer to [230, 231].\\nLLMs Steering Autonomous Agents: LLMs are the cognitive\\ncontrollers of the autonomous agents. They generate plans, rea-\\nson about tasks, incorporate memory to complete tasks, and\\nadapt the outline depending on the feedback from the environ-\\nment. Depending on the acquired capabilities of LLMs, many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='ment. Depending on the acquired capabilities of LLMs, many\\nmethods fine-tune, propose a better prompting approach, or uti-\\nlize different modules to enhance agents’ performance. Mod-\\nules and strategies employed in autonomous agents are briefly\\ndiscussed below.\\nPlanning and Reasoning: Completing a complex task requires\\nhuman-like logical thinking, planning necessary steps, and\\nreasoning current and future directions. Prompting methods'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='reasoning current and future directions. Prompting methods\\nlike chain-of-thoughts [103], tree-of-thoughts [105], and self-\\nconsistency [104] are central to agents, eliciting LLMs to rea-\\nson its actions and choose among different paths for task com-\\npletion. When LLMs are prompted with a task description and\\na sequence of actions, they can accurately generate plan ac-\\ntions without any fine-tuning [232]. Reasoning via planning\\n(RAP) [233] incorporates a re-purposed LLM as a world model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='(RAP) [233] incorporates a re-purposed LLM as a world model\\nto reason about future outcomes and explore alternative paths\\nfor task completion. Retroformer [234] uses a retrospective\\nLLM to improve main LLM planning and reasoning capabil-\\nities by providing helpful task cues.\\nFeedback: LLMs in open-loop systems generate plans and as-\\nsume that the agent will complete them successfully. However,\\nthe actual scenario is different with failures and variable re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='the actual scenario is different with failures and variable re-\\nsponses from the environment. To correctly complete tasks,\\nmany methods use LLMs in a closed-loop where the action re-\\nsponse is provided as feedback to the LLMs to re-assess and\\nupdate the plan as required [235, 236, 237, 195]. Another di-\\nrection of research exploits LLMs as reward functions to train\\nreinforcement learning (RL) policies instead of humans [238].\\nMemory: LLMs can learn from the context provided in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='Memory: LLMs can learn from the context provided in the\\nprompt. In addition to internal memory, various systems em-\\nploy external memory to save the response history.\\nReflex-\\nion [195] maintains an episodic memory to use previous re-\\nsponses as feedback to improve future decision-making. Retro-\\nformer [234] improves its responses by employing short-term\\nand long-term memory, where short-term memory contains re-\\ncent responses and long-term memory keeps summarized failed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='cent responses and long-term memory keeps summarized failed\\nattempts to add in the prompt as reflection.\\nMulti-Agents Systems: LLMs can play user-defined roles and\\nbehave like a specific domain expert. In multi-agent systems,\\neach LLM is assigned a unique role, simulating human behav-\\nior and collaborating with other agents to complete a complex\\ntask [229, 239].\\nLLMs in Physical Environment:\\nLLMs are good at\\ninstruction-following, however, utilizing them for physically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='LLMs are good at\\ninstruction-following, however, utilizing them for physically\\ngrounded tasks requires adaptation, as they lack real-world\\nknowledge. This could lead to generating illogical responses\\nfor a particular physical situation [240, 26].\\nSayCan [240]\\nmake LLMs aware of the available low-level task operations.\\nLLM (Say) builds a high-level plan to complete the task and\\na learned affordance function (Can) explores the possibility of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='a learned affordance function (Can) explores the possibility of\\nexecuting the plan in the real world. SayCan uses RL to train\\nthe language-conditioned affordance function. PaLM-E enables\\nthe LLM to solve grounded tasks by training multi-modal LLM\\nfeeding inputs directly from the sensors.\\nManipulation: In the area of manipulation [236, 241], LLMs\\nenhance a robot’s dexterity and adaptability, excelling in tasks\\nlike object recognition, grasping, and collaboration. They ana-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='like object recognition, grasping, and collaboration. They ana-\\nlyze visual and spatial information to determine the most effec-\\ntive approach to interact with objects.\\nNavigation: LLMs enhance a robot’s ability to navigate com-\\nplex environments with precision and adaptability [242, 243,\\n244, 245]. They generate feasible paths and trajectories for\\nrobots, accounting for intricate environmental details [246].\\nThis ability is valuable in scenarios requiring precise and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='This ability is valuable in scenarios requiring precise and\\ndynamically adaptable navigation in environments like ware-\\nhouses, transport, healthcare facilities, and residences.\\n3.6. Efficient LLMs\\nDeploying LLMs in production is expensive. Reducing their\\nrunning costs while preserving performance is an appealing\\narea of research. This section summarizes the approaches sug-\\ngested to enhance LLMs’ efficiency.\\n3.6.1. Parameter Efficient Fine-Tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='gested to enhance LLMs’ efficiency.\\n3.6.1. Parameter Efficient Fine-Tuning\\nFine-tuning LLMs with tens or hundreds of billions of pa-\\nrameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\\n(540B), etc., is computationally intensive and time-consuming.\\nTo avoid complete model fine-tuning, numerous parameter-\\nefficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try\\nto achieve acceptable model fine-tuning performance at reduced\\ncosts. As compared to full fine-tuning [248], PEFT performs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='costs. As compared to full fine-tuning [248], PEFT performs\\nbetter in low-resource setups, achieves comparable perfor-\\nmance on medium-resource scenarios, and performs worse than\\nfull fine-tuning under high-resource availability. An overview\\nof different PEFT approaches is shown in Figure 14.\\nAdapter Tuning: Adds a few trainable parameters within the\\ntransformer block. The adapter layer is a sequence of feature\\ndownscaling, non-linearity, and upscaling [106]. Variants of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='downscaling, non-linearity, and upscaling [106]. Variants of\\nadapter tuning inject adapter layers sequentially [106] and in\\nparallel [38], whereas the mixture of adapter (AdaMix) [249]\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\\nthe adapter tuning category.\\nemploys multiple adapter modules in a single layer. AdaMix\\nroutes input instances randomly to one of the multiple down-\\nscale and upscale modules. The mixture of adapters is averaged\\nout for inference to avoid additional latency. Low-Rank Adap-\\ntation (LoRA) [250] learns low-rank decomposed matrices to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='tation (LoRA) [250] learns low-rank decomposed matrices to\\nfreeze original weights. The learned weights are fused with the\\noriginal weights for inference, avoiding latency.\\nPrompt Tuning: Prompting is an effective way to adapt a\\npre-trained LLM for the downstream task. However, manual\\nprompts bring uncertainty in the model’s prediction, where a\\nchange in a single word drops the performance [247]. Prompt\\ntuning alleviates this problem by fine-tuning only 0.001%-3%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='tuning alleviates this problem by fine-tuning only 0.001%-3%\\nadditional parameters [251]. It concatenates trainable prompt\\nparameters with the model embeddings [247, 40, 251]. Task-\\nspecific fixed discrete prompts are concatenated with input em-\\nbeddings in [40]. As discrete prompts bring instability, prompts\\nare encoded through a learnable mapping in P-Tuning [247],\\nnaming continuous prompts, which are appended with the dis-\\ncrete prompts.\\nOnly the prompt encoder is trainable in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='crete prompts.\\nOnly the prompt encoder is trainable in the\\nmodel. In an extension of P-Tuning, continuous prompts are\\nconcatenated with each layer of the network in [251]. Progres-\\nsive prompts [252] avoid catastrophic forgetting and transfer\\npreviously learned knowledge by sequentially adding trainable\\nprompt embeddings to the previously frozen task embeddings.\\nPrefix Tuning: A set of trainable task-specific prefix vectors\\nare appended to the frozen transformer layers in prefix tun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='are appended to the frozen transformer layers in prefix tun-\\ning [41]. The prefix vectors are virtual tokens attended by the\\ncontext tokens on the right. In addition, adaptive prefix tun-\\ning [253] applies a gating mechanism to control the information\\nfrom the prefix and actual tokens.\\nBias Tuning: Fine-tuning only bias terms in small to medium\\ntraining data has been found effective in BitFit [254].\\nThis\\nmethod achieves full fine-tuning performance for tasks with less'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='This\\nmethod achieves full fine-tuning performance for tasks with less\\ntraining data and comparable performance with more training\\ndata.\\n3.6.2. Quantization\\nLLMs require extensive computing and memory for infer-\\nence.\\nDeploying a 175B parameter GPT-3 model needs at\\nleast five 80GB A100 GPUs and 350GB of memory to store in\\nFP16 format [44]. Such demanding requirements for deploying\\nLLMs make it harder for smaller organizations to utilize them.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='LLMs make it harder for smaller organizations to utilize them.\\nModel compression is an effective solution but comes at the cost\\nof degraded performance, especially at large scales greater than\\n6B. These models exhibit very large magnitude outliers that do\\nnot exist in smaller models [255], making it challenging and re-\\nquiring specialized methods for quantizing LLMs [44, 256].\\nPost-Training Quantization: Minimal or no training is re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='Post-Training Quantization: Minimal or no training is re-\\nquired in this type of quantization, without significantly com-\\npromising the model performance. LLM-8-bit [255] uses full-\\nprecision matrix multiplication for weights associated with out-\\nlier features and 8-bit for remaining features. The lower pre-\\ncision multiplication outputs are converted to FP-16 and con-\\ncatenated with others. The quantized models have homogenous\\nword embeddings, which may degrade their performance. To'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='word embeddings, which may degrade their performance. To\\nfix this, token-level knowledge distillation is employed in [45]\\nalong with independent quantization scaling factors for each\\nmodule due to varying weight distribution. Feature distribu-\\ntions are asymmetric and appear in different channels; outlier\\nsuppression [257] shifts and scales per-channel activation dis-\\ntributions for effective quantization. SmoothQuant [44] quan-\\ntizes activations and weights to INT8 format by smoothing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='tizes activations and weights to INT8 format by smoothing\\nactivations and migrating the quantization difficulty toward\\nweights. It multiplies the inverse of the smoothing factor with\\nweights, which introduces a few outliers in the weights but is\\neasier to quantify than unsmoothed activations. OPTQ [256]\\nuses the optimal brain compression (OBC) [258] algorithm to\\nquantize the model layer-by-layer and update weights to com-\\npensate for quantization error.\\nTo improve speed and per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='pensate for quantization error.\\nTo improve speed and per-\\nformance, OPTQ updates weights in arbitrary order, employs\\nlazy updates, and uses better Cholesky kernels. Outlier-aware\\nweight quantization (OWQ) [259] uses the OPTQ algorithm for\\nquantization but assigns higher precision to vulnerable weights,\\ncausing outliers and lower precision for others.\\nQuantization-Aware Training:\\nTo compensate for perfor-\\nmance degradation,\\na quantized model is fine-tuned in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='To compensate for perfor-\\nmance degradation,\\na quantized model is fine-tuned in\\nquantization-aware training (QAT) [260, 261, 262].\\nAl-\\npha Tuning quantizes the model using binary coding quan-\\ntization (BCQ) [263] and fine-tunes only quantization scal-\\ning factors.\\nThis approach improves performance over\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='parameter-efficient fine-tuning of the pre-trained model. Sim-\\nilarly, parameter-efficient and quantization-aware adaptation\\n(PEQA) [264] reduces the precision of fully-connected layers\\nand fine-tunes only quantization scaling parameters.\\nLLM-\\nQAT [262] generates training data from the pre-trained network\\nand trains a quantized student model with knowledge distilla-\\ntion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM\\nwith LoRA [250] using a 4-bit normal float, which shows better'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='with LoRA [250] using a 4-bit normal float, which shows better\\nperformance over a 4-bit integer and float.\\n3.6.3. Pruning\\nPruning is an alternative approach to quantization to com-\\npress model size, thereby reducing LLMs deployment costs\\nsignificantly. Compared to task-agnostic pruning, task-specific\\npruning is easily achievable with good performance, where a\\nmodel is fine-tuned on the downstream task and pruned for\\nfaster inference. It is possible to prune LLMs for individual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='faster inference. It is possible to prune LLMs for individual\\ntasks, but the cost of pruning and deploying task-specific mod-\\nels is high. To overcome this, many structured and unstructured\\npruning methods for LLMs have been proposed to maintain rea-\\nsonable performance across all tasks while shrinking the model\\nsize [265, 42, 266].\\nUnstructured Pruning: This kind of pruning removes less im-\\nportant weights without maintaining any structure.\\nExisting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='portant weights without maintaining any structure.\\nExisting\\nLLM pruning methods take advantage of the unique charac-\\nteristics of LLMs, uncommon for smaller models, where a\\nsmall subset of hidden states are activated with large magni-\\ntude [255]. Pruning by weights and activations (Wanda) [265]\\nprunes weights in every row based on importance, calculated\\nby multiplying the weights with the norm of input. The pruned\\nmodel does not require fine-tuning, thereby saving computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='model does not require fine-tuning, thereby saving computa-\\ntional costs. Outlier weighed layerwise sparsity (OWL) [267]\\nextends Wanda with non-uniform layer pruning. It shows that\\nthe number of outliers varies for different layers; therefore, the\\nmodel should have variable pruning ratios for better perfor-\\nmance for every layer. Contrastive pruning (CAP) [43] itera-\\ntively prunes the model by training the sparse model using con-\\ntrastive loss between pre-trained, fine-tuned, and snapshots of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='trastive loss between pre-trained, fine-tuned, and snapshots of\\nprevious sparse models to learn task-specific and task-agnostic\\nknowledge.\\nStructured Pruning: Here, the parameters are removed in\\ngroups, rows, columns, or matrices, which speeds up the\\ninference because of effective hardware tensor core utiliza-\\ntion [265].\\nLLM-Pruner [42] employs a 3-stage structured\\npruning strategy, identifying the groups of hidden states caus-\\ning each other to activate during the forward-pass, keeping im-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='ing each other to activate during the forward-pass, keeping im-\\nportant groups and removing less important ones, and fine-\\ntuning the pruned model with LoRA. Sparsity-induced mask\\nlearning (SIMPLE) [268] prunes the network using learnable\\nmasks. Similarly, another method prunes LLMs by learning\\nmasks and removing unimportant rank-1 components of the\\nfactorized weight matrix [266].\\n3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-\\ning applications, an increasing number of research works are\\nnow facilitating LLMs to perceive different modalities of infor-\\nmation like image [269, 270, 271], video [272, 273, 274], au-\\ndio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present\\nsubstantial benefits compared to standard LLMs that process\\nonly text. By incorporating information from various modal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='only text. By incorporating information from various modal-\\nities, MLLMs can achieve a deeper understanding of context,\\nleading to more intelligent responses infused with a variety of\\nexpressions. Importantly, MLLMs align closely with human\\nperceptual experiences, leveraging the synergistic nature of our\\nmultisensory inputs to form a comprehensive understanding of\\nthe world [276, 26]. Coupled with a user-friendly interface,\\nMLLMs can offer intuitive, flexible, and adaptable interactions,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='MLLMs can offer intuitive, flexible, and adaptable interactions,\\nallowing users to engage with intelligent assistants through a\\nspectrum of input methods. According to the ways of construct-\\ning models, current MLLMs can be generally divided into three\\nstreams: pre-training, fine-tuning, and prompting. In this sec-\\ntion, we will discuss more details of these main streams, as well\\nas the important application of MLLMs in visual reasoning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='as the important application of MLLMs in visual reasoning.\\nPre-training: This stream of MLLMs intends to support differ-\\nent modalities using unified end-to-end models. For instance,\\nFlamingo [269] applies gated cross-attention to fuse vision and\\nlanguage modalities, which are collected from pre-trained and\\nfrozen visual encoder and LLM, respectively. Moreover, BLIP-\\n2 [270] proposes a two-stage strategy to pre-train a Querying\\nTransformer (Q-Former) for the alignment between vision and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='Transformer (Q-Former) for the alignment between vision and\\nlanguage modalities: in the first stage, vision-language repre-\\nsentation learning is bootstrapped from a frozen visual encoder;\\nand in the second stage, a frozen LLM bootstraps vision-to-\\nlanguage generative learning for zero-shot image-to-text gen-\\neration. Similarly, MiniGPT-4 [277] deploys pre-trained and\\nfrozen ViT [278], Q-Former and Vicuna LLM [159], only train-\\ning the linear projection layer for vision and language modali-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='ing the linear projection layer for vision and language modali-\\nties alignment.\\nFine-tuning: Derived from instruction tuning [16] for NLP\\ntasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\\nusing multimodal instructions. Following this method, LLMs\\ncan be easily and effectively extended as multimodal chat-\\nbots [277, 271, 29] and multimodal task solvers [279, 30, 280].\\nThe key issue of this stream of MLLMs is to collect multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='The key issue of this stream of MLLMs is to collect multi-\\nmodal instruction-following data for fine-tuning [58]. To ad-\\ndress this issue, the solutions of benchmark adaptation [279,\\n281, 282], self-instruction [19, 31, 283], and hybrid composi-\\ntion [284, 280] are employed, respectively. To mitigate the gap\\nbetween the original language modality and additional modal-\\nities, the learnable interface is introduced to connect differ-\\nent modalities from frozen pre-trained models.\\nParticularly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='ent modalities from frozen pre-trained models.\\nParticularly,\\nthe learnable interface is expected to work in a parameter-\\nefficient tuning manner: e.g., LLaMA-Adapter [285] applies\\nan efficient transformer-based adapter module for training,\\nand LaVIN [284] dynamically learns the multimodal feature\\nweights using a mixture-of-modality adapter. Different from\\nthe learnable interface, the expert models can directly convert\\nmultimodalities into language: e.g., VideoChat-Text [272] in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='multimodalities into language: e.g., VideoChat-Text [272] in-\\ncorporates Whisper [286], a speech recognition expert model,\\nto generate the captions of given videos for the understanding\\nof following LLMs.\\nPrompting:\\nDifferent from the fine-tuning technique that\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='directly updates the model parameters given task-specific\\ndatasets, the prompting technique provides certain context, ex-\\namples, or instructions to the model, fulfilling specialized tasks\\nwithout changing the model parameters. Since prompting can\\nsignificantly reduce the need for large-scale multimodal data,\\nthis technique is widely used to construct MLLMs. Particularly,\\nto solve multimodal Chain of Thought (CoT) problems [103],\\nLLMs are prompted to generate both the reasoning process and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='LLMs are prompted to generate both the reasoning process and\\nthe answer given multimodal inputs [287]. On this front, differ-\\nent learning paradigms are exploited in practice: for example,\\nMultimodal-CoT [287] involves two stages of rationale genera-\\ntion and answer inference, where the input of the second stage\\nis a combination of the original input and the output of the first\\nstage; and CoT-PT [288] applies both prompt tuning and spe-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='stage; and CoT-PT [288] applies both prompt tuning and spe-\\ncific visual bias to generate a chain of reasoning implicitly. In\\naddition to CoT problems, LLMs can also be prompted with\\nmultimodal descriptions and tools, effectively dividing complex\\ntasks into sub-tasks [289, 290].\\nVisual Reasoning Application: Recent visual reasoning sys-\\ntems [291, 292, 216, 293] tend to apply LLMs for better visual\\ninformation analysis and visual-language integration. Differ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='information analysis and visual-language integration. Differ-\\nent from previous works [294, 295] that rely on limited VQA\\ndatasets and small-scale neural networks, current LLM-aided\\nmethods offer benefits of stronger generalization ability, emer-\\ngent ability, and interactivity [58]. To realize visual reasoning\\nwith the help of LLMs, prompting and fine-tuning techniques\\ncan also be utilized: for example, PointClip V2 [292] applies\\nLLMs to generate 3D-specific prompts, which are encoded as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='LLMs to generate 3D-specific prompts, which are encoded as\\ntextual features and then combined with visual features for\\n3D recognition; and GPT4Tools [31] employs LoRA [250] to\\nfine-tune LLMs following tool-related instructions.\\nServing\\nas a controller [293], decision maker [296], or semantics re-\\nfiner [291, 297], LLMs significantly facilitates the progress of\\nvisual reasoning research.\\n3.8. Summary and Discussion\\n3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-\\ntecture and training strategies have a big impact on performance\\nand stability. Here, we summarize key architectural modules\\nused in various LLMs, leading to better performance, reduced\\ntraining time and memory, and better training stability.\\nLayer Normalization: The performance and training stability\\nof LLMs are affected significantly by layer normalization. Pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='of LLMs are affected significantly by layer normalization. Pre-\\nnorm, that is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [6, 127, 108].\\nBLOOM [13] and AlexaTM [122] utilize an additional layer\\nnormalization before embedding layer to stabilize the training\\nof large-scale models, while the model’s zero-shot generaliza-\\ntion ability can be negatively impacted [13]. However, another\\nstudy [33] finds that pre-norm degrades fine-tuned model per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='study [33] finds that pre-norm degrades fine-tuned model per-\\nformance as compared to post-norm, and there are no stability\\nbenefits of pre-norm beyond the 100B scale. Therefore, GLM-\\n130B [33] used deep-norm which is a variant of post-norm for\\nbetter downstream task performance after fine-tuning.\\nPositional Encoding: Like other building blocks of the model,\\npositional encoding also affects the performance and training\\nstability of LLMs.\\nBLOOM [13] finds ALiBi outperforms'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='stability of LLMs.\\nBLOOM [13] finds ALiBi outperforms\\nlearned and rotary positional encodings.\\nContrary to this,\\nGLM-130B [33] identifies rotary positional encoding as being\\nbetter than ALiBi. So, there is no conclusion in the literature\\nabout positional encodings yet.\\nParallel Attention: In this type of attention, feed-forward and\\nattention layers are parallel to each other rather than sequen-\\ntial in a transformer block. It has been shown to reduce train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='tial in a transformer block. It has been shown to reduce train-\\ning time by 15%. There is no evidence of performance drop\\ndue to this change in the literature and it is used by the models\\nPaLM [15], GPT-NeoX [118], and CodeGen [140].\\nMulti-Query Attention It has shared key and value attention\\nheads in a transformer block while query attention heads are\\nprojected as usual. This reduces memory usage and speeds up\\nsampling in autoregressive decoding. No performance degrada-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='sampling in autoregressive decoding. No performance degrada-\\ntion has been observed with this change and it makes the train-\\ning efficient allowing larger batch sizes. Multi-query attention\\nis used in [15, 142].\\nMixture of Experts: This type of architecture enables eas-\\nily scaling models to trillions of parameters [92, 91]. Only a\\nfew experts are activated during the computation making them\\ncompute-efficient. The performance of MoE models is better'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='compute-efficient. The performance of MoE models is better\\nthan dense models for the same amount of data and requires less\\ncomputation during fine-tuning to achieve performance similar\\nto dense models as discussed in [91]. MoE architectures are\\nless prone to catastrophic forgetting, therefore are more suited\\nfor continual learning [92]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [92].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='making MoE architecture hardware-friendly [92].\\nSparse vs Dense Activated: GPT-3 [6] uses sparse transform-\\ners [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\\narchitectures to lower computational costs and increase the\\nmodel size and capacity. According to the literature, sparse\\nmodules do not degrade the model’s performance [67]. How-\\never, more experiments are required to verify this statement.\\n3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-\\ning costs, avoid loss divergence, and achieve better perfor-\\nmance. We summarize and discuss some of these key tricks\\nused in different LLMs.\\nMixed Precision: It is a famous method for LLMs to reduce\\nmemory usage and improve training efficiency. In mixed pre-\\ncision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='format whereas optimizer states and master weights are kept\\nin FP32 format [120]. A drawback associated with this for-\\nmat change is training instability due to a smaller value range\\nresulting in loss spikes [33]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs precision-\\nsensitive operations like gradient accumulation and softmax in\\nFP32 [13]. BF16 has better performance and training stability\\nbut uses more memory and is supported on specific hardware,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='but uses more memory and is supported on specific hardware,\\nfor example, A100 GPUs. Therefore, its adoption in LLMs is\\nlimited.\\nTraining Instability: Loss divergence or spiking is a common\\nissue in LLMs that occurs multiple times during training. This\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='happens in the presence of gradient clipping [15]. To mitigate\\nthis problem, many approaches suggest restarting training from\\nan earlier checkpoint [15, 33, 91], skipping 200-500 earlier\\ndata batches at the point of divergence in [15] and re-shuffling\\nbatches in [91]. The embedding layer gradient shrink proves to\\nfurther stabilize the training as its gradient norm is significantly\\nlarger than the other layers [33]. Another suggestion to improve'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='larger than the other layers [33]. Another suggestion to improve\\ntraining stability for larger models is not to use biases in dense\\nand norm layers as in [15].\\nWeight Initialization: It plays a significant role in model con-\\nvergence and training stability.\\nGPT-NeoX [118] initializes\\nfeed-forward layers before residuals with\\n2\\nL\\n√\\nd as in [153] and\\nother layers with the small initialization scheme [298]. This\\navoids activations growing exponentially with increasing depth.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='avoids activations growing exponentially with increasing depth.\\nMT-NLG [117] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [298]. Various models perform random weight initial-\\nization which can cause bad initialization, Galactica [148] sug-\\ngests a longer warmup to negate the effect.\\nLearning Rate: A suitable learning rate is important for sta-\\nble training. It is suggested to use a lower value [13, 15, 124]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='ble training. It is suggested to use a lower value [13, 15, 124]\\nwith warmup and decay (cosine or linear). Usually, the learn-\\ning rate is within the range 1e−4 to 8e−4. Moreover, MT-NLG\\n(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\\ning learning rates based on the model size using the GPT-3 [6]\\nmodels ranging between 13B and 175B. This avoids tuning the\\nlearning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='learning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,\\npipeline, and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\\nIn addition to 3D parallelism, BLOOM [13] uses a zero op-\\ntimizer [37] to shard optimizer states.\\nPanGu-α [108] and\\nPanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\\nlelism which additionally contains optimizer parallelism and\\nrematerialization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='lelism which additionally contains optimizer parallelism and\\nrematerialization.\\nMode Switching: It adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve downstream task performance\\nin [125, 124, 122]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation: Generating credible and con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='Controllable Text Generation: Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-3 [6]\\nand other LLMs use in-context learning to control generated\\ntext. While in-context learning helps in controlling the gener-\\nated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\\nto rank its generated text for credibility and soft prompts such as\\ngenre, topic, keywords, sentiment, and length for better control\\non generated text.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='genre, topic, keywords, sentiment, and length for better control\\non generated text.\\n3.8.3. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing di-\\nverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks by\\na large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='a large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance difference between zero-shot and few-shot is\\nlarge for pre-trained models [6, 15], naming LLMs as meta-\\nlearners [6]. LLMs zero-shot evaluations underperform unsu-\\npervised methods in neural machine translation [6]. The liter-\\nature shows pre-training is not enough for good zero-shot per-\\nformance [15, 16]. To improve the zero-shot performance the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='formance [15, 16]. To improve the zero-shot performance the\\nliterature suggests using instruction fine-tuning that improves\\nthe zero-shot performance significantly and outperforms base-\\nlines. Instruction fine-tuning has also been shown to improve\\nzero-shot generalization to unseen tasks. Another model, Flan-\\nPaLM [16], unlocks zero-shot reasoning with CoT training.\\n3.8.5. Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for different'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='Traditionally, these architectures perform well for different\\ntasks, for example, encoder-only for NLU tasks, decoder-only\\nfor NLG, and encoder-decoder for sequence2sequence model-\\ning. Encoder-only models are famous for smaller models such\\nas Bert [7], RoBERTa [299], etc., whereas LLMs are either\\ndecoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\\nWhile decoder-only models are good at NLG tasks, various\\nLLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\\nLLaMA [156], are decoder-only models with significant per-\\nformance gains on both NLU and NLG tasks. In contradic-\\ntion to this, T5 [10] and UL2 [125] identify encoder-decoder\\nmodels out-performing decoder-only models. In another study,\\nPaLM [15] finds increasing the size of decoder-only models\\ncan reduce the performance gap between decoder-only and\\nencoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='encoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [125, 122] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5+ [34]\\nuses an encoder-decoder architecture with multiple training ob-\\njectives for different tasks, activating the encoder, decoder, or\\nboth according to the tasks. These variations in architecture'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='both according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\n4. Model Configurations\\nWe provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='publication venue, license type, model creators, steps trained,\\nparallelism, etc in Table 3 and Table 4. Architecture details\\nof pre-trained LLMs are available in Table 5. Providing these\\ndetails for instruction-tuned models is unnecessary because it\\nfine-tunes pre-trained models for instruction datasets. Hence,\\narchitectural details are the same as the baselines. Moreover,\\noptimization settings for various LLMs are available in Table 6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='optimization settings for various LLMs are available in Table 6\\nand Table 7. We do not include details on precision, warmup,\\nand weight decay in Table 7. These details are not as important\\nas others to mention for instruction-tuned models, and are not\\nprovided by the papers.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='Table 3: Summary of pre-trained LLMs (>10B). Only the LLMs discussed individually in the previous sections are summarized. “Data/Tokens” is the model’s\\npre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\\n(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\\nre-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\\n(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"“DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\\nModels\\nPublication\\nVenue\\nLicense\\nType\\nModel\\nCreators Purpose\\nNo. of\\nParams\\nCommercial\\nUse\\nSteps\\nTrained\\nData/\\nTokens\\nData\\nCleaning\\nNo. of\\nProcessing Units\\nProcessing\\nUnit Type\\nTraining\\nTime\\nCalculated\\nTrain. Cost\\nTraining\\nParallelism\\nLibrary\\nT5 [10]\\nJMLR'20\\nApache-2.0\\nGoogle\\nGeneral\\n11B\\n✓\\n1M\\n1T\\nHeur+Dedup\\n1024\\nTPU v3\\n-\\n-\\nD+M\\nMesh TensorFlow\\nGPT-3 [6]\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Apache-2.0\\nGoogle\\nGeneral\\n11B\\n✓\\n1M\\n1T\\nHeur+Dedup\\n1024\\nTPU v3\\n-\\n-\\nD+M\\nMesh TensorFlow\\nGPT-3 [6]\\nNeurIPS'20\\n-\\nOpenAI\\nGeneral\\n175B\\n×\\n-\\n300B\\nDedup+QF\\n-\\nV100\\n-\\n-\\nM\\n-\\nmT5 [11]\\nNAACL'21\\nApache-2.0\\nGoogle\\nGeneral\\n13B\\n✓\\n1M\\n1T\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nPanGu-α [108]\\narXiv'21\\nApache-2.0\\nHuawei\\nGeneral\\n200B\\n✓\\n260k\\n1.1TB\\nHeur+Dedup\\n2048\\nAscend 910\\n-\\n-\\nD+OP+P+O+R\\nMindSpore\\nCPM-2 [12]\\nAI Open'21\\nMIT\\nTsinghua\\nGeneral\\n198B\\n✓\\n1M\\n2.6TB\\nDedup\\n-\\n-\\n-\\n-\\nD+M\\nJAXFormer\\nCodex [141]\\narXiv'21\\n-\\nOpenAI\\nCoding\\n12B\\n×\\n-\\n100B\\nHeur\\n-\\n-\\n-\\n-\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"1M\\n2.6TB\\nDedup\\n-\\n-\\n-\\n-\\nD+M\\nJAXFormer\\nCodex [141]\\narXiv'21\\n-\\nOpenAI\\nCoding\\n12B\\n×\\n-\\n100B\\nHeur\\n-\\n-\\n-\\n-\\n-\\n-\\nERNIE 3.0 [110]\\narXiv'21\\n-\\nBaidu\\nGeneral\\n10B\\n×\\n120k∗\\n375B\\nHeur+Dedup\\n384\\nV100\\n-\\n-\\nM∗\\nPaddlePaddle\\nJurassic-1 [112]\\nWhite-Paper'21 Apache-2.0\\nAI21\\nGeneral\\n178B\\n✓\\n-\\n300B\\n-\\n800\\nGPU\\n-\\n-\\nD+M+P\\nMegatron+DS\\nHyperCLOVA [114]\\nEMNLP'21\\n-\\nNaver\\nGeneral\\n82B\\n×\\n-\\n300B\\nClf+Dedup+PF\\n1024\\nA100\\n321h\\n1.32 Mil\\nM\\nMegatron\\nYuan 1.0 [115]\\narXiv'21\\nApache-2.0\\n-\\nGeneral\\n245B\\n✓\\n26k∗\\n180B Heur+Clf+Dedup\\n2128\\nGPU\\n-\\n-\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Megatron\\nYuan 1.0 [115]\\narXiv'21\\nApache-2.0\\n-\\nGeneral\\n245B\\n✓\\n26k∗\\n180B Heur+Clf+Dedup\\n2128\\nGPU\\n-\\n-\\nD+T+P\\n-\\nGopher [116]\\narXiv'21\\n-\\nGoogle\\nGeneral\\n280B\\n×\\n-\\n300B\\nQF+Dedup\\n4096\\nTPU v3\\n920h\\n13.19 Mil\\nD+M\\nJAX+Haiku\\nERNIE 3.0 Titan [35]\\narXiv'21\\n-\\nBaidu\\nGeneral\\n260B\\n×\\n-\\n300B\\nHeur+Dedup\\n-\\nAscend 910\\n-\\n-\\nD+M+P+D*\\nPaddlePaddle\\nGPT-NeoX-20B [118] BigScience'22\\nApache-2.0\\nEleutherAI\\nGeneral\\n20B\\n✓\\n150k\\n825GB\\nNone\\n96\\n40G A100\\n-\\n-\\nM\\nMegatron+DS+PyTorch\\nOPT [14]\\narXiv'22\\nMIT\\nMeta\\nGeneral\\n175B\\n✓\\n150k\\n180B\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"None\\n96\\n40G A100\\n-\\n-\\nM\\nMegatron+DS+PyTorch\\nOPT [14]\\narXiv'22\\nMIT\\nMeta\\nGeneral\\n175B\\n✓\\n150k\\n180B\\nDedup\\n992\\n80G A100\\n-\\n-\\nD+T\\nMegatron\\nBLOOM [13]\\narXiv'22\\nRAIL-1.0\\nBigScience\\nGeneral\\n176B\\n✓\\n-\\n366B\\nDedup+PR\\n384\\n80G A100\\n2520h\\n3.87 Mil\\nD+T+P\\nMegatron+DS\\nGalactica [148]\\narXiv'22\\nApache-2.0\\nMeta\\nScience\\n120B\\n×\\n225k\\n106B\\nDedup\\n128\\n80GB A100\\n-\\n-\\n-\\nMetaseq\\nGLaM [91]\\nICML'22\\n-\\nGoogle\\nGeneral\\n1.2T\\n×\\n600k∗\\n600B\\nClf\\n1024\\nTPU v4\\n-\\n-\\nM\\nGSPMD\\nLaMDA [150]\\narXiv'22\\n-\\nGoogle\\nDialog\\n137B\\n×\\n3M\\n2.81T\\nFiltered\\n1024\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"600B\\nClf\\n1024\\nTPU v4\\n-\\n-\\nM\\nGSPMD\\nLaMDA [150]\\narXiv'22\\n-\\nGoogle\\nDialog\\n137B\\n×\\n3M\\n2.81T\\nFiltered\\n1024\\nTPU v3\\n1384h\\n4.96 Mil\\nD+M\\nLingvo\\nMT-NLG [117]\\narXiv'22\\nApache-v2.0 MS.+Nvidia General\\n530B\\n×\\n-\\n270B\\n-\\n4480\\n80G A100\\n-\\n-\\nD+T+P\\nMegatron+DS\\nAlphaCode [142]\\nScience'22\\nApache-v2.0\\nGoogle\\nCoding\\n41B\\n✓\\n205k\\n967B\\nHeur+Dedup\\n-\\nTPU v4\\n-\\n-\\nM\\nJAX+Haiku\\nChinchilla [96]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n70B\\n×\\n-\\n1.4T\\nQF+Dedup\\n-\\nTPUv4\\n-\\n-\\n-\\nJAX+Haiku\\nPaLM [15]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n255k\\n780B\\nHeur\\n6144\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"QF+Dedup\\n-\\nTPUv4\\n-\\n-\\n-\\nJAX+Haiku\\nPaLM [15]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n255k\\n780B\\nHeur\\n6144\\nTPU v4\\n-\\n-\\nD+M\\nJAX+T5X\\nAlexaTM [122]\\narXiv'22\\nApache v2.0\\nAmazon\\nGeneral\\n20B\\n×\\n500k\\n1.1T\\nFiltered\\n128\\nA100\\n2880h\\n1.47 Mil\\nM\\nDS\\nU-PaLM [124]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n20k\\n-\\n-\\n512\\nTPU v4\\n120h\\n0.25 Mil\\n-\\n-\\nUL2 [125]\\nICLR'23\\nApache-2.0\\nGoogle\\nGeneral\\n20B\\n✓\\n2M\\n1T\\n-\\n512\\nTPU v4\\n-\\n-\\nM\\nJAX+T5X\\nGLM [33]\\nICLR'23\\nApache-2.0\\nMultiple\\nGeneral\\n130B\\n×\\n-\\n400B\\n-\\n768\\n40G A100\\n1440h\\n3.37 Mil\\nM\\n-\\nCodeGen [140]\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"ICLR'23\\nApache-2.0\\nMultiple\\nGeneral\\n130B\\n×\\n-\\n400B\\n-\\n768\\n40G A100\\n1440h\\n3.37 Mil\\nM\\n-\\nCodeGen [140]\\nICLR'23\\nApache-2.0\\nSalesforce\\nCoding\\n16B\\n✓\\n650k\\n577B\\nHeur+Dedup\\n-\\nTPU v4\\n-\\n-\\nD+M\\nJAXFormer\\nLLaMA [127]\\narXiv'23\\n-\\nMeta\\nGeneral\\n65B\\n×\\n350k\\n1.4T Clf+Heur+Dedup\\n2048\\n80G A100\\n504h\\n4.12 Mil\\nD+M\\nxFormers\\nPanGuΣ [92]\\narXiv'23\\n-\\nHuawei\\nGeneral 1.085T\\n×\\n-\\n329B\\n-\\n512\\nAscend 910\\n2400h\\n-\\nD+OP+P+O+R\\nMindSpore\\nBloombergGPT [151]\\narXiv23\\n-\\nBloomberg\\nFinance\\n50B\\n×\\n139k\\n569B\\nDedup\\n512\\n40G A100\\n1272h\\n1.97 Mil\\nM\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"BloombergGPT [151]\\narXiv23\\n-\\nBloomberg\\nFinance\\n50B\\n×\\n139k\\n569B\\nDedup\\n512\\n40G A100\\n1272h\\n1.97 Mil\\nM\\nPyTorch\\nXuan Yuan 2.0 [152]\\narXiv23\\nRAIL-1.0\\nDu Xiaoman Finance\\n176B\\n✓\\n-\\n366B\\nFiltered\\n-\\n80GB A100\\n-\\n-\\nP\\nDS\\nCodeT5+ [34]\\narXiv'23\\nBSD-3\\nSalesforce\\nCoding\\n16B\\n✓\\n110k\\n51.5B\\nDedup\\n16\\n40G A100\\n-\\n-\\n-\\nDS\\nStarCoder [147]\\narXiv'23\\nOpenRAIL-M BigCode\\nCoding\\n15.5B\\n✓\\n250k\\n1T\\nDedup+QF+PF\\n512\\n80G A100\\n624h\\n1.28 Mil\\nD+T+P\\nMegatron-LM\\nLLaMA-2 [21]\\narXiv'23\\nLLaMA-2.0\\nMeta\\nGeneral\\n70B\\n✓\\n500k\\n2T\\nMinimal Filtering\\n-\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"D+T+P\\nMegatron-LM\\nLLaMA-2 [21]\\narXiv'23\\nLLaMA-2.0\\nMeta\\nGeneral\\n70B\\n✓\\n500k\\n2T\\nMinimal Filtering\\n-\\n80G A100\\n1.7Mh\\n-\\n-\\n-\\nPaLM-2 [123]\\narXiv'23\\n-\\nGoogle\\nGeneral\\n-\\n×\\n-\\n-\\nDdedup+PF+QF\\n-\\n-\\n-\\n-\\n-\\n-\\nLLaMA-3.1 [130]\\narXiv'24\\nLLaMA-3.0\\nMeta\\nGeneral\\n405B\\n✓\\n1.2M\\n15T\\nDedup+QF\\n16k\\n80G H100 30.84Mh\\n-\\nD+T+P+C\\nPyTorch\\nMixtral 8x22B [131]\\nweb'24\\nApache-2.0\\nMistral AI\\nGeneral\\n141B\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSnowflake Arctic [132] web'24\\nApache-2.0\\nSnowflake\\nGeneral\\n480B\\n✓\\n-\\n3.5T\\n-\\n-\\n-\\n-\\nT+P\\nDS\\nNemotron-4 340B [137]web'24\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Apache-2.0\\nSnowflake\\nGeneral\\n480B\\n✓\\n-\\n3.5T\\n-\\n-\\n-\\n-\\nT+P\\nDS\\nNemotron-4 340B [137]web'24\\nNvidia\\nNvidia\\nGeneral\\n340B\\n✓\\n-\\n9T\\n-\\n6144\\n80G H100\\n-\\n-\\nD+T+P\\n-\\nDeepSeek [138]\\narXiv'24\\nMIT\\nDeepSeek\\nGeneral\\n67B\\n✓\\n-\\n2T\\nDedup+QF\\n-\\n-\\n300.6Kh\\n-\\nD+T+P\\nDS\\nDeepSeek-v2 [139]\\narXiv'24\\nMIT\\nDeepSeek\\nGeneral\\n67B\\n✓\\n-\\n8.1T\\nQF\\n-\\nH800\\n172.8Kh\\n-\\nD+P\\nHAI-LLM\\nTable 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"of training samples.\\nModels\\nPublication\\nVenue\\nLicense\\nType\\nModel\\nCreators\\nPurpose\\nNo. of\\nParams\\nCommercial\\nUse\\nPre-trained\\nModels\\nSteps\\nTrained\\nData/\\nTokens\\nNo. of\\nProcessing Units\\nProcessing\\nUnit Type\\nTrain.\\nTime\\nCalculated\\nTrain. Cost\\nTrain.\\nParallelism\\nLibrary\\nWebGPT [166]\\narXiv'21\\n-\\nOpenAI\\nGeneral\\n175B\\n×\\nGPT-3\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nT0 [17]\\nICLR'22\\nApache-2.0\\nBigScience\\nGeneral\\n11B\\n✓\\nT5\\n-\\n250B\\n512\\nTPU v3\\n270h\\n0.48 Mil\\n-\\n-\\nTk-Instruct [18]\\nEMNLP'22\\nMIT\\nAI2+\\nGeneral\\n11B\\n✓\\nT5\\n1000\\n-\\n256\\nTPU v3\\n4h\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"TPU v3\\n270h\\n0.48 Mil\\n-\\n-\\nTk-Instruct [18]\\nEMNLP'22\\nMIT\\nAI2+\\nGeneral\\n11B\\n✓\\nT5\\n1000\\n-\\n256\\nTPU v3\\n4h\\n0.0036 Mil\\n-\\nGoogle T5\\nOPT-IML [97]\\narXiv'22\\n-\\nMeta\\nGeneral\\n175B\\n×\\nOPT\\n8k\\n2B\\n128\\n40G A100\\n-\\n-\\nD+T\\nMegatron\\nFlan-U-PaLM [16] ICLR'22\\nApache-2.0\\nGoogle\\nGeneral\\n540B\\n✓\\nU-PaLM\\n30k\\n-\\n512\\nTPU v4\\n-\\n-\\n-\\nJAX+T5X\\nmT0 [154]\\nACL'23\\nApache-2.0 HuggingFace+ General\\n13B\\n✓\\nmT5\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSparrow [167]\\narXiv'22\\n-\\nGoogle\\nDialog\\n70B\\n×\\nChinchilla\\n-\\n-\\n64\\nTPU v3\\n-\\n-\\nM\\n-\\nWizardCoder [164] arXiv'23\\nApache-2.0\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"-\\nGoogle\\nDialog\\n70B\\n×\\nChinchilla\\n-\\n-\\n64\\nTPU v3\\n-\\n-\\nM\\n-\\nWizardCoder [164] arXiv'23\\nApache-2.0\\nHK Bapt.\\nCoding\\n15B\\n×\\nStarCoder\\n200\\nS-78k\\n-\\n-\\n-\\n-\\n-\\n-\\nAlpaca [158]\\nGithub'23\\nApache-2.0\\nStanford\\nGeneral\\n13B\\n✓\\nLLaMA\\n3-Epoch\\nS-52k\\n8\\n80G A100\\n3h\\n600\\nFSDP\\nPyTorch\\nVicuna [159]\\nGithub'23\\nApache-2.0\\nLMSYS\\nGeneral\\n13B\\n✓\\nLLaMA\\n3-Epoch S-125k\\n-\\n-\\n-\\n-\\nFSDP\\nPyTorch\\nLIMA [185]\\narXiv'23\\n-\\nMeta+\\nGeneral\\n65B\\n-\\nLLaMA\\n15-Epoch S-1000\\n-\\n-\\n-\\n-\\n-\\n-\\nKoala [300]\\nGithub'23\\nApache-2.0\\nUC-Berkley\\nGeneral\\n13B\\n×\\nLLaMA\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"-\\nLLaMA\\n15-Epoch S-1000\\n-\\n-\\n-\\n-\\n-\\n-\\nKoala [300]\\nGithub'23\\nApache-2.0\\nUC-Berkley\\nGeneral\\n13B\\n×\\nLLaMA\\n2-Epoch S-472k\\n8\\nA100\\n6h\\n100\\n-\\nJAX/FLAX\\n5. Datasets and Evaluation\\nGenerating training and evaluation datasets is expensive be-\\ncause of the large-scale data demand of LLMs. Hence, datasets\\nfor training and benchmarking these models are topics of key\\nimportance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='importance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\\nThe performance of LLMs largely depends on the training\\ndata’s quality, size, and diversity. Preparing training datasets\\nof high quality at a large scale is laborious. Researchers have\\nsuggested various pre-training and fine-tuning datasets to en-\\nhance LLMs capabilities. We summarize these efforts in Ta-\\nble 8. While numerous training datasets are available in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='ble 8. While numerous training datasets are available in the\\nliterature, we cover the most widely used ones in our summary.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\\nsize of hidden states.\\nModels\\nType\\nTraining\\nObjective\\nAttention\\nVocab\\nTokenizer\\nNorm\\nPE\\nActivation\\nBias\\nnL\\nnH\\nHS\\nT5 (11B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n×\\n24\\n128\\n1024\\nGPT3 (175B)\\nCausal-Dec\\nNext Token\\nDense+Sparse\\n-\\n-\\nLayer\\nLearned\\nGeLU\\n✓\\n96\\n96\\n12288\\nmT5 (13B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Dense+Sparse\\n-\\n-\\nLayer\\nLearned\\nGeLU\\n✓\\n96\\n96\\n12288\\nmT5 (13B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n-\\n-\\n-\\n-\\nPanGu-α (200B)\\nCausal-Dec\\nNext Token\\nStandard\\n40k\\nBPE\\nLayer\\n-\\n-\\n-\\n64\\n128\\n16384\\nCPM-2 (198B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n-\\n24\\n64\\n-\\nCodex (12B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE+\\nPre-Layer\\nLearned\\nGeLU\\n-\\n96\\n96\\n12288\\nERNIE 3.0 (10B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n64'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='12288\\nERNIE 3.0 (10B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n64\\n4096\\nJurassic-1 (178B)\\nCausal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece∗\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n76\\n96\\n13824\\nHyperCLOVA (82B)\\nCausal-Dec\\nNext Token\\nDense+Sparse\\n-\\nBPE*\\nPre-Layer\\nLearned\\nGeLU\\n-\\n64\\n80\\n10240\\nYuan 1.0 (245B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\n-\\n-\\n-\\n-\\n-\\n76\\n-\\n16384\\nGopher (280B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n128\\n16384\\nERNIE 3.0 Titan (260B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Next Token\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n128\\n16384\\nERNIE 3.0 Titan (260B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n192\\n12288\\nGPT-NeoX-20B\\nCausal-Dec\\nNext Token\\nParallel\\n50k\\nBPE\\nLayer\\nRotary\\nGeLU\\n✓\\n44\\n64\\n-\\nOPT (175B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE\\n-\\n-\\nReLU\\n✓\\n96\\n96\\n-\\nBLOOM (176B)\\nCausal-Dec\\nNext Token\\nStandard\\n250k\\nBPE\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n112\\n14336\\nGalactica (120B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE+custom\\nLayer\\nLearned\\nGeLU\\n×\\n96'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='70\\n112\\n14336\\nGalactica (120B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE+custom\\nLayer\\nLearned\\nGeLU\\n×\\n96\\n80\\n10240\\nGLaM (1.2T)\\nMoE-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\nLayer\\nRelative\\nGeLU\\n✓\\n64\\n128\\n32768\\nLaMDA (137B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nLayer\\nRelative\\nGeGLU\\n-\\n64\\n128\\n8192\\nMT-NLG (530B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n105\\n128\\n20480\\nAlphaCode (41B)\\nEnc-Dec\\nNext Token\\nMulti-query\\n8k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n128\\n6144\\nChinchilla (70B)\\nCausal-Dec'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Enc-Dec\\nNext Token\\nMulti-query\\n8k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n128\\n6144\\nChinchilla (70B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n64\\n8192\\nPaLM (540B)\\nCausal-Dec\\nNext Token\\nParallel+Multi-query\\n256k\\nSentencePiece\\nLayer\\nRoPE\\nSwiGLU\\n×\\n118\\n48\\n18432\\nAlexaTM (20B)\\nEnc-Dec\\nDenoising\\nStandard\\n150k\\nSentencePiece\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n78\\n32\\n4096\\nSparrow (70B)\\nCausal-Dec\\nPref.&Rule RM\\n-\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n16∗\\n64\\n8192\\nU-PaLM (540B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Causal-Dec\\nPref.&Rule RM\\n-\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n16∗\\n64\\n8192\\nU-PaLM (540B)\\nNon-Causal-Dec\\nMoD\\nParallel+Multi-query\\n256k\\nSentencePiece\\nLayer\\nRoPE\\nSwiGLU\\n×\\n118\\n48\\n18432\\nUL2 (20B)\\nEnc-Dec\\nMoD\\nStandard\\n32k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n16\\n4096\\nGLM (130B)\\nNon-Causal-Dec\\nAR Blank Infilling\\nStandard\\n130k\\nSentencePiece\\nDeep\\nRoPE\\nGeGLU\\n✓\\n70\\n96\\n12288\\nCodeGen (16B)\\nCausal-Dec\\nNext Token\\nParallel\\n-\\nBPE\\nLayer\\nRoPE\\n-\\n-\\n34\\n24\\n-\\nLLaMA (65B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nPre-RMS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Parallel\\n-\\nBPE\\nLayer\\nRoPE\\n-\\n-\\n34\\n24\\n-\\nLLaMA (65B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n80\\n64\\n8192\\nPanGu-Σ (1085B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE\\nFused Layer\\n-\\nFastGeLU\\n-\\n40\\n40\\n5120\\nBloombergGPT (50B)\\nCausal-Dec\\nNext Token\\nStandard\\n131k\\nUnigram\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n40\\n7680\\nXuan Yuan 2.0 (176B)\\nCausal-Dec\\nNext Token\\nSelf\\n250k\\nBPE\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n112\\n14336\\nCodeT5+ (16B)\\nEnc-Dec\\nSC+NT+Cont.+Match\\nStandard\\n-\\nCode-Specific\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nStarCoder (15.5B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='CodeT5+ (16B)\\nEnc-Dec\\nSC+NT+Cont.+Match\\nStandard\\n-\\nCode-Specific\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nStarCoder (15.5B)\\nCausal-Dec\\nFIM\\nMulti-query\\n49k\\nBPE\\n-\\nLearned\\n-\\n-\\n40\\n48\\n6144\\nLLaMA-2 (70B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n32k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLUE\\n-\\n-\\n-\\n-\\nPaLM-2\\n-\\nMoD\\nParallel\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nLLaMA-3.1 (405B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n128k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n126\\n128\\n16384\\nNemotron-4 (340B)\\nCausal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\n-\\nRoPE\\nReLU\\n×\\n96\\n96\\n18432\\nDeepSeek (67B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Causal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\n-\\nRoPE\\nReLU\\n×\\n96\\n96\\n18432\\nDeepSeek (67B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n100k\\nBBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n95\\n64\\n8192\\nDeepSeek-v2 (67B)\\nMoE-Dec\\nNext Token\\nMulti-Head Latent\\n100k\\nBBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n60\\n128\\n5120\\n5.2. Evaluation Datasets and Tasks\\nThe evaluation of LLMs is important in gauging their profi-\\nciency and limitations. This process measures the model’s abil-\\nity to comprehend, generate, and interact with human language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='ity to comprehend, generate, and interact with human language\\nacross a spectrum of tasks. Evaluating a language model (LM)\\nis divided into two broader categories: 1) natural language un-\\nderstanding (NLU) and 2) natural language generation (NLG).\\nIt is emphasized that tasks in NLU and NLG are softly catego-\\nrized and are often used interchangeably in the literature.\\nNatural Language Understanding: It measures the language\\nunderstanding capacity of LMs. It encompasses multiple tasks,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='understanding capacity of LMs. It encompasses multiple tasks,\\nincluding sentiment analysis, text classification, natural lan-\\nguage inference (NLI), question answering (QA), common-\\nsense reasoning (CR), mathematical reasoning (MR), reading\\ncomprehension (RC), etc.\\nNatural Language Generation: It assesses the language gener-\\nation capabilities of LLMs by understanding the provided input\\ncontext. It includes tasks such as summarization, sentence com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='context. It includes tasks such as summarization, sentence com-\\npletion, machine translation (MT), dialogue generation, etc.\\nNumerous datasets are proposed for each task, evaluating\\nLLMs against different characteristics. To provide an overview\\nof evaluation datasets, we briefly discuss a few famous datasets\\nwithin each category and offer a comprehensive list of datasets\\nin Table 9. Moreover, we show a detailed overview of the train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='in Table 9. Moreover, we show a detailed overview of the train-\\ning datasets and evaluation tasks and benchmarks used by vari-\\nous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\\nble 11. We also compare the top-performing LLMs in various\\nNLP tasks in Table 12.\\n5.2.1. Multi-task\\nMMLU [307]: A benchmark that measures the knowledge\\nacquired by models during pretraining and evaluates models in\\nzero-shot and few-shot settings across 57 subjects, testing both'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='zero-shot and few-shot settings across 57 subjects, testing both\\nworld knowledge and problem-solving ability.\\nSuperGLUE [2]: A more challenging and diverse successor\\nto the GLUE [309] benchmark, SuperGLUE includes a variety\\nof language understanding tasks, such as question answering,\\nnatural language inference, and co-reference resolution. It is\\ndesigned to provide a rigorous test of language understanding\\nand requires significant progress in areas like sample-efficient,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='and requires significant progress in areas like sample-efficient,\\ntransfer, multi-task, and unsupervised or self-supervised learn-\\ning.\\nBIG-bench [308]: The BIG-bench (Behavior of Intelligent\\nGenerative Models Benchmark) is a large-scale benchmark de-\\nsigned to test the abilities of LLMs across a wide range of\\ntasks, including reasoning, creativity, ethics, and understanding\\nof specific domains.\\nGLUE [309]: The General Language Understanding Evalua-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='of specific domains.\\nGLUE [309]: The General Language Understanding Evalua-\\ntion (GLUE) benchmark is a collection of resources for train-\\ning, evaluating, and analyzing natural language understanding\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\\nfor most of the LLMs.\\nSequence\\nLR\\nOptimizers\\nPrecision\\nWeight\\nGrad\\nModels\\nBatch Size\\nLength\\nLR\\nWarmup\\nDecay\\nAdaFactorAdam AdamWFP16 BF16 Mixed Decay\\nClip\\nDropout\\nT5 (11B)\\n211\\n512\\n0.01\\n×\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nGPT3 (175B)\\n32K\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nmT5 (13B)\\n1024\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='32K\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nmT5 (13B)\\n1024\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nPanGu-α (200B)\\n-\\n1024\\n2e-5\\n-\\n-\\n-\\n-\\n-\\n-\\n✓\\n-\\n-\\n-\\n-\\nCPM-2 (198B)\\n1024\\n1024\\n0.001\\n-\\n-\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nCodex (12B)\\n-\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n-\\n-\\nERNIE 3.0 (12B)\\n6144\\n512\\n1e-4\\n✓\\nlinear\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nJurassic-1 (178B)\\n3.2M\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nHyperCLOVA (82B)\\n1024\\n-\\n6e-5\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nYuan 1.0 (245B)\\n<10M\\n2048\\n1.6e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nGopher (280B)\\n3M\\n2048\\n4e-5\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='-\\nYuan 1.0 (245B)\\n<10M\\n2048\\n1.6e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nGopher (280B)\\n3M\\n2048\\n4e-5\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n-\\n✓\\n-\\nERNIE 3.0 Titan (260B)\\n-\\n512\\n1e-4\\n✓\\nlinear\\n✓\\n✓\\n✓\\n✓\\n-\\nGPT-NeoX-20B\\n1538\\n2048\\n0.97e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nOPT (175B)\\n2M\\n2048\\n1.2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n✓\\n✓\\nBLOOM (176B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nGalactica (120B)\\n2M\\n2048\\n7e-6\\n✓\\nlinear decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n✓\\nGLaM (1.2T)\\n1M\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\nFP32 + ✓\\n-\\n✓\\n×\\nLaMDA (137B)\\n256K\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='1M\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\nFP32 + ✓\\n-\\n✓\\n×\\nLaMDA (137B)\\n256K\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMT-NLG (530B)\\n1920\\n2048\\n5e-5\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n✓\\n✓\\n-\\nAlphaCode (41B)\\n2048\\n1536+768\\n1e-4\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n✓\\n✓\\n-\\nChinchilla (70B)\\n1.5M\\n2048\\n1e-4\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n-\\n-\\n-\\nPaLM (540B)\\n2048\\n2048\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n×\\nAlexaTM (20B)\\n2M\\n1024\\n1e-4\\n-\\nlinear decay to 5%\\n✓\\n✓\\n✓\\n-\\n✓\\nU-PaLM (540B)\\n32\\n2048\\n1e-4\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\nUL2 (20B)\\n1024\\n1024\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='✓\\n✓\\n✓\\n-\\n✓\\nU-PaLM (540B)\\n32\\n2048\\n1e-4\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\nUL2 (20B)\\n1024\\n1024\\n-\\n-\\ninverse square root\\n-\\n-\\n-\\n-\\n-\\n-\\n×\\n-\\n-\\nGLM (130B)\\n4224\\n2048\\n8e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeGen (16B)\\n2M\\n2048\\n5e-5\\n✓\\ncosine\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nLLaMA (65B)\\n4M Tokens\\n2048\\n1.5e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nPanGu-Σ (1.085T)\\n512\\n1024\\n2e-5\\n✓\\n-\\n✓\\n✓\\n-\\n-\\n-\\nBloombergGPT (50B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nXuan Yuan 2.0 (176B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nCodeT5+ (16B)\\n2048\\n1024\\n2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nCodeT5+ (16B)\\n2048\\n1024\\n2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n-\\n-\\nStarCoder (15.5B)\\n512\\n8k\\n3e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n-\\n-\\nLLaMA-2 (70B)\\n4M Tokens\\n4k\\n1.5e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nLLaMA-3.1 (405B)\\n16M\\n8192\\n8e-5\\n✓\\nlinear+cosine\\n✓\\n✓\\n-\\n-\\n-\\nNemotron-4 (340B)\\n2304\\n4096\\n-\\n-\\nlinear\\n-\\n-\\n-\\n✓\\n-\\n-\\n×\\nDeepSeek (67B)\\n4608\\n4096\\n3.2e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nDeepSeek-v2 (67B)\\n9216\\n4k\\n2.4e-4\\n✓\\nstep-decay\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='4608\\n4096\\n3.2e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nDeepSeek-v2 (67B)\\n9216\\n4k\\n2.4e-4\\n✓\\nstep-decay\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\\nno model uses weight decay for instruction tuning.\\nSequence\\nOptimizers\\nGrad\\nModels\\nBatch Size\\nLength\\nLR\\nWarmup\\nLR_Decay\\nAdaFactor\\nAdam\\nAdamW\\nClip\\nDropout\\nWebGPT (175B)\\nBC:512, RM:32\\n-\\n6e-5\\n-\\n-\\n✓\\n-\\n-\\nT0 (11B)\\n1024\\n1280\\n1e-3\\n-\\n-\\n✓\\n-\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='AdamW\\nClip\\nDropout\\nWebGPT (175B)\\nBC:512, RM:32\\n-\\n6e-5\\n-\\n-\\n✓\\n-\\n-\\nT0 (11B)\\n1024\\n1280\\n1e-3\\n-\\n-\\n✓\\n-\\n✓\\nTk-Instruct (11B)\\n1024\\n-\\n1e-5\\n-\\nconstant\\n-\\n-\\n-\\n-\\n-\\nOPT-IML (175B)\\n128\\n2048\\n5e-5\\n×\\nlinear\\n✓\\n✓\\n✓\\nFlan-U-PaLM (540B)\\n32\\n-\\n1e-3\\n-\\nconstant\\n✓\\n-\\n✓\\nSparrow (70B)\\nRM: 8+16, RL:16\\n-\\n2e-6\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n×\\nWizardCoder (15B)\\n512\\n2048\\n2e-5\\n✓\\ncosine\\n-\\n-\\n-\\n-\\n-\\nAlpaca (13B)\\n128\\n512\\n1e-5\\n✓\\ncosine\\n-\\n-\\n✓\\n✓\\n×\\nVicuna (13B)\\n128\\n-2048\\n2e-5\\n✓\\ncosine\\n✓\\n-\\n×\\nLIMA (65B)\\n32\\n2048\\n1e-5\\n×\\nlinear\\n✓\\n-\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='cosine\\n-\\n-\\n✓\\n✓\\n×\\nVicuna (13B)\\n128\\n-2048\\n2e-5\\n✓\\ncosine\\n✓\\n-\\n×\\nLIMA (65B)\\n32\\n2048\\n1e-5\\n×\\nlinear\\n✓\\n-\\n✓\\nsystems. It includes a variety of tasks that test a wide range of\\nlinguistic phenomena, making it a comprehensive tool for eval-\\nuating language understanding in AI.\\n5.2.2. Language Understanding\\nWinoGrande [354]: A large-scale dataset inspired by the orig-\\ninal Winograd [357] Schema Challenge tests models on their\\nability to resolve pronoun ambiguity and encourages the devel-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='ability to resolve pronoun ambiguity and encourages the devel-\\nopment of models that understand the broad context in natural\\nlanguage text.\\nCoQA [316]: A conversational question-answering dataset,\\nCoQA challenges models with questions that rely on conver-\\nsation history and require free-form text answers. Its diverse\\ncontent from seven domains makes it a rigorous test for mod-\\nels’ ability to handle a wide range of topics and conversational\\ncontexts.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='els’ ability to handle a wide range of topics and conversational\\ncontexts.\\nWiC [317]: This dataset assesses a model’s ability to dis-\\ncern word meanings based on context, aiding in tasks related\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\nDataset\\nType\\nSize/Samples\\nTasks\\nSource\\nCreation\\nComments\\nC4 [10]\\nPretrain\\n806GB\\n-\\nCommon Crawl\\nAutomated\\nA clean, multilingual dataset with billions\\nof tokens\\nmC4 [11]\\nPretrain\\n38.49TB\\n-\\nCommon Crawl\\nAutomated\\nA multilingual extension of the C4\\ndataset, mC4 identifies over 100 lan-\\nguages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301]\\nPretrain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='guages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301]\\nPretrain\\n825GB\\n-\\nCommon Crawl, PubMed Central,\\nOpenWebText2, ArXiv, GitHub,\\nBooks3, and others\\nAutomated\\nA massive dataset comprised of 22 con-\\nstituent sub-datasets\\nROOTs [302]\\nPretrain\\n1.61TB\\n-\\n498 Hugging Face datasets\\nAutomated\\n46 natural and 13 programming lan-\\nguages\\nMassiveText [116]\\nPretrain\\n10.5TB\\n-\\nMassiveWeb, Books, News,\\nWikipedia, Github, C4\\nAutomated\\n99% of the data is in English\\nWikipedia [303]\\nPretrain\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Wikipedia, Github, C4\\nAutomated\\n99% of the data is in English\\nWikipedia [303]\\nPretrain\\n-\\n-\\nWikipedia\\nAutomated\\nDump of wikipedia\\nRedPajama [304]\\nPretrain\\n5TB\\n-\\nCommonCrawl, C4, Wikipedia,\\nGithub, Books, StackExchange\\nAutomated\\nOpen-source replica of LLaMA dataset\\nPushShift.io Reddit\\nPretrain\\n21.1GB\\n-\\nReddit\\nAutomated\\nSubmissions and comments on Reddit\\nfrom 2005 to 2019\\nBigPython [140]\\nPretrain\\n5.5TB\\nCoding\\nGitHub\\nAutomated\\n-\\nPool of Prompt (P3) [17]\\nInstructions\\n12M\\n62\\nPromptSource\\nManual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='5.5TB\\nCoding\\nGitHub\\nAutomated\\n-\\nPool of Prompt (P3) [17]\\nInstructions\\n12M\\n62\\nPromptSource\\nManual\\nA Subset of PromptSource, created from\\n177 datasets including summarization,\\nQA, classification, etc.\\nxP3 [154]\\nInstructions\\n81M\\n71\\nP3+Multilingual datasets\\nManual\\nExtending P3 to total 46 languages\\nSuper-NaturalInstructions (SNI) [18]\\nInstructions\\n12.4M\\n1616\\nMultiple datasets\\nManual\\nExtending P3 with additional multi-\\nlingual datasets, total 46 languages\\nFlan [16]\\nInstructions\\n15M\\n1836'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='lingual datasets, total 46 languages\\nFlan [16]\\nInstructions\\n15M\\n1836\\nMuffin+T0-SF+NIV2\\nManual\\nTotal 60 languages\\nOPT-IML [97]\\nInstructions\\n18.1M\\n1667\\n-\\nManual\\n-\\nSelf-Instruct [19]\\nInstructions\\n82k\\n175\\n-\\nAutomated\\nGenerated 52k instructions with 82k sam-\\nples from 175 seed tasks using GPT-3\\nAlpaca [158]\\nInstructions\\n52k\\n-\\n-\\nAutomated\\nEmployed self-instruct method to gener-\\nate data from text-davinci-003\\nVicuna [159]\\nInstructions\\n125k\\n-\\nShareGPT\\nAutomated\\nConversations\\nshared\\nby\\nusers\\non'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Vicuna [159]\\nInstructions\\n125k\\n-\\nShareGPT\\nAutomated\\nConversations\\nshared\\nby\\nusers\\non\\nShareGPT using public APIs\\nLLaMA-GPT-4 [160]\\nInstructions\\n52k\\n-\\nAlpaca\\nAutomated\\nRecreated Alpaca dataset with GPT-4 in\\nEnglish and Chinese\\nUnnatural Instructions [305]\\nInstructions\\n68k\\n-\\n15-Seeds (SNI)\\nAutomated\\n-\\nLIMA [185]\\nInstructions\\n1k\\n-\\nMultiple datasets\\nManual\\nCarefully created samples to test perfor-\\nmance with fine-tuning on less data\\nAnthropic-HH-RLHF [306]\\nAlignment\\n142k\\n-\\n-\\nManual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='mance with fine-tuning on less data\\nAnthropic-HH-RLHF [306]\\nAlignment\\n142k\\n-\\n-\\nManual\\nAnthropic-HH-RLHF-2 [178]\\nAlignment\\n39k\\n-\\n-\\nManual\\nto Word Sense Disambiguation.\\nWikitext103 [318]:\\nWith over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as lan-\\nguage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='guage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from\\nProject Gutenberg. It is specifically designed to facilitate re-\\nsearch in unsupervised learning and language modeling, with a\\nspecial focus on long-form content.\\nC4 [10]: A clean, multilingual dataset, C4 offers billions of to-\\nkens from web-crawled data. It is a comprehensive resource for\\ntraining advanced Transformer models on various languages.\\nLCQMC [320]: The Large-scale Chinese Question Matching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='LCQMC [320]: The Large-scale Chinese Question Matching\\nCorpus (LCQMC) is a dataset for evaluating the performance\\nof models in semantic matching tasks. It contains pairs of ques-\\ntions in Chinese and their matching status, making it a valuable\\nresource for research in Chinese language understanding.\\n5.2.3. Story Cloze and Sentence Completion\\nStoryCloze [334]: It introduces a new “StoryCloze Test”, a\\ncommonsense reasoning framework for evaluating story under-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='commonsense reasoning framework for evaluating story under-\\nstanding, generation, and script learning. It considers a model’s\\nability to understand and generate coherent and sensible stories.\\nLAMBADA [335]: This dataset evaluates contextual text un-\\nderstanding through a word prediction task. Models must pre-\\ndict the last word of a passage, which is easy for humans when\\ngiven the whole passage, but not when given only the last sen-\\ntence.\\n5.2.4. Physical Knowledge and World Understanding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='tence.\\n5.2.4. Physical Knowledge and World Understanding\\nPIQA [340]: A dataset that probes the physical knowledge of\\nmodels, aiming to understand how well they are learning about\\nthe real world.\\nTriviaQA [341]: A dataset that tests models on reading com-\\nprehension and open domain question answering (QA) tasks,\\nwith a focus on Information Retrieval (IR)-style QA.\\nARC [342]: A larger version of the ARC-Challenge, this\\ndataset contains both easy and challenging grade-school level,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='dataset contains both easy and challenging grade-school level,\\nmultiple-choice science questions. It is a comprehensive test of\\na model’s ability to understand and answer complex questions.\\nARC-Easy [342]:\\nA subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Table 9: Categorized evaluation datasets used in evaluating LLMs.\\nType\\nDatasets/Benchmarks\\nMulti-Task\\nMMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-\\nCLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]\\nLanguage Understanding\\nCoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],\\nCB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\\nCLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]\\nStory Cloze and\\nSentence Completion\\nStoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-\\nFC [312]\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\\nBookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]\\nContextual Language\\nUnderstanding\\nRACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],\\ncMedQA [351],cMedQA2 [352], MATINF-QA [353]\\nCommonsense Reasoning\\nWinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\\nCLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]\\nReading Comprehension\\nSQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],\\nCMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-\\ntiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\\nDuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD\\n1.0 [380], CAIL2018-Task1 & Task2 [381]\\nMathematical Reasoning\\nMATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-\\nDiv [388], MAWPS [389], SVAMP [390]\\nProblem Solving\\nHumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]\\nNatural Language Inference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Natural Language Inference\\n& Logical Reasoning\\nANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],\\nANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-\\ngyQA [349]\\nCross-Lingual Understanding\\nMLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-\\nGoldP [403], MLSum [404]\\nTruthfulness and Fact Checking\\nTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Truthfulness and Fact Checking\\nTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\\nBiases and Ethics in AI\\nETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]\\nToxicity\\nRealToxicityPrompts [413], CivilComments toxicity classification [414]\\nLanguage Translation\\nWMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]\\nScientific Knowledge\\nAminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral\\nGroups [148]\\nDialogue'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Groups [148]\\nDialogue\\nWizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],\\nKdConv [421]\\nTopic Classification\\nTNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]\\nIt is a great starting point for models beginning to explore ad-\\nvanced question-answering.\\nARC-Challenge [342]:\\nA rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='A rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval, test-\\ning the true comprehension capabilities of models.\\n5.2.5. Contextual Language Understanding\\nRACE [347]: The RACE dataset is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='tions on long and complex passages, simulating the challenge\\nof a real-world examination.\\nRACE-Middle [347]: Another subset of the RACE [347]\\ndataset, RACE-Middle, contains middle school-level English\\nexam questions. It offers a slightly less challenging but academ-\\nically oriented evaluation of a model’s comprehension skills.\\nRACE-High [347]: A subset of the RACE [347] dataset,\\nRACE-High consists of high school-level English exam ques-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='RACE-High consists of high school-level English exam ques-\\ntions. It is designed to evaluate the comprehension ability of\\nmodels in a more academic and challenging context.\\nQuAC [348]: This dataset simulates an information-seeking\\ndialog between students and teachers using hidden Wikipedia\\ntext. It introduces unique challenges not found in machine com-\\nprehension datasets, making it a valuable resource for advanc-\\ning dialog systems.\\n5.2.6. Commonsense Reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='ing dialog systems.\\n5.2.6. Commonsense Reasoning\\nHellaSwag [355]: A dataset that challenges models to pick the\\nbest ending to a context uses Adversarial Filtering to create a\\n‘Goldilocks’ zone of complexity, where generated text is absurd\\nto humans but often misclassified by models.\\nCOPA [401]: This dataset evaluates a model’s progress in\\nopen-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='comprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability to\\nunderstand and reason about cause and effect.\\nWSC [357]: The Winograd Schema Challenge (WSC) is a\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\\nis natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\\n“Mem.” is memorization.\\nBenchmark\\nModels\\nTraining Dataset\\nBIG-\\nbench\\nMMLU\\nSuper\\nGLUE\\nQA\\nClf\\nNLI\\nMT\\nCloze/\\nCompletion\\nRC\\nCR\\nMR\\nCoding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5\\nC4 [10]\\n✓\\n✓\\n✓\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='QA\\nClf\\nNLI\\nMT\\nCloze/\\nCompletion\\nRC\\nCR\\nMR\\nCoding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5\\nC4 [10]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGPT-3\\nCommon Crawl, WebText, Books Cor-\\npora, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nmT5\\nmC4 [11]\\n✓\\n✓\\n✓\\nPanGu-α\\n1.1TB Chinese Text Corpus\\n✓\\n✓\\n✓\\n✓\\n✓\\nCPM-2\\nWuDaoCorpus [109]\\n✓\\n✓\\nCodex\\n54 million public repositories from Github\\n✓\\nERNIE-3.0\\nChinese text corpora, Baidu Search, Web\\ntext, QA-long, QA-short, Poetry and Cou-\\nplet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='plet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge\\ngraph with more than 50 million facts\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nJurassic-1\\nWikipedia, OWT, Books, C4, Pile [301],\\narXiv, GitHub\\n✓\\n✓\\n✓\\n✓\\nHyperCLOVA\\nKorean blogs, Community sites, News,\\nKiN Korean Wikipedia, Wikipedia (En-\\nglish and Japanese), Modu-Corpus: Mes-\\nsenger, News, Spoken and written lan-\\nguage corpus, Web corpus\\n✓\\nYuan 1.0\\nCommon Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓\\n✓\\n✓\\n✓\\nGopher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='✓\\nYuan 1.0\\nCommon Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓\\n✓\\n✓\\n✓\\nGopher\\nsubsets of MassiveWeb Books, C4, News,\\nGitHub and Wikipedia samples from Mas-\\nsiveText\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nERNIE-3.0 TITAN\\nSame as ERNIE 3.0 and ERNIE 3.0 ad-\\nversarial dataset, ERNIE 3.0 controllable\\ndataset\\n✓\\n✓\\n✓\\n✓\\n✓\\nGPT-NeoX-20B\\nPile [301]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nOPT\\nRoBERTa [299], Pile [301], PushShift.io\\nReddit [423]\\n✓\\n✓\\n✓\\n✓\\nBLOOM\\nROOTs [13]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGalactica\\narXiv, PMC, Semantic Scholar, Wikipedia,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='✓\\n✓\\n✓\\n✓\\nBLOOM\\nROOTs [13]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGalactica\\narXiv, PMC, Semantic Scholar, Wikipedia,\\nStackExchange, LibreText, Open Text-\\nbooks, RefSeq Genome, OEIS, LIPID\\nMAPS, NASAExoplanet, Common Crawl,\\nScientificCC, AcademicCC, GitHub repos-\\nitories Khan Problems, GSM8K, OneS-\\nmallStep\\n✓\\n✓\\n✓\\n✓\\n✓\\nGLaM\\nFiltered Webpages, Social media conversa-\\ntions Wikipedia, Forums, Books, News\\n✓\\n✓\\n✓\\n✓\\n✓\\nLaMDA\\nInfiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG\\nTwo snapshots of Common Crawl and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='Infiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG\\nTwo snapshots of Common Crawl and\\nBooks3, OpenWebText2, Stack Exchange,\\nPubMed Abstracts,\\nWikipedia,\\nPG-19\\n[242], BookCorpus2, NIH ExPorter, Pile,\\nCC-Stories, RealNews\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlphaCode\\nSelected GitHub repositories, CodeCon-\\ntests: Codeforces, Description2Code, Co-\\ndeNet\\n✓\\nChinchilla\\nMassiveWeb,\\nMassiveText Books,\\nC4,\\nNews, GitHub, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM\\nwebpages, books, Wikipedia, news, arti-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='C4,\\nNews, GitHub, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM\\nwebpages, books, Wikipedia, news, arti-\\ncles, source code, social media conversa-\\ntions\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlexaTM\\nWikipedia, mC4\\n✓\\n✓\\n✓\\n✓\\n✓\\nU-PaLM\\nSame as PaLM\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nUL2\\n-\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGLM-130B\\n-\\n✓\\n✓\\n✓\\nCodeGen\\nPile, BigQuery, BigPython\\n✓\\nLLaMA\\nCommonCrawl, C4, Github, Wikipedia,\\nBooks, arXiv, StackExchange\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPanGu-Σ\\nWuDaoCorpora, CLUE, Pile, C4, Python\\ncode\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nBloombergGPT\\ninPile, Pile, C4, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeT5+'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='code\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nBloombergGPT\\ninPile, Pile, C4, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeT5+\\nCodeSearchNet, Github Code\\n✓\\n✓\\nStarCoder\\nThe Stack v1.2\\n✓\\n✓\\n✓\\n✓\\nLLaMA-2\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM-2\\nWeb documents, Code, Books, Maths,\\nConversation\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\nModels\\nTraining Dataset\\nBIG-\\nbench\\nMMLU\\nBBH\\nRAFT\\nFLAN\\nSNI\\nPromptSource\\nTyDiQA\\nHumanEval\\nMBPP\\nTruthful/\\nBias/\\nToxicity\\nT0\\nPool of Prompts\\n✓\\nWebGPT\\nELI5\\n[424],\\nELI5\\nfact-\\ncheck\\n[166],\\nTriviaQA\\n[341],\\nARC-Challenge\\n[342],\\nARC-\\nEasy\\n[342],\\nHand-written\\ndata,\\nDemonstrations of humans, Com-\\nparisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCT\\nSNI [18]\\n✓\\nmT0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='parisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCT\\nSNI [18]\\n✓\\nmT0\\nxP3 [154]\\nOPT-IML\\nPromptSource [17], FLAN [16],\\nSNI\\n[425],\\nUnifiedSKG\\n[426],\\nCrossFit\\n[427],\\nExMix\\n[428],\\nT5 [10], Reasoning\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nFlan\\nMuffin, T0-SF, NIv2, CoT\\n✓\\n✓\\n✓\\nWizardCoder\\nCode Alpaca\\n✓\\n✓\\nreading comprehension task in which a system must resolve\\nreferences in a text, often requiring world knowledge and rea-\\nsoning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='soning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering\\ndataset that requires commonsense knowledge to evaluate the\\nability of AI models to understand and answer questions.\\n5.2.7. Reading Comprehension\\nBoolQ [363]: A dataset derived from Google search queries,\\nBoolQ challenges models to answer binary (yes/no) questions.\\nThe questions are naturally occurring and are paired with a\\nparagraph from a Wikipedia article containing the answer. It'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='paragraph from a Wikipedia article containing the answer. It\\nis a test of reading comprehension and reasoning.\\nSQUADv2 [364]: The Stanford Question Answering Dataset\\n(SQuAD) [362] is a collection of questions posed by crowd\\nworkers on a set of Wikipedia articles, where the answer to ev-\\nery question is a segment of text from the corresponding reading\\npassage. SQuADv2 combines the original SQuAD1.1 dataset\\nwith over 50,000 unanswerable questions. The aim is to evalu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='with over 50,000 unanswerable questions. The aim is to evalu-\\nate a model’s ability to understand and answer questions based\\non a given context and to determine when a question is unan-\\nswerable.\\nDROP [365]: DROP, or Discrete Reasoning Over the con-\\ntent of Paragraphs, is designed to test a model’s ability to un-\\nderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='comprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]:\\nThe Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textual\\nentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\nWebQA [367]: A dataset for open-domain question answering,\\nWebQA offers a large collection of web-based question-answer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='WebQA offers a large collection of web-based question-answer\\npairs. It is designed to assess the ability of AI models to under-\\nstand and answer questions based on web content.\\nCMRC2018 [369]: This dataset is a test of Chinese language\\nmodels’ ability to reason comprehensively and is designed with\\na challenging span-extraction format that pushes the boundaries\\nof machine performance.\\n5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the\\nmathematical problem-solving abilities of AI models. It con-\\ntains a diverse set of math problems, ranging from arithmetic\\nto calculus, and is designed to test the model’s ability to under-\\nstand and solve complex mathematical problems.\\nMath23k [383]: This one challenges a model’s ability to un-\\nderstand and solve mathematical word problems. It contains'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='derstand and solve mathematical word problems. It contains\\n23,000 Chinese arithmetic word problems that require models\\nto perform reasoning and computation based on the problem\\ndescription.\\nGSM8K [384]: A dataset of diverse grade school math word\\nproblems, testing a model’s ability to perform multi-step math-\\nematical reasoning.\\n5.2.9. Problem Solving and Logical Reasoning\\nANLI [393]: A large-scale dataset designed to test the robust-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='ANLI [393]: A large-scale dataset designed to test the robust-\\nness of machine learning models in Natural Language Inference\\n(NLI) is created through an iterative, adversarial process where\\nhumans try to generate examples that models cannot correctly\\nclassify.\\nHumanEval [141]: A dataset for evaluating the problem-\\nsolving ability of AI models, which includes a diverse set of\\ntasks that require various cognitive abilities, making it a com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='tasks that require various cognitive abilities, making it a com-\\nprehensive tool for assessing general intelligence in AI.\\nStrategyQA [349]: A question-answering dataset that re-\\nquires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the bound-\\naries of what machines can understand and answer.\\n5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the\\nMultiNLI [429] corpus to 15 languages, including low-resource\\nones like Urdu. It tests models on cross-lingual sentence under-\\nstanding, with 112,500 annotated pairs across three categories:\\nentailment, contradiction, and neutral.\\nPAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver-\\nsaries from Word Scrambling, is a multilingual version of the\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='PAWS [430] dataset for paraphrase identification. It includes\\nexamples in seven languages and is designed to evaluate the\\nperformance of cross-lingual paraphrase identification models.\\n5.2.11. Truthfulness\\nTruthful-QA [405]: A unique benchmark that measures a\\nlanguage model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some designed to test the model against com-\\nmon human misconceptions.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='law, and politics, some designed to test the model against com-\\nmon human misconceptions.\\n5.2.12. Biases and Ethics in AI\\nETHOS [408]: ETHOS is a hate speech detection dataset\\nbuilt from YouTube and Reddit comments. It is a tool in the\\nfight against online hate speech, offering binary and multi-label\\nvariants for robust content moderation.\\nStereoSet [409]: StereoSet is a comprehensive dataset de-\\nsigned to measure and evaluate the presence of stereotypical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='signed to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. Contrasting stereotypi-\\ncal bias against language modeling ability provides a valuable\\ntool for understanding and mitigating biases in large language\\nmodels.\\n6. Applications\\nApplying Large Language Models (LLMs) to a variety of\\ndownstream tasks has become a popular trend in both AI-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='downstream tasks has become a popular trend in both AI-\\nrelated research communities and industries, with many emerg-\\ning uses being discovered and explored daily. LLMs, which are\\ncapable of understanding and generating human-like text, have\\nfound meaningful applications across a variety of fields. This\\nsection provides an overview of LLM applications in medicine,\\neducation, science, mathematics, law, finance, robotics, and\\ncoding. While each of these domains pose different challenges,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='coding. While each of these domains pose different challenges,\\nLLMs open up opportunities to make significant contributions\\nto these domains through their generalizability.\\nGeneral Purpose:\\nLLMs are being widely considered as\\ngeneral-purpose tools for a wide variety of tasks [431]. This\\nis due to their inherent ability to understand, generate, and\\nmanipulate human-like text in a contextually relevant man-\\nner. This allows them to perform tasks ranging from simple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='ner. This allows them to perform tasks ranging from simple\\nlanguage translation and question-answering to more complex\\ntasks like summarization, text generation, and even program-\\nming help [432]. The utility of LLMs is further enhanced by\\ntheir ability to adapt to the specific style and tone of the text\\nthey are processing, making the outputs more user-friendly and\\ncontext-aware. In everyday applications, LLMs can be used\\nas personal assistants, helping users draft emails or schedule'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='as personal assistants, helping users draft emails or schedule\\nappointments [433]; they can also be deployed in customer ser-\\nvice to handle common questions or applied to generate content\\nfor digital platforms like websites by creating human-like text\\nbased on given prompts [434]. Moreover, LLMs play a cru-\\ncial role in data analysis, where they can filter large volumes of\\ntext data, summarize key points, and find patterns that would'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='text data, summarize key points, and find patterns that would\\ntake humans much longer to identify [435]. Despite their wide-\\nranging applications, it is essential to remember that LLMs,\\nsimilar to any AI system, are only as good as the data they have\\nbeen trained on.\\nMedicine: The application of LLMs in the field of medicine is\\nreshaping healthcare delivery and research. For example, LLMs\\nare increasingly used in clinical decision support systems to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='are increasingly used in clinical decision support systems to\\nprovide physicians with evidence-based treatment recommen-\\ndations [436, 437, 438]. By analyzing patient data and medical\\nliterature, they can help identify potential diagnoses, suggest\\nappropriate tests, and recommend optimal treatment strategies.\\nMoreover, LLMs can also enhance patient interactions with\\nhealthcare systems; e.g., they can be used in chatbot applica-\\ntions [439, 440, 441] to answer patient queries about symptoms'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='tions [439, 440, 441] to answer patient queries about symptoms\\nor medications, schedule appointments, and even provide es-\\nsential health advice. For medical research, LLMs are used to\\nextract and filter information from a considerable amount of\\nmedical literature, identify relevant studies, summarize find-\\nings, and even predict future research trends [442, 443, 444].\\nFor medical education, LLMs can help create training mate-\\nrials, generate exam questions, provide detailed explanations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='rials, generate exam questions, provide detailed explanations\\nof complex medical topics, and offer personalized feedback to\\nstudents [445, 446, 447, 448]. They can also simulate patient\\ninteractions, enabling students to practice and improve their\\nclinical skills. At a broader level, LLMs can assist in public\\nhealth initiatives by analyzing media data to detect disease out-\\nbreaks, monitor public sentiment towards health policies, and\\ndisseminate health information in a clear and understandable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='disseminate health information in a clear and understandable\\nmanner [449]. LLMs can be employed to support public health\\ninitiatives, addressing related issues such as data privacy, the\\nnecessity for explainability, and the potential risk of propagat-\\ning biases [450, 451].\\nEducation: The integration of LLMs into the educational sec-\\ntor offers opportunities to enhance learning experiences, teacher\\nsupport, and educational content development. For students, by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='support, and educational content development. For students, by\\nanalyzing their learning styles, performance, and preferences,\\nLLMs can provide customized study materials and practice\\nquestions to develop personalized learning experiences [452].\\nFor teachers, LLMs can help to create lesson plans and grade\\nassignments and generate diverse and inclusive educational\\ncontent, significantly saving more time for teaching and student\\ninteraction [453, 454]. In language learning, LLMs serve as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='interaction [453, 454]. In language learning, LLMs serve as\\nadvanced conversational partners capable of simulating conver-\\nsations in multiple languages, correcting grammar, enhancing\\nvocabulary, and aiding pronunciation for the needs of fluency\\nin practice [455]. Furthermore, LLMs improve accessibility\\nin education by providing support for students with disabili-\\nties. They can generate real-time transcriptions for the hear-\\ning impaired, offer reading assistance for the visually impaired,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='ing impaired, offer reading assistance for the visually impaired,\\nand simplify complex texts for those with learning disabili-\\nties [451]. As LLMs continue to evolve, their applications in\\neducation can benefit more students and teachers from different\\nperspectives in practice.\\nScience: Similar to medical applications, LLMs can expedite\\nthe research process by quickly analyzing and summarizing sci-\\nentific literature. By briefing comprehensible and accessible re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='entific literature. By briefing comprehensible and accessible re-\\nsearch summaries, LLMs can assist researchers in staying up-\\nto-date with the latest findings, even in fields outside their area\\nof expertise [456, 457]. In addition, LLMs can aid scientists\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\\nto the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\\nbenchmark.\\nTask\\nDataset/Benchmark\\nTop-1\\nTop-2\\nTop-3\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nMulti-Task\\nBIG-bench (B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Score (N-shots)\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nMulti-Task\\nBIG-bench (B)\\nChinchilla (70B)\\n65.1 (5-shot)\\nGopher (280B)\\n53.97 (5-shot)\\nPaLM (540B)\\n53.7 (5-shot)\\nMMLU (B)\\nGPT-4 (-)\\n86.4 (5-shot)\\nGemini (Ultra)\\n83.7 (5-shot)\\nFlan-PaLM-2( f) (Large)\\n81.2 (5-shot)\\nLanguage Understanding\\nSuperGLUE (B)\\nERNIE 3.0 (12B)\\n90.6 (-)\\nPaLM(f) (540B)\\n90.4 (-)\\nT5 (11B)\\n88.9 (-)\\nStory Comprehension and\\nGeneration\\nHellaSwag\\nGPT-4 (-)\\n95.3 (10-shot)\\nGemini (Ultra)\\n87.8 (10-shot)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Story Comprehension and\\nGeneration\\nHellaSwag\\nGPT-4 (-)\\n95.3 (10-shot)\\nGemini (Ultra)\\n87.8 (10-shot)\\nPaLM-2 (Large)\\n86.8 (one shot)\\nStoryCloze\\nGPT3 (175B)\\n87.7 (few shot)\\nPaLM-2 (Large)\\n87.4 (one shot)\\nOPT (175B)\\n79.82 (-)\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA\\nPaLM-2 (Large)\\n85.0 (one shot)\\nLLaMa (65B)\\n82.8 (zero shot)\\nMT-NLG (530B)\\n81.99 (zero shot)\\nTriviaQA\\nPaLM-2 (Large)\\n86.1 (one shot)\\nLLaMA-2 (70B)\\n85.0 (one shot)\\nPaLM (540B)\\n81.4 (one shot)\\nContextual Language\\nUnderstanding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='LLaMA-2 (70B)\\n85.0 (one shot)\\nPaLM (540B)\\n81.4 (one shot)\\nContextual Language\\nUnderstanding\\nLAMBADA\\nPaLM (540B)\\n89.7 (few shot)\\nMT-NLG (530B)\\n87.15 (few shot)\\nPaLM-2 (Large)\\n86.9 (one shot)\\nCommonsense Reasoning\\nWinoGrande\\nGPT-4 (-)\\n87.5 (5-shot)\\nPaLM-2 (Large)\\n83.0 (one shot)\\nPaLM (540B)\\n81.1 (zero shot)\\nSIQA\\nLLaMA (65B)\\n52.3 (zero shot)\\nChinchilla (70B)\\n51.3 (zero shot)\\nGopher (280B)\\n50.6 (zero shot)\\nReading Comprehension\\nBoolQ\\nPaLM(f) (540B)\\n92.2 (-)\\nT5 (11B)\\n91.2 (-)\\nPaLM-2 (Large)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Reading Comprehension\\nBoolQ\\nPaLM(f) (540B)\\n92.2 (-)\\nT5 (11B)\\n91.2 (-)\\nPaLM-2 (Large)\\n90.9 (one shot)\\nTruthfulness\\nTruthful-QA\\nLLaMA (65B)\\n57 (-)\\nMathematical Reasoning\\nMATH\\nGemini (Ultra)\\n53.2 (4-shot)\\nPaLM-2 (Large)\\n34.3 (4-shot)\\nLLaMa-2 (65B)\\n13.5 (4-shot)\\nGSM8K\\nGPT-4 (-)\\n92.0 (5-shot)\\nPaLM-2 (Large)\\n80.7 (8-shot)\\nU-PaLM (540B)\\n58.5 (-)\\nProblem Solving and\\nLogical Reasoning\\nHumanEval\\nGemini( f) (Ultra)\\n74.4 (zero shot)\\nGPT-4 (-)\\n67.0 (zero shot)\\nCode Llama (34B)\\n48.8 (zero shot)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Gemini( f) (Ultra)\\n74.4 (zero shot)\\nGPT-4 (-)\\n67.0 (zero shot)\\nCode Llama (34B)\\n48.8 (zero shot)\\nin formulating new hypotheses and research questions since\\ntheir ability to process large-scale datasets allows them to un-\\nveil insights that might not be immediately apparent to human\\nresearchers [458]. Moreover, for scientific writing, LLMs can\\nhelp researchers draft documents, suggest improvements, and\\nensure adherence to specific formatting guidelines [459, 460].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='ensure adherence to specific formatting guidelines [459, 460].\\nThis not only saves time but also improves the clarity of scien-\\ntific communication, enabling interdisciplinary teams to work\\ntogether more effectively.\\nMaths: In addition to providing mathematical research and\\neducation support, LLMs can assist in solving mathematical\\nproblems by giving step-by-step explanations and guiding users\\nthrough complex proofs and calculations. They can help iden-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='through complex proofs and calculations. They can help iden-\\ntify errors in reasoning or computation and suggest corrections,\\nserving as an invaluable tool for both learning and verification\\npurposes [461, 462]. LLMs can be employed to check the valid-\\nity of mathematical proofs, offering a preliminary filter before\\nhuman review. While they are not a substitute for the meticu-\\nlous work of mathematicians, they can help simplify the process'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='lous work of mathematicians, they can help simplify the process\\nof proof verification [463, 464]. Moreover, LLMs enhance ac-\\ncessibility to mathematics by translating complex concepts and\\nfindings into understandable language for non-specialists [465],\\nwhere the gap between theoretical mathematics and applied\\ncontexts such as physics, engineering, and economics can be\\nbridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='bridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-\\numents, including generating initial coding for datasets, iden-\\ntifying themes, and classifying data according to these themes.\\nThis collaborative effort between legal experts and LLMs has\\nproved to be effective in analyzing legal texts such as court\\nopinions on theft, improving both the efficiency and quality of\\nthe research [466]. Additionally, LLMs have been evaluated for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='the research [466]. Additionally, LLMs have been evaluated for\\ntheir ability to generate explanations of legal terms, focusing\\non improving factual accuracy and relevance by incorporating\\nsentences from case law. By feeding relevant case law into the\\nLLM, the augmented models can generate higher-quality expla-\\nnations with less factually incorrect information [467]. More-\\nover, LLMs can be trained with specialized domain knowledge\\nto perform legal reasoning tasks [468] and answer legal ques-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='to perform legal reasoning tasks [468] and answer legal ques-\\ntions [469].\\nFinance: LLMs like BloombergGPT [151], trained on exten-\\nsive proprietary financial datasets, exhibit superior performance\\non financial tasks. This indicates the value of domain-specific\\ntraining in creating LLMs that can more accurately understand\\nand process industry-specific language and concepts. The intro-\\nduction of FinGPT [470] as an open-source model offers trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='duction of FinGPT [470] as an open-source model offers trans-\\nparent and accessible resources to develop novel applications\\nsuch as robo-advising, algorithmic trading, and low-code so-\\nlutions, ultimately expanding the capabilities of financial ser-\\nvices. Both BloombergGPT and FinGPT show the adaptabil-\\nity of LLMs to the financial domain, with the former showing\\nthe power of custom datasets and the latter emphasizing a data-\\ncentric approach and low-rank adaptation techniques for cus-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='centric approach and low-rank adaptation techniques for cus-\\ntomization. Moreover, LLMs demonstrate an ability to break\\ndown complex financial tasks into actionable plans, enabling\\nend-to-end solutions that were previously unfeasible with a sin-\\ngle model [471].\\nRobotics: In robotics research, LLMs have promising appli-\\ncations, such as enhancing human-robot interaction [28, 472,\\n473, 474], task planning [237], motion planning [246], nav-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='473, 474], task planning [237], motion planning [246], nav-\\nigation [246, 475], object manipulation [236], personalized\\nrobots [476], etc. LLMs enable robots to understand the en-\\nvironment effectively and generate plans to complete tasks col-\\nlaboratively [240, 26]. They can facilitate continuous learning\\nby allowing robots to access and integrate information from a\\nwide range of sources, helping robots acquire new skills, adapt\\nto changes, and refine their paths [224, 233, 234].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='to changes, and refine their paths [224, 233, 234].\\n7. Challenges and Future Directions\\nLLMs such as GPT-4 and its predecessors have significantly\\nadvanced natural language processing. Nevertheless, they also\\nbring along a set of challenges. The computational cost, ad-\\nversarial robustness, and interpretability are among the tech-\\nnical challenges that are intrinsic to these models.\\nFurther-\\nmore, as these models are scaled up to handle more complex\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tasks or to operate in more complex or dynamic environments,\\nnew challenges in scalability, privacy, and real-time processing\\nemerge. On the frontier of foundational research, integrating\\nmulti-modality and the effectiveness of transfer learning are be-\\ning keenly explored. Additionally, the continuous learning as-\\npect of these models, which aims to have models that can adapt\\nto new information over time, presents a fresh set of challenges.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='to new information over time, presents a fresh set of challenges.\\nThese challenges not only underscore the technical intricacies\\ninvolved but also highlight the broader impact and the future\\ntrajectory of LLMs in real-world applications. The following\\nsections delve into these challenges, shedding light on the on-\\ngoing and potential efforts to address them.\\nComputational Cost: Training LLMs require extensive compu-\\ntational resources, which increases production costs and raises'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tational resources, which increases production costs and raises\\nenvironmental concerns due to substantial energy consump-\\ntion during large-scale training. Improved performance occurs\\nas computational resources increase, but the rate of improve-\\nment gradually decreases when both the model and dataset\\nsize remain fixed, following the power law of diminishing re-\\nturns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='turns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-\\nases in their training data. These biases can manifest in the\\nmodel’s outputs, leading to potential ethical and fairness is-\\nsues [478].\\nOverfitting: Although LLMs possess substantial learning ca-\\npabilities, they are susceptible to overfitting noisy and peculiar\\npatterns within their extensive training data. Consequently, this\\nmay cause them to generate illogical responses [479]. The de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='may cause them to generate illogical responses [479]. The de-\\nbate about Memorization vs. Generalization in LLMs is about\\nfinding the right balance. Memorization allows the model to\\nremember specific details from its training data, ensuring it can\\nprovide accurate answers to precise questions. However, gen-\\neralization enables the model to make inferences and produce\\nresponses for inputs it has not seen before, which is essential\\nfor handling various real-world tasks. Striking the right bal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='for handling various real-world tasks. Striking the right bal-\\nance is the challenge: too much memorization can lead to over-\\nfitting, making the model inflexible and struggling with new\\ninputs [480].\\nEconomic and Research Inequality: The high cost of train-\\ning and deploying LLMs may make their development concen-\\ntrated within well-funded organizations, potentially worsening\\neconomic and research inequalities in AI [481].\\nReasoning and Planning: Some reasoning and planning tasks,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Reasoning and Planning: Some reasoning and planning tasks,\\neven as seemingly simple as common-sense planning, which\\nhumans find easy, remain well beyond the current capabilities\\nof LLMs evaluated using an assessment framework. This is not\\nentirely unexpected, considering that LLMs primarily generate\\ntext completions based on likelihood and offer no solid guaran-\\ntees in terms of reasoning abilities [482].\\nHallucinations: LLMs exhibit “hallucinations\", where they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Hallucinations: LLMs exhibit “hallucinations\", where they\\ngenerate responses that, while sounding plausible, are incorrect\\nor do not align with the provided information [483]. Hallucina-\\ntions can be categorized into three categories.\\n• Input-conflicting hallucination, wherein LLMs produce\\ncontent that diverges from the input given by users.\\n• Context-conflicting hallucination, where LLMs generate\\ncontent that contradicts information they have generated\\nearlier.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='content that contradicts information they have generated\\nearlier.\\n• Fact-conflicting hallucination involves LLM’s generation\\nof content that does not align with established world\\nknowledge.\\nPrompt Engineering: Prompts serve as inputs to LLMs, and\\ntheir syntax and semantics play a crucial role in determining\\nthe model’s output. The prompt variations, sometimes counter-\\nintuitive to humans, can result in significant changes in model\\noutput and are addressed through prompt engineering, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='output and are addressed through prompt engineering, which\\ninvolves designing natural language queries to guide LLMs\\nresponses effectively [484, 32].\\nLimited Knowledge: Information acquired during pretraining\\nis limited and may become obsolete after some time.\\nRe-\\ntraining the model using updated data is costly. To generate\\nfactually accurate responses, people use a retrieval augmen-\\ntation pipeline [198].\\nHowever, pre-trained models are not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tation pipeline [198].\\nHowever, pre-trained models are not\\ntrained with retrieval augmentation generation (RAG) [6, 21];\\nhence, adapting the training pipeline is necessary [193, 25].\\nSafety and Controllability: Using LLMs comes with the risk\\nof generating harmful, misleading, or inappropriate content,\\nwhether by accident or when given specific prompts. Ensuring\\nthese models are safely utilized is a significant concern [485].\\nSecurity and Privacy: LLMs are prone to leaking personal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Security and Privacy: LLMs are prone to leaking personal\\ninformation and generating false, unethical, misaligned re-\\nsponses. Researchers have explored various security attacks,\\ni.e., backdoor attacks, jailbreaking, prompt injection, and data\\npoisoning, that lead to breaking LLMs security.\\nTherefore,\\ndeveloping better defense mechanisms is essential to ensure\\nLLMs are safe, reliable, and trustworthy for complex AI\\napplications [486].\\nMulti-Modality:\\nMulti-modal learning, where LLMs are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='applications [486].\\nMulti-Modality:\\nMulti-modal learning, where LLMs are\\ntrained on diverse data like text, images, and videos, aims to\\ncreate models with richer understanding but faces challenges\\nin data alignment, fusion strategies, and higher computational\\ndemands.\\nCatastrophic Forgetting:\\nLLMs are often pre-trained on\\nlarge datasets and then fine-tuned on domain-specific data,\\nreducing training resources.\\nHowever, they face issues like'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='reducing training resources.\\nHowever, they face issues like\\ndomain adaptation and catastrophic forgetting, which hinder\\nthe retention of original knowledge when learning new tasks.\\nAdversarial Robustness:\\nLarge Language Models (LLMs)\\nhave shown great capabilities in various tasks but are vul-\\nnerable to adversarial attacks, where slight, deliberate input\\nalterations can mislead them.\\nEspecially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Especially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-\\nthough it sometimes compromises generalization [487].\\nAs\\nLLMs integrate more into complex systems, examining their\\nsecurity properties becomes crucial, given the emerging field\\nof adversarial attacks on LLMs within trustworthy ML [488].\\nThis vulnerability is notable in safety-critical domains, ne-\\ncessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='cessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].\\nInterpretability and Explainability: The “black-box” nature\\nof LLMs poses challenges in understanding their decision-\\nmaking, which is crucial for broader acceptance and trust,\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='especially in sensitive domains.\\nDespite their advanced\\ncapabilities, the lack of insight into their operation limits their\\neffectiveness and trustworthiness [490, 491]. Efforts are being\\nmade to make LLMs more explainable to promote user trust\\nand to ensure responsible AI usage. Understanding the logic\\nbehind LLMs’ responses is essential for fostering trust and\\nensuring they align with human values and legal standards.\\nPrivacy Concerns:\\nPrivacy concerns in Large Language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='Privacy Concerns:\\nPrivacy concerns in Large Language\\nModels (LLMs) have escalated with their growth in complexity\\nand size, particularly around data sharing and potential misuse.\\nThere is a risk of malicious content creation, filter bypass,\\nand data privacy issues, especially in e-commerce, where\\nprotecting customer privacy is crucial. If models are trained\\non private data, additional concerns arise if such models are\\nmade publicly available. LLMs tend to memorize phrases from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='made publicly available. LLMs tend to memorize phrases from\\ntheir training sets, which an adversary could exploit to extract\\nsensitive data, posing a threat to personal privacy [492, 493].\\nReal-Time Processing: Real-time processing in Large Lan-\\nguage Models (LLMs) is pivotal for various applications,\\nespecially with the rising popularity of mobile AI applications\\nand concerns regarding information security and privacy.\\nHowever, LLMs often have hundreds of layers and millions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='However, LLMs often have hundreds of layers and millions\\nof parameters, which impede real-time processing due to the\\nhigh computational demands and limited weight storage on\\nhardware platforms, particularly in edge computing environ-\\nments [494].\\nWhile certain efforts like MobileBERT aim\\nto reduce memory requirements, they still face substantial\\nexecution overhead due to the large number of model layers,\\nleading to high inference latency.\\nLong-Term Dependencies:\\nLarge Language Models have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='leading to high inference latency.\\nLong-Term Dependencies:\\nLarge Language Models have\\nshown considerable progress in understanding and generating\\ntext, yet they often struggle with preserving context and\\nhandling long-term dependencies, particularly in complex,\\nmulti-turn conversations or long documents. This limitation\\ncan lead to incoherent or irrelevant responses.\\nHardware Acceleration: The growth of LLMs presents signif-\\nicant hardware challenges due to the increasing computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='icant hardware challenges due to the increasing computational\\nand memory demands associated with training and deploying\\nthese models. GPUs have played a crucial role in meeting the\\nhardware requirements for training LLMs, with the networking\\nindustry also evolving to optimize hardware for training\\nworkloads. However, the growing size of LLMs, which has\\nbeen outpacing hardware progress, makes model inference in-\\ncreasingly costly. Model quantization is a promising approach'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='creasingly costly. Model quantization is a promising approach\\nto bridge the widening gap between LLM size and hardware\\ncapacity [495].\\nAlthough specialized hardware acceleration\\nlike GPUs or TPUs can significantly reduce the computational\\ncost, making real-time applications more feasible, they may not\\nfully resolve all limitations, necessitating further advancements\\nin hardware technology.\\nRegulatory and Ethical Frameworks: The rapid advancements'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='in hardware technology.\\nRegulatory and Ethical Frameworks: The rapid advancements\\nin artificial intelligence have given rise to sophisticated Large\\nLanguage Models (LLMs) like OpenAI’s GPT-4 [157] and\\nGoogle’s Bard. These developments underscore the imperative\\nfor regulatory oversight to manage the ethical and social\\nchallenges accompanying LLMs’ widespread use [496]. For\\ninstance, LLMs can generate content that can be used posi-\\ntively or negatively, emphasizing the need for proactive ethical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='tively or negatively, emphasizing the need for proactive ethical\\nframeworks and policy measures to guide their responsible\\nuse and assign accountability for their outputs [497]. Auditing\\nis identified as a promising governance mechanism to ensure\\nthat AI systems, including LLMs, are designed and deployed\\nethically, legally, and technically robust [498].\\n8. Conclusion\\nThis article has comprehensively reviewed the develop-\\nments in LLMs.\\nIt contributes to summarizing significant'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='ments in LLMs.\\nIt contributes to summarizing significant\\nfindings of LLMs in the existing literature and provides a\\ndetailed analysis of the design aspects, including architec-\\ntures, datasets, and training pipelines.\\nWe identified crucial\\narchitectural components and training strategies employed by\\ndifferent LLMs.\\nThese aspects are presented as summaries\\nand discussions throughout the article.\\nMoreover, we have\\ndiscussed the performance differences of LLMs in zero-shot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='Moreover, we have\\ndiscussed the performance differences of LLMs in zero-shot\\nand few-shot settings, explored the impact of fine-tuning, and\\ncompared supervised and generalized models and encoder vs.\\ndecoder vs. encoder-decoder architectures. A comprehensive\\nreview of multi-modal LLMs, retrieval augmented LLMs,\\nLLMs-powered agents, efficient LLMs, datasets, evaluation,\\napplications, and challenges is also provided. This article is\\nanticipated to serve as a valuable resource for researchers,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='anticipated to serve as a valuable resource for researchers,\\noffering insights into the recent advancements in LLMs and\\nproviding fundamental concepts and details to develop better\\nLLMs.\\nAcknowledgement:\\nThe author/s would like to acknowl-\\nedge the support received from Saudi Data and AI Authority\\n(SDAIA) and King Fahd University of Petroleum and Miner-\\nals (KFUPM) under SDAIA-KFUPM Joint Research Center for\\nArtificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='Artificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\\ntory” for natural language processing?, in:\\nMachine Learning and\\nKnowledge Discovery in Databases. Research Track: European Con-\\nference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\\nProceedings, Part III 21, Springer, 2021, pp. 677–693. 1\\n[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, S. Bowman, Superglue: A stickier benchmark for general-\\npurpose language understanding systems, Advances in neural informa-\\ntion processing systems 32 (2019). 1, 26, 29\\n[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\\nZ. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human-\\nlike open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\\n[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n151 (2) (2022) 183–197. 2\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\\nLanguage models are unsupervised multitask learners, OpenAI blog\\n1 (8) (2019) 9. 2, 7\\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\\nare few-shot learners, Advances in neural information processing sys-\\ntems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018). 2, 18, 24\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nL. Zettlemoyer, Deep contextualized word representations, in: NAACL-\\nHLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\\n2\\n[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV. Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehen-\\nsion, arXiv preprint arXiv:1910.13461 (2019). 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='sion, arXiv preprint arXiv:1910.13461 (2019). 2\\n[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\\na unified text-to-text transformer, The Journal of Machine Learning Re-\\nsearch 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31\\n[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-\\ntext transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\\n25, 28, 30\\n[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,\\nJ. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\\nguage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\\nparameter open-access multilingual language model, arXiv preprint\\narXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30\\n[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\\nM. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer\\nlanguage models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24,\\n25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\\ning language modeling with pathways, arXiv preprint arXiv:2204.02311\\n(2022). 2, 6, 9, 11, 23, 24, 25\\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\\nlanguage models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='language models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31\\n[17] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\\nprompted training enables zero-shot task generalization, arXiv preprint\\narXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31\\n[18] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\\nSuper-naturalinstructions: Generalization via declarative instructions on\\n1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\\n11, 16, 17, 24, 25, 28, 31\\n[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\\njishirzi, Self-instruct: Aligning language model with self generated in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='jishirzi, Self-instruct: Aligning language model with self generated in-\\nstructions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28\\n[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\\nels to follow instructions with human feedback, Advances in Neural In-\\nformation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\\n22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\\nfoundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\\n(2023). 2, 7, 10, 16, 25, 34\\n[22] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\\ngatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\\nlarge language models, arXiv preprint arXiv:2206.07682 (2022). 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='large language models, arXiv preprint arXiv:2206.07682 (2022). 2\\n[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\\nlanguage models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\\n[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\\nentific research capabilities of large language models, arXiv preprint\\narXiv:2304.05332 (2023). 2\\n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\\nretrieval augmented language models, arXiv preprint arXiv:2208.03299\\n(2022). 2, 18, 19, 34\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='multimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33\\n[27] A. Parisi, Y. Zhao, N. Fiedel, Talm: Tool augmented language models,\\narXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models\\nfor human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\\n33\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi,\\nY. Shi, et al., mplug-owl: Modularization empowers large language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='Y. Shi, et al., mplug-owl: Modularization empowers large language\\nmodels with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\\n22\\n[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\\nT. Lu, J. Zhou, Y. Qiao, et al., Visionllm: Large language model\\nis also an open-ended decoder for vision-centric tasks, arXiv preprint\\narXiv:2305.11175 (2023). 2, 22\\n[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:\\nTeaching large language model to use tools via self-instruction, arXiv\\npreprint arXiv:2305.18752 (2023). 2, 19, 22, 23\\n[32] E.\\nSaravia,\\nPrompt\\nEngineering\\nGuide,\\nhttps://github.com/dair-\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\\nW. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\\nmodel, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25\\n[34] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5+:\\nOpen code large language models for code understanding and genera-\\ntion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25\\n[35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\\nY. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\\nedge enhanced pre-training for language understanding and generation,\\narXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25\\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: System op-\\ntimizations enable training deep learning models with over 100 billion\\nparameters, in: Proceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5\\n[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimiza-\\ntions toward training trillion parameter models, in: SC20: International\\nConference for High Performance Computing, Networking, Storage and\\nAnalysis, IEEE, 2020, pp. 1–16. 2, 4, 24\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\\na unified view of parameter-efficient transfer learning, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='a unified view of parameter-efficient transfer learning, arXiv preprint\\narXiv:2110.04366 (2021). 2, 20, 21\\n[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\\nria, Llm-adapters: An adapter family for parameter-efficient fine-tuning\\nof large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\\n20\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\\nefficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\\nFrom dense to sparse: Contrastive pruning for better pre-trained lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='From dense to sparse: Contrastive pruning for better pre-trained lan-\\nguage model compression, in: Proceedings of the AAAI Conference on\\nArtificial Intelligence, Vol. 36, 2022, pp. 11547–11555. 2, 22\\n[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\\nAccurate and efficient post-training quantization for large language\\nmodels, in: ICML, Vol. 202 of Proceedings of Machine Learning Re-\\nsearch, PMLR, 2023, pp. 38087–38099. 2, 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='search, PMLR, 2023, pp. 38087–38099. 2, 21\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nCompression of generative pre-trained language models via quantiza-\\ntion, arXiv preprint arXiv:2203.10705 (2022). 2, 21\\n[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\\nGiraffe: Adventures in expanding context lengths in llms, arXiv preprint\\narXiv:2308.10882 (2023). 2, 17\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\\nEfficient con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\\nEfficient con-\\ntext window extension of large language models, arXiv preprint\\narXiv:2309.00071 (2023). 2, 17\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Longt5: Efficient text-to-text transformer for long sequences, arXiv\\npreprint arXiv:2112.07916 (2021). 2, 18\\n[49] S. Chen, S. Wong, L. Chen, Y. Tian, Extending context window\\nof large language models via positional interpolation, arXiv preprint\\narXiv:2306.15595 (2023). 2, 17\\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\\nJ. Zhang, Z. Dong, et al., A survey of large language models, arXiv\\npreprint arXiv:2303.18223 (2023). 2, 3, 7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='preprint arXiv:2303.18223 (2023). 2, 3, 7\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\\nvey on word representation models: From classical to state-of-the-art\\nword representation language models, Transactions on Asian and Low-\\nResource Language Information Processing 20 (5) (2021) 1–35. 2, 3\\n[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='E. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\\ncessing via large pre-trained language models: A survey, arXiv preprint\\narXiv:2111.01243 (2021). 2, 3\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He, et al., A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\\n2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\\narXiv:2301.00234 (2022). 2, 7, 18\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\\n[56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\\nQ. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='preprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='ING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='words with subword units, in: Proceedings of the 54th Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long\\nPapers), 2016, pp. 1715–1725. 4\\n[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\\nIEEE international conference on acoustics, speech and signal process-\\ning (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='A. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization in\\nnlp, arXiv preprint arXiv:2112.10508 (2021). 4\\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017). 4, 7\\n[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\\nlinear biases enables input length extrapolation, in: International Con-\\nference on Learning Representations, 2022.\\nURL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\\n[66] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: En-\\nhanced transformer with rotary position embedding, arXiv preprint\\narXiv:2104.09864 (2021). 4, 9, 17\\n[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\\nwith sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\\n23\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness, Advances in Neural\\nInformation Processing Systems 35 (2022) 16344–16359. 4\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='are universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n[70] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\\nmann machines, in: Proceedings of the 27th international conference on\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\npreprint arXiv:1606.08415 (2016). 4\\n[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overfitting, The\\njournal of machine learning research 15 (1) (2014) 1929–1958. 4\\n[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\\nKe, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regular-\\nizing rnns by randomly preserving hidden activations, arXiv preprint\\narXiv:1606.01305 (2016). 4\\n[74] N.\\nShazeer,\\nGlu\\nvariants\\nimprove\\ntransformer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='arXiv:1606.01305 (2016). 4\\n[74] N.\\nShazeer,\\nGlu\\nvariants\\nimprove\\ntransformer,\\narXiv\\npreprint\\narXiv:2002.05202 (2020). 4\\n[75] Y. N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\\ngated convolutional networks, in: International conference on machine\\nlearning, PMLR, 2017, pp. 933–941. 4\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\narXiv:1607.06450 (2016). 4\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\nin Neural Information Processing Systems 32 (2019). 4\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\\ntransformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\\n[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\\nMegatron-lm: Training multi-billion parameter language models using\\nmodel parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\\n[81] \"bmtrain: Efficient training for big models.\".\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n[82] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\\ntac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='tac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\\nart natural language processing, in: Proceedings of the 2020 conference\\non empirical methods in natural language processing: system demon-\\nstrations, 2020, pp. 38–45. 5\\n[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\\nrin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\\nJax: composable transformations of python+ numpy programs (2018).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Jax: composable transformations of python+ numpy programs (2018).\\n5\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You,\\nColossal-ai: A unified deep learning system for large-scale parallel train-\\ning, arXiv preprint arXiv:2110.14883 (2021). 5\\n[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe:\\nA\\nfast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\\n(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\\nwork, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\\n162. 5\\n[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\\native style, high-performance deep learning library, Advances in neural\\ninformation processing systems 32 (2019). 5\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\\nscale machine learning., in: Osdi, Vol. 16, Savannah, GA, USA, 2016,\\npp. 265–283. 5\\n[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,\\nB. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and efficient machine\\nlearning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='learning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\\nlion parameter models with simple and efficient sparsity, The Journal of\\nMachine Learning Research 23 (1) (2022) 5232–5270. 5, 9\\n[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\\nY. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Y. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\\nmodels with mixture-of-experts, in: International Conference on Ma-\\nchine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25\\n[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-P: Towards trillion\\nparameter language model with sparse heterogeneous computing, arXiv\\npreprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='preprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, C. Raffel, What language model architecture and pretrain-\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='ing objective works best for zero-shot generalization?, in: International\\nConference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou,\\nH.-W. Hon, Unified language model pre-training for natural language\\nunderstanding and generation, Advances in neural information process-\\ning systems 32 (2019). 6\\n[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\\nS. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\\nmodels, arXiv preprint arXiv:2001.08361 (2020). 6\\n[96] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\\net al., Training compute-optimal large language models, arXiv preprint\\narXiv:2203.15556 (2022). 6, 9, 25, 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='arXiv:2203.15556 (2022). 6, 9, 25, 29\\n[97] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\\nT. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\\nstruction meta learning through the lens of generalization, arXiv preprint\\narXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\\n[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan,\\nPrinciple-driven self-alignment of language models from scratch with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='Principle-driven self-alignment of language models from scratch with\\nminimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\\n7, 17\\n[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\\nN. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\\nas a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\\n7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\\nP. Christiano, G. Irving, Fine-tuning language models from human pref-\\nerences, arXiv preprint arXiv:1909.08593 (2019). 7\\n[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\\ntion: Improving zero-shot and few-shot learning of language models via\\nchain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='chain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\\nining the power of symbolic tasks in instruction tuning, arXiv preprint\\narXiv:2304.07995 (2023). 7, 16\\n[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\\nD. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\\nlanguage models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='language models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23\\n[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022). 7, 20\\n[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. Narasimhan,\\nTree of thoughts: Deliberate problem solving with large language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='Tree of thoughts: Deliberate problem solving with large language mod-\\nels, arXiv preprint arXiv:2305.10601 (2023). 7, 20\\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\\ning for nlp, in: International Conference on Machine Learning, PMLR,\\n2019, pp. 2790–2799. 7, 20\\n[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\\nof large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\\n[108] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang, et al., Pangu-α : Large-scale autoregressive pre-\\ntrained chinese language models with auto-parallel computation, arXiv\\npreprint arXiv:2104.12369 (2021). 8, 23, 24, 25\\n[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\\nJ. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\\ntraining language models, AI Open 2 (2021) 65–68. 8, 30\\n[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY. Zhao, Y. Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation, arXiv preprint\\narXiv:2107.02137 (2021). 8, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='arXiv:2107.02137 (2021). 8, 25\\n[111] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov,\\nTransformer-xl: Attentive language models beyond a fixed-length con-\\ntext, arXiv preprint arXiv:1901.02860 (2019). 8\\n[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\\n[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\\nficiencies of self-attention, Advances in Neural Information Processing\\nSystems 33 (2020) 22640–22651. 8, 11\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\\nmodels bring?\\nintensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='generative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25\\n[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\\nL. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\\nshot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\\n24, 25\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='J. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\\nguage models: Methods, analysis & insights from training gopher, arXiv\\npreprint arXiv:2112.11446 (2021). 8, 9, 25, 28\\n[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al.,\\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a\\nlarge-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='large-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25\\n[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\\nH. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\\nsource autoregressive language model, arXiv preprint arXiv:2204.06745\\n(2022). 9, 23, 24, 25\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9\\n[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\\ncision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\\n[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\\nton, J. Dean, Outrageously large neural networks: The sparsely-gated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='ton, J. Dean, Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\\n[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\\natm 20b: Few-shot learning using a large-scale multilingual seq2seq\\nmodel, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25\\n[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\\narXiv preprint arXiv:2305.10403 (2023). 9, 25\\n[124] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia,\\nH. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\\nwith 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\\n24, 25\\n[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='24, 25\\n[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\\nguage learning paradigms, in: The Eleventh International Conference\\non Learning Representations, 2022. 9, 10, 24, 25\\n[126] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\\neral language model pretraining with autoregressive blank infilling, in:\\nProceedings of the 60th Annual Meeting of the Association for Compu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='Proceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. 10\\n[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and efficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023). 10, 23, 25\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\npreprint arXiv:2112.05682 (2021). 10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='preprint arXiv:2112.05682 (2021). 10\\n[129] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\nM. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\\ntransformer models, Proceedings of Machine Learning and Systems 5\\n(2023). 10\\n[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of\\nmodels, arXiv preprint arXiv:2407.21783 (2024). 10, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='models, arXiv preprint arXiv:2407.21783 (2024). 10, 25\\n[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25\\n[132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n25\\n[133] https://github.com/xai-org/grok-1. 10\\n[134] https://x.ai/blog/grok-1.5. 10\\n[135] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly\\ncapable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='capable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10\\n[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem-\\nini 1.5: Unlocking multimodal understanding across millions of tokens\\nof context, arXiv preprint arXiv:2403.05530 (2024). 10\\n[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun-\\ndyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b\\ntechnical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25\\n[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\\nQ. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\\nwith longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25\\n[139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,\\nC. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li,\\nF. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang,\\nH. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\\nJ. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao,\\nK. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li,\\nM. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang,\\nP. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge,\\nR. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye,\\nS. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\\nT. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao,\\nW. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen,\\nX. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical,\\nand efficient mixture-of-experts language model, CoRR abs/2405.04434\\n(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,\\nC. Xiong, Codegen: An open large language model for code with multi-\\nturn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11,\\n23, 25, 28\\n[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\\nwards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31\\n[142] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\\ncode generation with alphacode, Science 378 (6624) (2022) 1092–1097.\\n11, 23, 25, 29\\n[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\narXiv preprint arXiv:1911.02150 (2019). 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='arXiv preprint arXiv:1911.02150 (2019). 11\\n[144] R. Y. Pang, H. He, Text generation by learning from demonstrations,\\narXiv preprint arXiv:2009.07839 (2020). 11\\n[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine\\ntranslation models, arXiv preprint arXiv:2009.09372 (2020). 11\\n[146] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and genera-\\ntion, arXiv preprint arXiv:2109.00859 (2021). 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='tion, arXiv preprint arXiv:2109.00859 (2021). 11\\n[147] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\\nwith you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25\\n[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\\nA. Poulton, V. Kerkez, R. Stojnic, Galactica: A large language model for\\nscience, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='science, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\\n[149] FairScale authors, Fairscale: A general purpose modular pytorch library\\nfor high performance and large scale training, https://github.com/\\nfacebookresearch/fairscale (2021). 11\\n[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models\\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='for dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\\n[151] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33\\n[152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\\ncial chat model with hundreds of billions parameters, arXiv preprint\\narXiv:2305.12002 (2023). 11, 17, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='arXiv:2305.12002 (2023). 11, 17, 25\\n[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\nformer language model with jax (2021). 12, 24\\n[154] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\\nCrosslingual generalization through multitask finetuning, arXiv preprint\\narXiv:2211.01786 (2022). 16, 25, 28, 31\\n[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\\nDynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue, et al., Llama-adapter v2: Parameter-efficient visual in-\\nstruction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\\n[157] Openai. gpt-4 technical report (2023). 16, 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[157] Openai. gpt-4 technical report (2023). 16, 35\\n[158] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\\nT. B. Hashimoto, Stanford alpaca:\\nAn instruction-following llama\\nmodel,\\nhttps://github.com/tatsu-lab/stanford_alpaca\\n(2023). 16, 25, 28\\n[159] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\\nS. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).\\nURL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\\n28\\n[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprint\\narXiv:2304.06975 (2023). 16\\n[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\\nWizardlm: Empowering large language models to follow complex in-\\nstructions, arXiv preprint arXiv:2304.12244 (2023). 16\\n[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nD. Jiang, Wizardcoder: Empowering code large language models with\\nevol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\\n[165] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\\ning language models to support answers with verified quotes, arXiv\\npreprint arXiv:2203.11147 (2022). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='preprint arXiv:2203.11147 (2022). 17\\n[166] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-\\nassisted question-answering with human feedback, arXiv preprint\\narXiv:2112.09332 (2021). 17, 19, 20, 25, 31\\n[167] A. Glaese, N. McAleese, M. Tr˛ebacz, J. Aslanides, V. Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\\nalignment of dialogue agents via targeted human judgements, arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='alignment of dialogue agents via targeted human judgements, arXiv\\npreprint arXiv:2209.14375 (2022). 17, 20, 25\\n[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\\nDirect preference optimization: Your language model is secretly a re-\\nward model, arXiv preprint arXiv:2305.18290 (2023). 17\\n[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\\nT. Zhang, Raft: Reward ranked finetuning for generative foundation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='T. Zhang, Raft: Reward ranked finetuning for generative foundation\\nmodel alignment, arXiv preprint arXiv:2304.06767 (2023). 17\\n[170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\\nresponses to align language models with human feedback without tears,\\narXiv preprint arXiv:2304.05302 (2023). 17\\n[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, H. Wang, Preference rank-\\ning optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='ing optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17\\n[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\\ntuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\\n17\\n[173] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\\nA. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\\nlessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='lessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\\n[174] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang,\\nT. B. Hashimoto,\\nAlpacafarm:\\nA simulation frame-\\nwork for methods that learn from human feedback, arXiv preprint\\narXiv:2305.14387 (2023). 17\\n[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\\nPrompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='Prompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17\\n[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯ut˙e, A. Chen,\\nA. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\\nity for moral self-correction in large language models, arXiv preprint\\narXiv:2302.07459 (2023). 17\\n[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 17\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath,\\nB. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\\nguage models to reduce harms: Methods, scaling behaviors, and lessons\\nlearned, arXiv preprint arXiv:2209.07858 (2022). 17, 28\\n[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\\nlish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='lish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17\\n[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\\nN. McAleese, G. Irving, Red teaming language models with language\\nmodels, arXiv preprint arXiv:2202.03286 (2022). 17\\n[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\\ncontinual learners, in: Proceedings of the 2022 Conference on Empirical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='continual learners, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 6107–6122. 17\\n[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\\nC. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='C. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17\\n[184] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong,\\nJ. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\\nof low training data instruction tuning, arXiv preprint arXiv:2305.09246\\n(2023). 17\\n[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\\nP. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\\narXiv:2305.11206 (2023). 17, 25, 28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='arXiv:2305.11206 (2023). 17, 25, 28\\n[186] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm-infinite: Sim-\\nple on-the-fly length generalization for large language models, arXiv\\npreprint arXiv:2308.16137 (2023). 17, 18\\n[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan-\\nskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay, et al., Colt5: Faster\\nlong-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='long-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18\\n[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\\nLongnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\\narXiv:2307.02486 (2023). 18\\n[189] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\\ncient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='cient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18\\n[190] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend,\\nE. Karpas, A. Shashua, K. Leyton-Brown, Y. Shoham, Parallel context\\nwindows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank:\\nEnhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18\\n[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\\nReflexion: Language agents with verbal reinforcement learning, arXiv\\npreprint arXiv:2303.11366 14 (2023). 18, 20\\n[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\\ning llms with databases as their symbolic memory, arXiv preprint\\narXiv:2306.03901 (2023). 18\\n[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, G. Neubig, Active retrieval augmented generation, arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='J. Callan, G. Neubig, Active retrieval augmented generation, arXiv\\npreprint arXiv:2305.06983 (2023). 18\\n[198] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, Y. Shoham, In-context retrieval-augmented language models,\\narXiv preprint arXiv:2302.00083 (2023). 18, 34\\n[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='improve with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18\\n[200] D. Schuurmans, Memory augmented large language models are compu-\\ntationally universal, arXiv preprint arXiv:2301.04589 (2023). 18\\n[201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\\ngeneral read-write memory for large language models, arXiv preprint\\narXiv:2305.14322 (2023). 18\\n[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\\nwork: Bm25 and beyond, Foundations and Trends® in Information Re-\\ntrieval 3 (4) (2009) 333–389. 18\\n[203] X. Wang,\\nJ. Wei,\\nD. Schuurmans,\\nQ. Le,\\nE. Chi,\\nD. Zhou,\\nRationale-augmented ensembles in language models, arXiv preprint\\narXiv:2207.00747 (2022). 18\\n[204] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-G. Lou, W. Chen,\\nRepocoder: Repository-level code completion through iterative retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='Repocoder: Repository-level code completion through iterative retrieval\\nand generation, arXiv preprint arXiv:2303.12570 (2023). 18\\n[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\\nO. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study, arXiv preprint\\narXiv:2304.06762 (2023). 19\\n[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19\\n[207] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What makes\\ngood in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\\n(2021). 19\\n[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='context learning, arXiv preprint arXiv:2112.08633 (2021). 19\\n[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\\nmodels, arXiv preprint arXiv:2301.12652 (2023). 19\\n[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\narXiv preprint arXiv:2306.13421 (2023). 19\\n[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\\nlanguage model pre-training, in: International conference on machine\\nlearning, PMLR, 2020, pp. 3929–3938. 19\\n[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: Efficient and ef-\\nfective retrieval-augmented text generation, in: Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2023, pp. 1437–1447. 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='Information Retrieval, 2023, pp. 1437–1447. 19\\n[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\nation, arXiv preprint arXiv:2107.07566 (2021). 19\\n[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\\naugmented language models through few-shot prompting for open-\\ndomain question answering, arXiv preprint arXiv:2203.05115 (2022).\\n19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\\ngpt: A general multi-modal assistant that can plan, execute, inspect, and\\nlearn, arXiv preprint arXiv:2306.08640 (2023). 19\\n[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\\nJ. Gao, Chameleon: Plug-and-play compositional reasoning with large\\nlanguage models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23\\n[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\\nRibeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\\nguage models, arXiv preprint arXiv:2303.09014 (2023). 19\\n[218] C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y. Fujii, A. Ratner, C.-Y. Lee, R. Kr-\\nishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\\nlarge language models, arXiv preprint arXiv:2308.00675 (2023). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='large language models, arXiv preprint arXiv:2308.00675 (2023). 19\\n[219] Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt:\\nConnecting large language models with real-world applications via rest-\\nful apis, arXiv preprint arXiv:2306.06624 (2023). 19\\n[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\\nguage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='guage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='lation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='20\\n[224] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv-\\ning ai tasks with chatgpt and its friends in huggingface, arXiv preprint\\narXiv:2303.17580 (2023). 19, 20, 33\\n[225] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji,\\nS. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\\ndation models with millions of apis, arXiv preprint arXiv:2303.16434\\n(2023). 19\\n[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='(2023). 19\\n[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\\n[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\\nuser assistance systems, Business & Information Systems Engineering\\n58 (2016) 367–370. 20\\n[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\\nmulti-agent collaborative framework, arXiv preprint arXiv:2308.00352\\n(2023). 20\\n[230] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou, et al., The rise and potential of large language model\\nbased agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\\n[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\\nX. Chen, Y. Lin, et al., A survey on large language model based au-\\ntonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20\\n[232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\\nshot planners: Extracting actionable knowledge for embodied agents,\\nin: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='in: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20\\n[233] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\\ning with language model is planning with world model, arXiv preprint\\narXiv:2305.14992 (2023). 20, 33\\n[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy,\\nZ. Chen, J. Zhang, D. Arpit, et al., Retroformer:\\nRetrospective\\nlarge language agents with policy gradient optimization, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='Retrospective\\nlarge language agents with policy gradient optimization, arXiv preprint\\narXiv:2308.02151 (2023). 20, 33\\n[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\\nlogue: Embodied reasoning through planning with language models, in:\\n6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20\\n[236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\\nEmbodied finetuning for vision-language reasoning in robot manipula-\\ntion, arXiv preprint arXiv:2305.18898 (2023). 20, 33\\n[237] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, A. Garg, Progprompt: Generating situated robot task plans'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='J. Thomason, A. Garg, Progprompt: Generating situated robot task plans\\nusing large language models, in: 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20,\\n33\\n[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\\nChiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\\nfor robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='for robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\\n[239] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein,\\nMedagents: Large language models as collaborators for zero-shot med-\\nical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20\\n[240] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\\nJ. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\\nGrounding language in robotic affordances, in: Conference on Robot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='Grounding language in robotic affordances, in: Conference on Robot\\nLearning, PMLR, 2023, pp. 287–318. 20, 33\\n[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\\nguided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\\n20\\n[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\\nnav: Grounding large language models for dynamic planning to navi-\\ngation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='gation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20\\n[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su,\\nLlm-planner: Few-shot grounded planning for embodied agents with\\nlarge language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n[244] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\\npreprint arXiv:2303.03480 (2023). 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='preprint arXiv:2303.03480 (2023). 20\\n[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\\nrobot navigation, in: 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\\n[246] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\\nwith large language models for object rearrangement, arXiv preprint\\narXiv:2303.06247 (2023). 20, 33\\n[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\\n[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-efficient tun-\\ning: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\\n20\\n[249] Y. Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\\nAdamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\\nguage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='guage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\\n[250] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\\nW. Chen, Lora: Low-rank adaptation of large language models, arXiv\\npreprint arXiv:2106.09685 (2021). 21, 22, 23\\n[251] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks, in: Pro-\\nceedings of the 60th Annual Meeting of the Association for Computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='ceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 2: Short Papers), 2022, pp. 61–68. 21\\n[252] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\\nProgressive prompts: Continual learning for language models, arXiv\\npreprint arXiv:2301.12314 (2023). 21\\n[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\\nwards adaptive prefix tuning for parameter-efficient language model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='wards adaptive prefix tuning for parameter-efficient language model\\nfine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\\n[254] E. B. Zaken, S. Ravfogel, Y. Goldberg, Bitfit:\\nSimple parameter-\\nefficient fine-tuning for transformer-based masked language-models,\\narXiv preprint arXiv:2106.10199 (2021). 21\\n[255] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8 ():\\n8-bit matrix multiplication for transformers at scale, arXiv preprint\\narXiv:2208.07339 (2022). 21, 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='arXiv:2208.07339 (2022). 21, 22\\n[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq:\\nAccurate\\npost-training quantization for generative pre-trained transformers, arXiv\\npreprint arXiv:2210.17323 (2022). 21\\n[257] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\\npression+: Accurate quantization of large language models by equiva-\\nlent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='lent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21\\n[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\\naccurate post-training quantization and pruning, Advances in Neural In-\\nformation Processing Systems 35 (2022) 4475–4488. 21\\n[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, arXiv\\npreprint arXiv:2306.02272 (2023). 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='preprint arXiv:2306.02272 (2023). 21\\n[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\\nW. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained language models, arXiv\\npreprint arXiv:2210.03858 (2022). 21\\n[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient\\nfinetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='finetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22\\n[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr-\\nishnamoorthi, V. Chandra, Llm-qat: Data-free quantization aware train-\\ning for large language models, arXiv preprint arXiv:2305.17888 (2023).\\n21, 22\\n[263] Y. Guo, A. Yao, H. Zhao, Y. Chen, Network sketching: Exploiting bi-\\nnary structure in deep cnns, in: Proceedings of the IEEE Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='nary structure in deep cnns, in: Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\\nMemory-efficient fine-tuning of compressed large language models via\\nsub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\\n22\\n[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and effective pruning\\napproach for large language models, arXiv preprint arXiv:2306.11695'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='approach for large language models, arXiv preprint arXiv:2306.11695\\n(2023). 22\\n[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[267] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,\\nY. Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\\nmissing secret sauce for pruning llms to high sparsity, arXiv preprint\\narXiv:2310.05175 (2023). 22\\n[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nStructured pruning for efficient generative pre-trained language models,\\nin: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='in: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22\\n[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\\nguage model for few-shot learning, Advances in Neural Information Pro-\\ncessing Systems 35 (2022) 23716–23736. 22\\n[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models,\\narXiv preprint arXiv:2301.12597 (2023). 22\\n[271] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv preprint\\narXiv:2304.08485 (2023). 22\\n[272] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang,\\nY. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22\\n[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\\ntailed video understanding via large vision and language models, arXiv\\npreprint arXiv:2306.05424 (2023). 22\\n[274] H. Zhang,\\nX. Li,\\nL. Bing,\\nVideo-llama:\\nAn instruction-tuned\\naudio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='audio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22\\n[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\\nY. Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\\ndio captioning dataset for audio-language multimodal research, arXiv\\npreprint arXiv:2303.17395 (2023). 22\\n[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\\nllm: Multi-modal language modeling with image, audio, video, and text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='llm: Multi-modal language modeling with image, audio, video, and text\\nintegration, arXiv preprint arXiv:2306.09093 (2023). 22\\n[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models,\\narXiv preprint arXiv:2304.10592 (2023). 22\\n[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020). 22\\n[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\\nS. Hoi, Instructblip: Towards general-purpose vision-language models\\nwith instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\\n[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\\nshot learning via instruction tuning, arXiv preprint arXiv:2212.10773\\n(2022). 22\\n[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\\nChatbridge: Bridging modalities with large language model as a lan-\\nguage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\\n[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,\\nX. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\\nlingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\\n[283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\\nH. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\\narXiv preprint arXiv:2305.14167 (2023). 22\\n[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\\nEfficient vision-language instruction tuning for large language models,\\narXiv preprint arXiv:2305.15023 (2023). 22\\n[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y. Qiao,\\nLlama-adapter: Efficient fine-tuning of language models with zero-init\\nattention, arXiv preprint arXiv:2303.16199 (2023). 22\\n[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\\nRobust speech recognition via large-scale weak supervision, in: Inter-\\nnational Conference on Machine Learning, PMLR, 2023, pp. 28492–\\n28518. 22\\n[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\\nmodal chain-of-thought reasoning in language models, arXiv preprint\\narXiv:2302.00923 (2023). 23\\n[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt\\ntuning in vision language models, arXiv preprint arXiv:2304.07919\\n(2023). 23\\n[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\\ning, drawing and editing with visual foundation models, arXiv preprint\\narXiv:2303.04671 (2023). 23\\n[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\\nM. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\\nsoning and action, arXiv preprint arXiv:2303.11381 (2023). 23\\n[291] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao,\\nS. Zhao, Y. Shan, et al., Caption anything: Interactive image descrip-\\ntion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\\n(2023). 23\\n[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\\nAdapting clip for powerful 3d open-world learning, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='Adapting clip for powerful 3d open-world learning, arXiv preprint\\narXiv:2211.11682 (2022). 23\\n[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\\nsoning without training, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\\n23\\n[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\\nfusion with intra-and inter-modality attention flow for visual question'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='fusion with intra-and inter-modality attention flow for visual question\\nanswering, in: Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition, 2019, pp. 6639–6648. 23\\n[295] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Deep modular co-attention net-\\nworks for visual question answering, in: Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition, 2019, pp. 6281–\\n6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\\nW. Chang, S.-F. Chang, Idealgpt:\\nIteratively decomposing vision\\nand language reasoning via large language models, arXiv preprint\\narXiv:2305.14985 (2023). 23\\n[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li,\\nPrompt, generate, then cache: Cascade of foundation models makes\\nstrong few-shot learners, in: Proceedings of the IEEE/CVF Conference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='strong few-shot learners, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\\n23\\n[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\\nnormalization of self-attention, CoRR abs/1910.05895 (2019). 24\\n[299] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-\\ntraining approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='training approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\\n[300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nD. Song, Koala: A dialogue model for academic research, Blog post\\n(April 2023).\\nURL\\nhttps://bair.berkeley.edu/blog/2023/04/03/koala/\\n25\\n[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile:\\nAn\\n800gb dataset of diverse text for language modeling, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='An\\n800gb dataset of diverse text for language modeling, arXiv preprint\\narXiv:2101.00027 (2020). 28, 30\\n[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen,\\net al., The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset, Advances in Neural Information Processing Systems 35 (2022)\\n31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n[304] Together Computer, Redpajama: An open source recipe to reproduce\\nllama training dataset (Apr. 2023).\\nURL\\nhttps://github.com/togethercomputer/\\nRedPajama-Data 28\\n[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\\nTuning language models with (almost) no human labor, arXiv preprint\\narXiv:2212.09689 (2022). 28\\n[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='arXiv:2212.09689 (2022). 28\\n[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\\nD. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\\nharmless assistant with reinforcement learning from human feedback,\\narXiv preprint arXiv:2204.05862 (2022). 28\\n[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\\nJ. Steinhardt, Measuring massive multitask language understanding,\\narXiv preprint arXiv:2009.03300 (2020). 26, 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='arXiv preprint arXiv:2009.03300 (2020). 26, 29\\n[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='the imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models, arXiv preprint arXiv:2206.04615 (2022). 26, 29\\n[309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\\nA multi-task benchmark and analysis platform for natural language un-\\nderstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29\\n[310] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='J. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\\neration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\\n29\\n[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu,\\nC. Yu, et al., Clue: A chinese language understanding evaluation bench-\\nmark, arXiv preprint arXiv:2004.05986 (2020). 29\\n[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='X. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\\nbenchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y.-L. Boureau, Can\\nyou put it all together: Evaluating conversational agents’ ability to blend\\nskills, arXiv preprint arXiv:2004.08449 (2020). 29\\n[314] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of\\nlanguage models, arXiv preprint arXiv:2211.09110 (2022). 29\\n[315] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\\nY. Song, T. Oh, et al., Klue: Korean language understanding evaluation,\\narXiv preprint arXiv:2105.09680 (2021). 29\\n[316] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\\nanswering challenge, Transactions of the Association for Computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='answering challenge, Transactions of the Association for Computational\\nLinguistics 7 (2019) 249–266. 27, 29\\n[317] M.\\nT.\\nPilehvar,\\nJ.\\nCamacho-Collados,\\nWic:\\n10,000\\nexample\\npairs for evaluating context-sensitive representations, arXiv preprint\\narXiv:1808.09121 6 (2018). 27, 29\\n[318] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\\nmodels, arXiv preprint arXiv:1609.07843 (2016). 28, 29\\n[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\\nsive transformers for long-range sequence modelling, arXiv preprint\\narXiv:1911.05507 (2019). 28, 29\\n[320] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\\nlarge-scale chinese question matching corpus, in: Proceedings of the\\n27th international conference on computational linguistics, 2018, pp.\\n1952–1962. 28, 29\\n[321] S.\\nIyer,\\nN.\\nDandekar,\\nK.\\nCsernai,\\nFirst\\nquora\\ndataset\\nre-\\nlease:\\nQuestion\\npairs,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[321] S.\\nIyer,\\nN.\\nDandekar,\\nK.\\nCsernai,\\nFirst\\nquora\\ndataset\\nre-\\nlease:\\nQuestion\\npairs,\\nhttps://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs. 29\\n[322] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\\ncoreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\\n[323] M.-C. De Marneffe, M. Simons, J. Tonhauser, The commitmentbank: In-\\nvestigating projection in naturally occurring discourse, in: proceedings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='vestigating projection in naturally occurring discourse, in: proceedings\\nof Sinn und Bedeutung, Vol. 23, 2019, pp. 107–124. 29\\n[324] Z. Li, N. Ding, Z. Liu, H. Zheng, Y. Shen, Chinese relation extraction\\nwith multi-grained information and external linguistic knowledge, in:\\nProceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics, 2019, pp. 4377–4386. 29\\n[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\\nand relation extraction dataset for chinese literature text, arXiv preprint\\narXiv:1711.07010 (2017). 29\\n[326] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\\nlarge-scale domain-specific chinese corpus for sentence semantic equiv-\\nalence identification, in: Proceedings of the 2018 conference on empiri-\\ncal methods in natural language processing, 2018, pp. 4946–4951. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='cal methods in natural language processing, 2018, pp. 4946–4951. 29\\n[327] B. Liu, D. Niu, H. Wei, J. Lin, Y. He, K. Lai, Y. Xu, Matching arti-\\ncle pairs with graphical decomposition and convolutions, arXiv preprint\\narXiv:1802.07459 (2018). 29\\n[328] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Dataset and neu-\\nral recurrent sequence labeling model for open-domain factoid question\\nanswering, arXiv preprint arXiv:1607.06275 (2016). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='answering, arXiv preprint arXiv:1607.06275 (2016). 29\\n[329] N. Peng, M. Dredze, Named entity recognition for chinese social media\\nwith jointly trained embeddings, in: Proceedings of the 2015 conference\\non empirical methods in natural language processing, 2015, pp. 548–\\n554. 29\\n[330] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\\nnale generation: Learning to solve and explain algebraic word problems,\\narXiv preprint arXiv:1705.04146 (2017). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='arXiv preprint arXiv:1705.04146 (2017). 29\\n[331] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\\nlease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\\ntium (2011). 29\\n[332] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\\ncomplex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\\n[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\\nin social media: A case study of african-american english, arXiv preprint\\narXiv:1608.08868 (2016). 29\\n[334] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, J. Allen, A corpus and evaluation framework\\nfor deeper understanding of commonsense stories, arXiv preprint\\narXiv:1604.01696 (2016). 28, 29\\n[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\\nWord prediction requiring a broad discourse context, arXiv preprint\\narXiv:1606.06031 (2016). 28, 29\\n[336] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\\nrization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\\n[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\\nation with planning-based hierarchical variational model, arXiv preprint\\narXiv:1908.06605 (2019). 29\\n[338] J. Novikova, O. Dušek, V. Rieser, The e2e dataset: New challenges for\\nend-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\\n[339] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\\nfor cloze test, arXiv preprint arXiv:1906.01265 (2019). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='for cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\\n[340] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about phys-\\nical commonsense in natural language, in: Proceedings of the AAAI\\nconference on artificial intelligence, Vol. 34, 2020, pp. 7432–7439. 28,\\n29\\n[341] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\\ndistantly supervised challenge dataset for reading comprehension, arXiv\\npreprint arXiv:1705.03551 (2017). 28, 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='preprint arXiv:1705.03551 (2017). 28, 29, 31\\n[342] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nO. Tafjord, Think you have solved question answering? try arc, the ai2\\nreasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 28, 29,\\n31\\n[343] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost:\\nPhys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='Phys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29\\n[344] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering, arXiv\\npreprint arXiv:1809.02789 (2018). 29\\n[345] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\\nwebnlg+ shared task overview and evaluation results (webnlg+ 2020),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='webnlg+ shared task overview and evaluation results (webnlg+ 2020),\\nin: Proceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG+), 2020. 29\\n[346] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\\nA chinese dataset for cant understanding with common sense and world\\nknowledge, arXiv preprint arXiv:2104.02704 (2021). 29\\n[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\\nLarge-scale'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\\nLarge-scale\\nreading comprehension dataset from examinations, arXiv preprint\\narXiv:1704.04683 (2017). 29\\n[348] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\\nL. Zettlemoyer, Quac: Question answering in context, arXiv preprint\\narXiv:1808.07036 (2018). 29\\n[349] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\\ntle use a laptop? a question answering benchmark with implicit reason-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='tle use a laptop? a question answering benchmark with implicit reason-\\ning strategies, Transactions of the Association for Computational Lin-\\nguistics 9 (2021) 346–361. 29, 31\\n[350] J. Boyd-Graber, B. Satinoff, H. He, H. Daumé III, Besting the quiz mas-\\nter: Crowdsourcing incremental classification games, in: Proceedings of\\nthe 2012 joint conference on empirical methods in natural language pro-\\ncessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='cessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29\\n[351] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\\nquestion answer matching using end-to-end character-level multi-scale\\ncnns, Applied Sciences 7 (8) (2017) 767. 29\\n[352] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\\nteraction networks for chinese medical question answer selection, IEEE\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Access 6 (2018) 74061–74071. 29\\n[353] C. Xu, J. Pei, H. Wu, Y. Liu, C. Li, Matinf: A jointly labeled large-scale\\ndataset for classification, question answering and summarization, arXiv\\npreprint arXiv:2004.12302 (2020). 29\\n[354] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y. Choi, Winogrande: An\\nadversarial winograd schema challenge at scale, Communications of the\\nACM 64 (9) (2021) 99–106. 27, 29\\n[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a\\nmachine really finish your sentence?, arXiv preprint arXiv:1905.07830\\n(2019). 29\\n[356] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\\nnatives: An evaluation of commonsense causal reasoning., in: AAAI\\nspring symposium: logical formalizations of commonsense reasoning,\\n2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\\nlenge, in: Thirteenth international conference on the principles of knowl-\\nedge representation and reasoning, 2012. 27, 29\\n[358] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, arXiv preprint\\narXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='arXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:\\nCommonsense reasoning about social interactions, arXiv preprint\\narXiv:1904.09728 (2019). 29\\n[360] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\\nlenging chinese machine reading comprehension, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 141–155. 29\\n[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\\ning the gap between human and machine commonsense reading compre-\\nhension, arXiv preprint arXiv:1810.12885 (2018). 29\\n[362] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\\nfor machine comprehension of text, arXiv preprint arXiv:1606.05250\\n(2016). 29, 31\\n[363] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\\nK. Toutanova, Boolq: Exploring the surprising difficulty of natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='K. Toutanova, Boolq: Exploring the surprising difficulty of natural\\nyes/no questions, arXiv preprint arXiv:1905.10044 (2019). 29, 31\\n[364] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\\nable questions for squad, arXiv preprint arXiv:1806.03822 (2018). 29,\\n31\\n[365] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\\nA reading comprehension benchmark requiring discrete reasoning over\\nparagraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='paragraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\\n[366] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\\ntailment challenge, in: Machine learning challenges workshop, Springer,\\n2005, pp. 177–190. 29, 31\\n[367] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y. Bisk, Webqa: Mul-\\ntihop and multimodal qa, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='on Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31\\n[368] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\\nevaluation on chinese machine reading comprehension, arXiv preprint\\narXiv:1709.08299 (2017). 29\\n[369] Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\\nA span-extraction dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:1810.07366 (2018). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='arXiv preprint arXiv:1810.07366 (2018). 29, 31\\n[370] Y. Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\\nA sentence cloze dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:2004.03116 (2020). 29\\n[371] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, Y. Wang, Character-based bilstm-crf\\nincorporating pos and dictionaries for chinese opinion target extraction,\\nin: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='in: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29\\n[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\\ning beyond the surface: A challenge set for reading comprehension\\nover multiple sentences, in: Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, Volume 1 (Long Papers), 2018,\\npp. 252–262. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='tics: Human Language Technologies, Volume 1 (Long Papers), 2018,\\npp. 252–262. 29\\n[373] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\\nberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\\ntions: a benchmark for question answering research, Transactions of the\\nAssociation for Computational Linguistics 7 (2019) 453–466. 29\\n[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-\\nchine reading comprehension dataset, arXiv preprint arXiv:1806.00920\\n(2018). 29\\n[375] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu,\\nQ. She, et al., Dureader: a chinese machine reading comprehension\\ndataset from real-world applications, arXiv preprint arXiv:1711.05073\\n(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A\\nchinese dataset towards evaluating the robustness of machine reading\\ncomprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\\n[377] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\\nquestions, arXiv preprint arXiv:1707.06209 (2017). 29\\n[378] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\\nranking with kernel pooling, in: Proceedings of the 40th International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='ranking with kernel pooling, in: Proceedings of the 40th International\\nACM SIGIR conference on research and development in information\\nretrieval, 2017, pp. 55–64. 29\\n[379] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, R. Morante,\\nQa4mre 2011-2013: Overview of question answering for machine read-\\ning evaluation, in: Information Access Evaluation. Multilinguality, Mul-\\ntimodality, and Visualization: 4th International Conference of the CLEF'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='timodality, and Visualization: 4th International Conference of the CLEF\\nInitiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\\nceedings 4, Springer, 2013, pp. 303–320. 29\\n[380] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\\nreading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\\n[381] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y. Feng, X. Han,\\nZ. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Z. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\\nment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\\n[382] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\\ncompetence with apps, arXiv preprint arXiv:2105.09938 (2021). 29, 31\\n[383] Y. Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\\nin: Proceedings of the 2017 conference on empirical methods in natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='in: Proceedings of the 2017 conference on empirical methods in natural\\nlanguage processing, 2017, pp. 845–854. 29, 31\\n[384] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\\nto solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\\n29, 31\\n[385] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='E. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with\\nlarge language models, CoRR abs/2108.07732 (2021). 29\\n[386] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W.\\nChung, Y. Tay, S. Ruder, D. Zhou, et al., Language models are mul-\\ntilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\\n(2022). 29\\n[387] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\\npreprint arXiv:1608.01413 (2016). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='preprint arXiv:1608.01413 (2016). 29\\n[388] S.-Y. Miao, C.-C. Liang, K.-Y. Su, A diverse corpus for evaluating\\nand developing english math word problem solvers, arXiv preprint\\narXiv:2106.15772 (2021). 29\\n[389] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\\nMawps: A math word problem repository, in: Proceedings of the 2016\\nconference of the north american chapter of the association for computa-\\ntional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='tional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29\\n[390] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\\nsimple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\\n29\\n[391] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\\nD. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\\ndata science code generation, in: International Conference on Machine\\nLearning, PMLR, 2023, pp. 18319–18345. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Learning, PMLR, 2023, pp. 18319–18345. 29\\n[392] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\\nlanguage models, arXiv preprint arXiv:2108.07732 (2021). 29\\n[393] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\\nsarial nli: A new benchmark for natural language understanding, arXiv\\npreprint arXiv:1910.14599 (2019). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='preprint arXiv:1910.14599 (2019). 29, 31\\n[394] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\\ncorpus for sentence understanding through inference, arXiv preprint\\narXiv:1704.05426 (2017). 29\\n[395] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='nosing syntactic heuristics in natural language inference, arXiv preprint\\narXiv:1902.01007 (2019). 29\\n[396] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, Y. Zhang, Logiqa: A chal-\\nlenge dataset for machine reading comprehension with logical reason-\\ning, arXiv preprint arXiv:2007.08124 (2020). 29\\n[397] P. Lewis, B. O˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\\nuating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='uating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29\\n[398] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, V. Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\\nresentations, arXiv preprint arXiv:1809.05053 (2018). 29, 31\\n[399] Y. Yang,\\nY. Zhang,\\nC. Tar,\\nJ. Baldridge,\\nPaws-x:\\nA cross-\\nlingual adversarial dataset for paraphrase identification, arXiv preprint\\narXiv:1908.11828 (2019). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='arXiv:1908.11828 (2019). 29, 31\\n[400] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\\nsummary!, Topic-Aware Convolutional Neural Networks for Extreme\\nSummarization. ArXiv, abs (1808). 29\\n[401] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli´c, A. Korhonen,\\nXcopa: A multilingual dataset for causal commonsense reasoning, arXiv\\npreprint arXiv:2005.00333 (2020). 29\\n[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\\nas a baseline for cross-lingual transfer in commonsense reasoning, arXiv\\npreprint arXiv:2106.12066 (2021). 29\\n[403] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Niko-\\nlaev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\\ntion answering in typologically diverse languages, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 454–470. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='Association for Computational Linguistics 8 (2020) 454–470. 29\\n[404] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\\nMlsum:\\nThe multilingual summarization corpus,\\narXiv preprint\\narXiv:2004.14900 (2020). 29\\n[405] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\\nhuman falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29, 32\\n[406] I. Augenstein,\\nC. Lioma,\\nD. Wang,\\nL. C. Lima,\\nC. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc:\\nA real-world multi-domain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='D. Wang,\\nL. C. Lima,\\nC. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc:\\nA real-world multi-domain\\ndataset for evidence-based fact checking of claims, arXiv preprint\\narXiv:1909.03242 (2019). 29\\n[407] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\\nlarge-scale dataset for fact extraction and verification, arXiv preprint\\narXiv:1803.05355 (2018). 29\\n[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\\nhate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\\n29, 32\\n[409] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\\nbias in pretrained language models, arXiv preprint arXiv:2004.09456\\n(2020). 29, 32\\n[410] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='son, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\\nquestion answering, arXiv preprint arXiv:2110.08193 (2021). 29\\n[411] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, K.-W. Chang, Gender bias\\nin coreference resolution: Evaluation and debiasing methods, arXiv\\npreprint arXiv:1804.06876 (2018). 29\\n[412] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\\nlenge dataset for measuring social biases in masked language models,\\narXiv preprint arXiv:2010.00133 (2020). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='arXiv preprint arXiv:2010.00133 (2020). 29\\n[413] S. Gehman, S. Gururangan, M. Sap, Y. Choi, N. A. Smith, Realtoxic-\\nityprompts: Evaluating neural toxic degeneration in language models,\\narXiv preprint arXiv:2009.11462 (2020). 29\\n[414] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\\nmetrics for measuring unintended bias with real data for text classifica-\\ntion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='tion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29\\n[415] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V. Logacheva, C. Monz, et al., Find-\\nings of the 2016 conference on machine translation, in: Proceedings of\\nthe First Conference on Machine Translation: Volume 2, Shared Task\\nPapers, 2016, pp. 131–198. 29\\n[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-\\nman, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\\n2020 conference on machine translation (wmt20), in: Proceedings of\\nthe Fifth Conference on Machine Translation, Association for Compu-\\ntational Linguistics„ 2020, pp. 1–55. 29\\n[417] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\\nmatching dataset, arXiv preprint arXiv:2106.01979 (2021). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='matching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\\n[418] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\\nwikipedia: Knowledge-powered conversational agents, arXiv preprint\\narXiv:1811.01241 (2018). 29\\n[419] H. Rashkin, E. M. Smith, M. Li, Y.-L. Boureau, Towards empathetic\\nopen-domain conversation models: A new benchmark and dataset, arXiv\\npreprint arXiv:1811.00207 (2018). 29\\n[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,\\nD. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\\ntional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\\ntion: From Machine Learning to Intelligent Conversations, Springer,\\n2020, pp. 187–208. 29\\n[421] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\\nmulti-domain dialogue dataset towards multi-turn knowledge-driven\\nconversation, arXiv preprint arXiv:2004.04100 (2020). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='conversation, arXiv preprint arXiv:2004.04100 (2020). 29\\n[422] L. CO, Iflytek: a multiple categories chinese text classifier. competition\\nofficial website (2019). 29\\n[423] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\\npushshift reddit dataset, in: Proceedings of the international AAAI con-\\nference on web and social media, Vol. 14, 2020, pp. 830–839. 30\\n[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\\nform question answering, arXiv preprint arXiv:1907.09190 (2019). 31\\n[425] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\\nBenchmarking generalization via in-context instructions on 1,600+ lan-\\nguage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\\n[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\\nM. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\\ntasking structured knowledge grounding with text-to-text language mod-\\nels, arXiv preprint arXiv:2201.05966 (2022). 31\\n[427] Q. Ye, B. Y. Lin, X. Ren, Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\\n(2021). 31\\n[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='(2021). 31\\n[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,\\nH. Zhuang, V. Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\\nmulti-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\\n(2021). 31\\n[429] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\\npus for sentence understanding through inference, in: Proceedings of\\nthe 2018 Conference of the North American Chapter of the Associ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='the 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), Association for Computational Linguistics,\\nNew Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\\nN18-1101.\\nURL https://aclanthology.org/N18-1101 31\\n[430] Y. Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\\nscrambling, in: Proceedings of the 2019 Conference of the North Amer-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='scrambling, in: Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), Associa-\\ntion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\\n1298–1308. doi:10.18653/v1/N19-1131.\\nURL https://aclanthology.org/N19-1131 32\\n[431] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\\nGPT a general-purpose natural language processing task solver?, in: The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='GPT a general-purpose natural language processing task solver?, in: The\\n2023 Conference on Empirical Methods in Natural Language Process-\\ning, 2023.\\nURL https://openreview.net/forum?id=u03xn1COsO 32\\n[432] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\\nprehensive survey of its applications, challenges, limitations, and future\\nprospects, TechRxiv (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='prospects, TechRxiv (2023). 32\\n[433] X. L. Dong, S. Moon, Y. E. Xu, K. Malik, Z. Yu, Towards next-\\ngeneration intelligent assistants leveraging llm techniques, in: Proceed-\\nings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, 2023, pp. 5792–5793. 32\\n[434] K. Pandya, M. Holia, Automating customer service using langchain:\\nBuilding custom open-source gpt chatbot for organizations, arXiv\\npreprint arXiv:2310.05421 (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='preprint arXiv:2310.05421 (2023). 32\\n[435] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\\nR. Geng, et al., Can llm already serve as a database interface?\\na\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='big bench for large-scale database grounded text-to-sqls, arXiv preprint\\narXiv:2305.03111 (2023). 32\\n[436] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\\nchatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\\n2023–02. 32\\n[437] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\\nsir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\\nlanguage models for decision support in personalized oncology, JAMA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='language models for decision support in personalized oncology, JAMA\\nNetwork Open 6 (11) (2023) e2343689–e2343689. 32\\n[438] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\\nmaroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\\nploring the potential of chat-gpt as a supportive tool for sialendoscopy\\nclinical decision making and patient information support, European\\nArchives of Oto-Rhino-Laryngology (2023) 1–6. 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Archives of Oto-Rhino-Laryngology (2023) 1–6. 32\\n[439] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\\nData decentralisation of llm-based chatbot systems in chronic disease\\nself-management, in: Proceedings of the 2023 ACM Conference on In-\\nformation Technology for Social Good, 2023, pp. 205–212. 32\\n[440] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\\nhuman feedback for a therapy chatbot application (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='human feedback for a therapy chatbot application (2023). 32\\n[441] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\\nagents: A personalized llm-powered agent framework, arXiv preprint\\narXiv:2310.02374 (2023). 32\\n[442] K. V. Lemley, Does chatgpt help us understand the medical literature?,\\nJournal of the American Society of Nephrology (2023) 10–1681. 32\\n[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\\nnext-generation large language model (llm) or chatgpt is required for\\nbiomedical engineering and research, Annals of Biomedical Engineering\\n(2023) 1–4. 32\\n[444] Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\\ncalla dataset: Probing llms’ interactive knowledge acquisition from chi-\\nnese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='nese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\\n[445] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\\nS. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\\nlanguage models in medical education: Opportunities, challenges, and\\nfuture directions, JMIR Medical Education 9 (1) (2023) e48291. 32\\n[446] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\\nChatgpt passing usmle shines a spotlight on the flaws of medical educa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Chatgpt passing usmle shines a spotlight on the flaws of medical educa-\\ntion (2023). 32\\n[447] S. Ahn, The impending impacts of large language models on medical\\neducation, Korean Journal of Medical Education 35 (1) (2023) 103. 32\\n[448] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\\n(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\\n(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\\nArtificial intelligence and public health: Evaluating chatgpt responses to\\nvaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 32\\n[450] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\\nTozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\\nai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='ai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32\\n[451] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\\nformance of chatgpt and other large language models (llm) for scientific\\nand research advancements: a double-edged sword, International Re-\\nsearch Journal of Modernization in Engineering Technology and Science\\n5 (10) (2023) 875–899. 32\\n[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large\\nlanguage models provide feedback to students? a case study on chatgpt,\\nin: 2023 IEEE International Conference on Advanced Learning Tech-\\nnologies (ICALT), IEEE, 2023, pp. 323–325. 32\\n[453] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\\nChatgpt for good? on opportunities and challenges of large language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Chatgpt for good? on opportunities and challenges of large language\\nmodels for education, Learning and individual differences 103 (2023)\\n102274. 32\\n[454] N. Rane, Enhancing the quality of teaching and learning through chat-\\ngpt and similar large language models: Challenges, future prospects,\\nand ethical considerations in education, Future Prospects, and Ethical\\nConsiderations in Education (September 15, 2023) (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Considerations in Education (September 15, 2023) (2023). 32\\n[455] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\\ngenerating chatbot’s dialogue for english as a foreign language learning,\\nInternational Journal of Advanced Computer Science and Applications\\n14 (6) (2023). 32\\n[456] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\\nthe impacts of chatgpt on future scientific work, SocArXiv (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='the impacts of chatgpt on future scientific work, SocArXiv (2023). 32\\n[457] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\\nscholarly writing: Is the integrity of the scientific discourse in jeopardy?,\\narXiv preprint arXiv:2311.06981 (2023). 32\\n[458] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\\nLarge language models for scientific synthesis, inference and explana-\\ntion, arXiv preprint arXiv:2310.07984 (2023). 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='tion, arXiv preprint arXiv:2310.07984 (2023). 33\\n[459] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\\nin scientific writing, PsyArXiv (2023). 33\\n[460] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\\nentific writing: a friend or a foe?, Reproductive BioMedicine Online\\n(2023). 33\\n[461] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\\nusing large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='using large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33\\n[462] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\\non learning mathematical reasoning with large language models, arXiv\\npreprint arXiv:2308.01825 (2023). 33\\n[463] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\\nR. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\\naugmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='augmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33\\n[464] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\\nT. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\\nlanguage models for mathematics through interactions, arXiv preprint\\narXiv:2306.01694 (2023). 33\\n[465] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He,\\nZ. Liu, et al., Summary of chatgpt-related research and perspective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Z. Liu, et al., Summary of chatgpt-related research and perspective\\ntowards the future of large language models, Meta-Radiology (2023)\\n100017. 33\\n[466] J. Drápal, H. Westermann, J. Savelka, Using large language models\\nto support thematic analysis in empirical legal studies, arXiv preprint\\narXiv:2310.18729 (2023). 33\\n[467] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\\ning legal concepts with augmented large language models (gpt-4), arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='ing legal concepts with augmented large language models (gpt-4), arXiv\\npreprint arXiv:2306.09525 (2023). 33\\n[468] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\\nA. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\\nbench: A collaboratively built benchmark for measuring legal reasoning\\nin large language models, arXiv preprint arXiv:2308.11462 (2023). 33\\n[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases, arXiv\\npreprint arXiv:2306.16092 (2023). 33\\n[470] H. Yang, X.-Y. Liu, C. D. Wang, Fingpt: Open-source financial large\\nlanguage models, arXiv preprint arXiv:2306.06031 (2023). 33\\n[471] Y. Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\\nsurvey, in: Proceedings of the Fourth ACM International Conference on\\nAI in Finance, 2023, pp. 374–382. 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='AI in Finance, 2023, pp. 374–382. 33\\n[472] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model, arXiv preprint\\narXiv:2305.19352 (2023). 33\\n[473] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\\naction, in: ACM/IEEE International Conference on Human-Robot Inter-\\naction, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='action, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33\\n[474] Y. Ye, H. You, J. Du, Improved trust in human-robot collaboration with\\nchatgpt, IEEE Access (2023). 33\\n[475] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\\nknowledge from large language models for task and motion planning,\\nin: RSS 2023 Workshop on Learning for Task and Motion Planning,\\n2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\\nwith large language models, arXiv preprint arXiv:2305.05658 (2023).\\n33\\n[477] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\\nfor deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 34\\n[478] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='gers of stochastic parrots: Can language models be too big?, in: Pro-\\nceedings of the 2021 ACM conference on fairness, accountability, and\\ntransparency, 2021, pp. 610–623. 34\\n[479] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\\ndeep learning (still) requires rethinking generalization, Communications\\nof the ACM 64 (3) (2021) 107–115. 34\\n[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\\ntrained language models, arXiv preprint arXiv:2105.00828 (2021). 34\\n[481] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\\nNow (2019) 1–33. 34\\n[482] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\\nguage models still can’t plan (a benchmark for llms on planning and\\nreasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='reasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\\n[483] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen, et al., Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models, arXiv preprint arXiv:2309.01219\\n(2023). 34\\n[484] A. Webson, E. Pavlick, Do prompt-based models really understand the\\nmeaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='meaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\\n[485] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\\nthought, let’s not think step by step! bias and toxicity in zero-shot rea-\\nsoning, arXiv preprint arXiv:2212.08061 (2022). 34\\n[486] B. C. Das, M. H. Amini, Y. Wu, Security and privacy challenges of large\\nlanguage models: A survey, arXiv preprint arXiv:2402.00888 (2024). 34\\n[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-\\nial training for large neural language models, ArXiv (April 2020).\\nURL\\nhttps://www.microsoft.com/en-us/research/\\npublication/adversarial-training-for-large-neural-language-models/\\n34\\n[488] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu-\\nGhazaleh, Survey of vulnerabilities in large language models revealed\\nby adversarial attacks (2023). arXiv:2310.10844. 34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='by adversarial attacks (2023). arXiv:2310.10844. 34\\n[489] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\\nllm can fool itself: A prompt-based adversarial attack (2023). arXiv:\\n2310.13345. 34\\n[490] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nM. Du, Explainability for large language models: A survey (2023).\\narXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='arXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large\\nlanguage models explain themselves? a study of llm-generated self-\\nexplanations (2023). arXiv:2310.11207. 35\\n[492] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\\nmean for a language model to preserve privacy?, in: Proceedings of the\\n2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35\\n[493] R. Plant, V. Giuffrida, D. Gkatzia, You are what you write:\\nPre-\\nserving privacy in the era of large language models, arXiv preprint\\narXiv:2204.09391 (2022). 35\\n[494] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\\nB. Ren, Y. Wang, Real-time execution of large-scale language models\\non mobile (2020). arXiv:2009.06823. 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='on mobile (2020). arXiv:2009.06823. 35\\n[495] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo,\\nY. Zhu, Olive:\\nAccelerating large language models via hardware-\\nfriendly outlier-victim pair quantization, in: Proceedings of the 50th\\nAnnual International Symposium on Computer Architecture, 2023, pp.\\n1–15. 35\\n[496] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\\nlanguage models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='language models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35\\n[497] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\\nand policy implications for large language models: Guiding responsible\\ndevelopment and deployment, arXiv preprint arXiv:2308.02678 (2023).\\n35\\n[498] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\\nmodels: a three-layered approach, AI and Ethics (2023) 1–31. 35\\n47')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Chunking function\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "def chunk_documents(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 500,\n",
    "    chunk_overlap: int = 100\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller overlapping chunks for embedding.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "    chunked_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Chunked {len(documents)} documents into {len(chunked_docs)} chunks.\")\n",
    "    return chunked_docs\n",
    "\n",
    "# 3. Apply chunking to the loaded PDFs\n",
    "chunked_documents = chunk_documents(pdf_documents)\n",
    "chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "002515f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2226afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using sentenceTransformer\"\"\"\n",
    "    #It helps convert text into numerical vectors using a pre-trained model called all-MiniLM-L6-v2 from Hugging Face’s sentence-transformers library.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):  #This model will convert text into vector\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "\n",
    "        Args:\n",
    "        model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model() ## It is going to load all-MiniLM-L6-v2\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\" Load the sentence transformer model \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "    \n",
    "            ## Returns the **size of the vector - get_sentence_embedding_dimension. (e.g., 384 for MiniLM)\n",
    "    \n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error Loading model {self.model_name}: {e}\")\n",
    "            raise #Rethrows the error\n",
    "\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "           raise ValueError(\"Model not loaded\")\n",
    "    \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager=EmbeddingManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3c73c40",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "### Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f892a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store initialized. Collection: pdf_documents\n",
      "Existing doc in collection: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x24ecea9f230>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "   def __init__(self, collection_name:str = \"pdf_documents\", persist_directory: str = \"../data/vectore_store\"):\n",
    "      self.collection_name = collection_name\n",
    "      self.persist_directory = persist_directory\n",
    "      self.client = None\n",
    "      self.collection = None\n",
    "      self._initialize_store()\n",
    "\n",
    "   def _initialize_store(self):\n",
    "      # initialize chromadb client and collection\n",
    "      try:\n",
    "         os.makedirs(self.persist_directory, exist_ok=True)\n",
    "         self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "   \n",
    "         #Get or create collection\n",
    "         self.collection = self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            metadata={\"description\": \"Pdf foc embedding for RAG\"}\n",
    "         )   \n",
    "         print(f\"Vector Store initialized. Collection: {self.collection_name}\")\n",
    "         print(f\"Existing doc in collection: {self.collection_count()}\")\n",
    "      except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "   def collection_count(self) -> int:\n",
    "      try:\n",
    "            return self.collection.count()\n",
    "      except Exception as e:\n",
    "            print(f\"Error getting collection count: {e}\")\n",
    "            return 0\n",
    "\n",
    "   def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "    #Add docs and their embedding to the vector stores\n",
    "\n",
    "    if len(documents) != len(embeddings):\n",
    "        raise ValueError(\"No of docs must match number of embeddings\")\n",
    "    \n",
    "    print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "    #Prepare data for chromadb\n",
    "\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    documents_text = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i, (doc, embedding) in enumerate(zip(document,embeddings)):\n",
    "        # generate unique ID\n",
    "        doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "        ids.append(doc_id)\n",
    "\n",
    "        #prepare metadata\n",
    "        metadata = dict(doc.metadata)\n",
    "        metadata['doc_index'] = i\n",
    "        metadata['content_length'] = len(doc.page_content)\n",
    "        metadatas.append(metadata)\n",
    "\n",
    "        #Document content\n",
    "        documents_text.append(doc.page_content)\n",
    "\n",
    "        #Embedding\n",
    "        embeddings_list.append(embedding.tolist())\n",
    "\n",
    "    #Add to collection\n",
    "    try:\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings_list,\n",
    "            metadatas=metadata,\n",
    "            documents=documents_text\n",
    "        )\n",
    "\n",
    "        print(f\"Sucessfully added {len(documents)}documents to vectore store\")\n",
    "        print(f\"Total doc in collection: {self.collection_count()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error adding docs to vector store: {e}\")\n",
    "         raise\n",
    "    \n",
    "vectorestore=VectorStore()\n",
    "vectorestore     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "65bf781d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='A Comprehensive Overview of Large Language Models\\nHumza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\\nNick Barnesi, Ajmal Mianj\\naThe University of Sydney, Sydney, Australia\\nbUniversity of Engineering and Technology (UET), Lahore, Pakistan\\ncThe Chinese University of Hong Kong (CUHK), HKSAR, China\\ndUniversity of Technology Sydney (UTS), Sydney, Australia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='dUniversity of Technology Sydney (UTS), Sydney, Australia\\neCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\\nfKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\\ngSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\\nhThe University of Melbourne (UoM), Melbourne, Australia\\niAustralian National University (ANU), Canberra, Australia\\njThe University of Western Australia (UWA), Perth, Australia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='jThe University of Western Australia (UWA), Perth, Australia\\nAbstract\\nLarge Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\\nbeyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\\ntopics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\\nLLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\\nthe rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\\nrange of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\\nalong with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\\ninformative summaries of the existing works to advance the LLM research.\\nKeywords:\\nLarge Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\\n1. Introduction\\nLanguage plays a fundamental role in facilitating commu-\\nnication and self-expression for humans and their interaction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='nication and self-expression for humans and their interaction\\nwith machines. The need for generalized models stems from\\nthe growing demand for machines to handle complex language\\ntasks, including translation, summarization, information re-\\ntrieval, conversational interactions, etc. Recently, significant\\nbreakthroughs have been witnessed in language models, pri-\\nmarily attributed to transformers [1], increased computational\\ncapabilities, and the availability of large-scale training data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='capabilities, and the availability of large-scale training data.\\nThese developments have brought about a revolutionary trans-\\nformation by enabling the creation of LLMs that can approxi-\\nmate human-level performance on various tasks [2, 3]. Large\\n∗Equal contribution\\nEmail addresses: humza_naveed@yahoo.com (Humza Naveed),\\naukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\\nQiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\\nsaeed.anwar@kfupm.edu.sa (Saeed Anwar),\\nmuhammad.usman@kfupm.edu.sa (Muhammad Usman),\\nnaveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\\nnick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\\n(Ajmal Mian)\\nFigure 1: The trend of papers released over the years containing keywords\\n\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\\nLanguage Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 0}, page_content='Language Model + Alignment\".\\nPreprint submitted to Elsevier\\nOctober 18, 2024\\narXiv:2307.06435v10  [cs.CL]  17 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='2019\\nT5 (Oct)\\nGPT-3 (May)\\nWebGPT (Dec)\\nOPT-IML\\nTK-Instruct (May)\\nmT0 (Dec)\\nWizard-LM\\nVicuna\\nAlpaca (Mar)\\nHuaTuo (Apr)\\nKoala (May)\\nWizard-Coder (Jun)\\nGoat\\nPanGu-α (Apr)\\nCPM-2 (Jun)\\nGPT-NeoX-20B (Apr)\\nCodeGen (Mar) \\nGalactica (Nov)\\nGLM (Oct)\\nOPT\\nUL2 (May)\\nLLaMA (Feb)\\nLLaMA 2 (Jul)\\nMPT (Jun)\\nCodeT5+\\nCode Llama (Aug)\\nStarCoder\\nXuan Yuan 2.0 (May)\\n2020\\n2021\\n2022\\n2023\\n2024\\nmT5 (Oct)\\nHyperCLOVA (Sep)\\nERNIE 3.0\\nCodex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='Codex (Jul)\\nJurassic-1 (Aug)\\nYuan 1.0 (Oct)\\nGopher (Dec)\\nERNIE 3.0 Titan\\nGLaM\\nLaMDA\\nT0 (Oct)\\nChatGPT (Nov)\\nSparrow (Sep)\\nFLAN-U-PaLM (Oct)\\nBard (Oct)\\nMT-NLG (Jan)\\nAlphaCode (Feb)\\nChinchilla (Mar)\\nPaLM (Apr)\\nU-PALM (Oct)\\nBLOOM (Nov)\\nAlexaTM (Aug)\\nPaLM2 (May)\\nGPT-4\\nPanGu-Σ (Mar)\\nBloombergGPT\\nClaude\\nGemini (Dec)\\nDeepSeek (Jan)\\nLLaMA 3 \\nGrok-1 (Mar)\\nSnowflake Arctic (Apr)\\nDeepSeek-V2 (May)\\nMixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='Mixtral 8x22B\\nNemotron (Feb)\\nGPT-4o (May)\\nOpenAI o1 (Sep)\\nGemini-1.5 (Feb)\\nGrok-1.5 (Apr)\\nFigure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\\non the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='and open-source models, highlighting the evolving landscape and trends in natural language processing research.\\nLanguage Models (LLMs) have emerged as cutting-edge arti-\\nficial intelligence systems that can process and generate text\\nwith coherent communication [4] and generalize to multiple\\ntasks [5, 6].\\nThe historical progress in natural language processing (NLP)\\nevolved from statistical to neural language modeling and then\\nfrom pre-trained language models (PLMs) to LLMs.\\nWhile'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='from pre-trained language models (PLMs) to LLMs.\\nWhile\\nconventional language modeling (LM) trains task-specific mod-\\nels in supervised settings, PLMs are trained in a self-supervised\\nsetting on a large corpus of text [7, 8, 9] with the aim of learning\\na generic representation that is shareable among various NLP\\ntasks. After fine-tuning for downstream tasks, PLMs surpass\\nthe performance gains of traditional language modeling (LM).\\nThe larger PLMs bring more performance gains, which has led'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='The larger PLMs bring more performance gains, which has led\\nto the transitioning of PLMs to LLMs by significantly increas-\\ning model parameters (tens to hundreds of billions) [10] and\\ntraining dataset (many GBs and TBs) [10, 11]. Following this\\ndevelopment, numerous LLMs have been proposed in the lit-\\nerature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\\nnumber of released LLMs and names of a few significant LLMs\\nproposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='proposed over the years are shown in Fig 1 and Fig 2, respec-\\ntively.\\nThe early work on LLMs, such as T5 [10] and mT5 [11] em-\\nployed transfer learning until GPT-3 [6] showed LLMs are\\nzero-shot transferable to downstream tasks without fine-tuning.\\nLLMs accurately respond to task queries when prompted with\\ntask descriptions and examples. However, pre-trained LLMs\\nfail to follow user intent and perform worse in zero-shot set-\\ntings than in few-shot.\\nFine-tuning them with task instruc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='tings than in few-shot.\\nFine-tuning them with task instruc-\\ntions data [16, 17, 18, 19] and aligning with human prefer-\\nences [20, 21] enhances generalization to unseen tasks, im-\\nproving zero-shot performance significantly and reducing mis-\\naligned behavior.\\nIn addition to better generalization and domain adaptation,\\nLLMs appear to have emergent abilities, such as reasoning,\\nplanning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='planning, decision-making, in-context learning, answering in\\nzero-shot settings, etc.\\nThese abilities are known to be ac-\\nquired by them due to their gigantic scale even when the pre-\\ntrained LLMs are not trained specifically to possess these at-\\ntributes [22, 23, 24]. Such abilities have led LLMs to be widely\\nadopted in diverse settings, including multi-modal, robotics,\\ntool manipulation, question answering, autonomous agents, etc.\\nVarious improvements have also been suggested in these areas'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='Various improvements have also been suggested in these areas\\neither by task-specific training [25, 26, 27, 28, 29, 30, 31] or\\nbetter prompting [32].\\nThe LLMs abilities to solve diverse tasks with human-level\\nperformance come at the cost of slow training and inference,\\nextensive hardware requirements, and higher running costs.\\nSuch requirements have limited their adoption and opened up\\nopportunities to devise better architectures [15, 33, 34, 35]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='opportunities to devise better architectures [15, 33, 34, 35]\\nand training strategies [36, 37, 21, 38, 39, 40, 41].\\nParam-\\neter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\\ntion [44, 45], knowledge distillation, and context length inter-\\npolation [46, 47, 48, 49] among others are some of the methods\\nwidely studied for efficient LLM utilization.\\nDue to the success of LLMs on a wide variety of tasks, the\\nresearch literature has recently experienced a large influx of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='research literature has recently experienced a large influx of\\nLLM-related contributions.\\nResearchers have organized the\\nLLMs literature in surveys [50, 51, 52, 53], and topic-specific\\nsurveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\\ncontribution focuses on providing a comprehensive yet concise\\noverview of the general direction of LLM research. This arti-\\ncle summarizes architectural and training details of pre-trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='cle summarizes architectural and training details of pre-trained\\nLLMs and delves deeper into the details of concepts like fine-\\ntuning, multi-modal LLMs, augmented LLMs, datasets, eval-\\nuation, applications, challenges, and others to provide a self-\\ncontained comprehensive overview. Our key contributions are\\nsummarized as follows.\\n• We present a survey on the developments in LLM research,\\nproviding a concise, comprehensive overview of the direc-\\ntion.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='providing a concise, comprehensive overview of the direc-\\ntion.\\n• We present extensive summaries of pre-trained models that\\ninclude fine-grained details of architecture and training de-\\ntails.\\n• We summarize major findings of the popular contributions\\nand provide a detailed discussion on the key design and\\ndevelopment aspects of LLMs to help practitioners effec-\\ntively leverage this technology.\\n• In this self-contained article, we cover a range of con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 1}, page_content='tively leverage this technology.\\n• In this self-contained article, we cover a range of con-\\ncepts to present the general direction of LLMs compre-\\nhensively, including background, pre-training, fine-tuning,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\\n7. Challenges\\nmulti-modal LLMs, augmented LLMs, LLMs-powered\\nagents, datasets, evaluation, etc.\\nWe loosely follow the existing terminology to ensure a stan-\\ndardized outlook of this research direction. For instance, fol-\\nlowing [50], our survey discusses pre-trained LLMs with 10B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='lowing [50], our survey discusses pre-trained LLMs with 10B\\nparameters or more. We refer the readers interested in smaller\\npre-trained models to [51, 52, 53].\\nThe organization of this paper is as follows. Section 2 discusses\\nthe background of LLMs. Section 3 focuses on LLMs overview,\\narchitectures, training pipelines and strategies, fine-tuning, and\\nutilization in different domains. Section 4 highlights the config-\\nuration and parameters that play a crucial role in the function-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 2}, page_content='uration and parameters that play a crucial role in the function-\\ning of these models. Summary and discussions are presented\\nin section 3.8. The LLM training and evaluation, datasets, and\\nbenchmarks are discussed in section 5, followed by challenges\\nand future directions, and conclusion in sections 7 and 8, re-\\nspectively.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='2. Background\\nWe provide the relevant background to understand the fun-\\ndamentals related to LLMs in this section. We briefly discuss\\nnecessary components in LLMs and refer the readers interested\\nin details to the original works.\\n2.1. Tokenization\\nTokenization [59] is an essential pre-processing step in\\nLLM training that parses the text into non-decomposing units\\ncalled tokens. Tokens can be characters, subwords [60], sym-\\nbols [61], or words, depending on the tokenization process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='bols [61], or words, depending on the tokenization process.\\nSome of the commonly used tokenization schemes in LLMs\\ninclude wordpiece [62], byte pair encoding (BPE) [61], and un-\\nigramLM [60]. Readers are encouraged to refer to [63] for a\\ndetailed survey.\\n2.2. Encoding Positions\\nThe transformer processes input sequences in parallel and\\nindependently of each other.\\nMoreover, the attention mod-\\nule in the transformer does not capture positional information.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='Moreover, the attention mod-\\nule in the transformer does not capture positional information.\\nAs a result, positional encodings were introduced in trans-\\nformer [64], where a positional embedding vector is added to\\nthe token embedding. Variants of positional embedding include\\nabsolute, relative, or learned positional encodings. Within rel-\\native encoding, Alibi and RoPE are two widely used positional\\nembeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='embeddings in LLMs.\\nAlibi [65]: It subtracts a scalar bias from the attention score\\nthat increases with the distance between token positions. This\\nfavors using recent tokens for attention.\\nRoPE [66]: It rotates query and key representations at an an-\\ngle proportional to the token absolute position in the input\\nsequence, resulting in a relative positional encoding scheme\\nwhich decays with the distance between the tokens.\\n2.3. Attention in LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='which decays with the distance between the tokens.\\n2.3. Attention in LLMs\\nAttention assigns weights to input tokens based on impor-\\ntance so that the model gives more emphasis to relevant tokens.\\nAttention in transformers [64] calculates query, key, and value\\nmappings for input sequences, where the attention score is\\nobtained by multiplying the query and key, and later used to\\nweight values. We discuss different attention strategies used in\\nLLMs below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='weight values. We discuss different attention strategies used in\\nLLMs below.\\nSelf-Attention [64]: Calculates attention using queries, keys,\\nand values from the same block (encoder or decoder).\\nCross Attention: It is used in encoder-decoder architectures,\\nwhere encoder outputs are the queries, and key-value pairs\\ncome from the decoder.\\nSparse Attention [67]: Self-attention has O(n2) time complex-\\nity which becomes infeasible for large sequences. To speed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='ity which becomes infeasible for large sequences. To speed\\nup the computation, sparse attention [67] iteratively calculates\\nattention in sliding windows for speed gains.\\nFlash Attention [68]: Memory access is the major bottleneck\\nin calculating attention using GPUs.\\nTo speed up, flash\\nattention employs input tiling to minimize the memory reads\\nand writes between the GPU high bandwidth memory (HBM)\\nand the on-chip SRAM.\\n2.4. Activation Functions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='and the on-chip SRAM.\\n2.4. Activation Functions\\nThe activation functions serve a crucial role in the curve-\\nfitting abilities of neural networks [69]. We discuss activation\\nfunctions used in LLMs in this section.\\nReLU [70]: The Rectified linear unit (ReLU) is defined as:\\nReLU(x) = max(0, x)\\n(1)\\nGeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\\ncombination of ReLU, dropout [72] and zoneout [73].\\nGLU variants [74]: The Gated Linear Unit [75] is a neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='GLU variants [74]: The Gated Linear Unit [75] is a neural\\nnetwork layer that is an element-wise product (⊗) of a linear\\ntransformation and a sigmoid transformed (σ) linear projection\\nof the input given as:\\nGLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\\n(2)\\nwhere X is the input of layer and l, W, b, V and c are learned\\nparameters. Other GLU variants [74] used in LLMs are:\\nReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\\nGEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\\nS wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\\n2.5. Layer Normalization\\nLayer normalization leads to faster convergence and is an in-\\ntegrated component of transformers [64]. In addition to Layer-\\nNorm [76] and RMSNorm [77], LLMs use pre-layer normal-\\nization [78], applying it before multi-head attention (MHA).\\nPre-norm is shown to provide training stability in LLMs. An-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='Pre-norm is shown to provide training stability in LLMs. An-\\nother normalization variant, DeepNorm [79] fixes the issue with\\nlarger gradients in pre-norm.\\n2.6. Distributed LLM Training\\nThis section describes distributed LLM training approaches\\nbriefly. More details are available in [13, 37, 80, 81].\\nData Parallelism: Data parallelism replicates the model on\\nmultiple devices where data in a batch gets divided across de-\\nvices. At the end of each training iteration weights are synchro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='vices. At the end of each training iteration weights are synchro-\\nnized across all devices.\\nTensor Parallelism: Tensor parallelism shards a tensor compu-\\ntation across devices. It is also known as horizontal parallelism\\nor intra-layer model parallelism.\\nPipeline Parallelism: Pipeline parallelism shards model layers\\nacross different devices. This is also known as vertical paral-\\nlelism.\\nModel Parallelism: A combination of tensor and pipeline par-\\nallelism is known as model parallelism.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 3}, page_content='allelism is known as model parallelism.\\n3D Parallelism: A combination of data, tensor, and model par-\\nallelism is known as 3D parallelism.\\nOptimizer Parallelism: Optimizer parallelism also known as\\nzero redundancy optimizer [37] implements optimizer state\\npartitioning, gradient partitioning, and parameter partitioning\\nacross devices to reduce memory consumption while keeping\\nthe communication costs as low as possible.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='2.7. Libraries\\nSome commonly used libraries for LLMs training are:\\nTransformers [82]: The library provides access to various pre-\\ntrained transformer models with APIs to train, fine-tune, infer,\\nand develop custom models.\\nDeepSpeed [36]: A library for scalable distributed training and\\ninference of deep learning models.\\nMegatron-LM [80]: It provides GPU-optimized techniques for\\nlarge-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='large-scale training of LLMs.\\nJAX [83]: A Python library for high-performance numerical\\ncomputing and scaleable machine learning. It can differenti-\\nate native Python and NumPy functions and execute them on\\nGPUs.\\nColossal-AI [84]: A collection of components to write dis-\\ntributed deep learning models.\\nBMTrain [81]: A library to write efficient stand-alone LLMs\\ntraining code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='training code.\\nFastMoE [85]:\\nProvides API to build mixture-of-experts\\n(MoE) model in PyTorch.\\nMindSpore [86]: A deep learning training and inference frame-\\nwork extendable to mobile, edge, and cloud computing.\\nPyTorch [87]: A framework developed by Facebook AI Re-\\nsearch lab (FAIR) to build deep learning models. The main\\nfeatures of PyTorch include a dynamic computation graph and\\na pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='a pythonic coding style.\\nTensorflow [88]:\\nA deep learning framework written by\\nGoogle. The key features of TensorFlow are graph-based com-\\nputation, eager execution, scalability, etc.\\nMXNet [89]: Apache MXNet is a deep learning framework\\nwith support to write programs in multiple languages, includ-\\ning, Python, C++, Scala, R, etc. It also provides support for\\ndynamic and static computation graphs.\\n2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='2.8. Data PreProcessing\\nThis section briefly summarizes data preprocessing tech-\\nniques used in LLMs training.\\nQuality Filtering: For better results, training data quality is\\nessential. Some approaches to filtering data are: 1) classifier-\\nbased and 2) heuristics-based.\\nClassifier-based approaches\\ntrain a classifier on high-quality data and predict the quality of\\ntext for filtering, whereas heuristics-based employ some rules\\nfor filtering like language, metrics, statistics, and keywords.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='for filtering like language, metrics, statistics, and keywords.\\nData Deduplication: Duplicated data can affect model per-\\nformance and increase data memorization; therefore, to train\\nLLMs, data deduplication is one of the preprocessing steps.\\nThis can be performed at multiple levels, like sentences,\\ndocuments, and datasets.\\nPrivacy Reduction: Most of the training data for LLMs is\\ncollected through web sources.\\nThis data contains private\\ninformation; therefore, many LLMs employ heuristics-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The difference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='Figure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output.\\nHere, the encoder sees the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='decoder to generate the output.\\nHere, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing\\ncross-attention.\\nCausal Decoder: A type of architecture that does not have an\\nencoder and processes and generates output using a decoder,\\nwhere the predicted token depends only on the previous time\\nsteps.\\nPrefix Decoder: It is also known as a non-causal decoder,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='steps.\\nPrefix Decoder: It is also known as a non-causal decoder,\\nwhere the attention calculation is not strictly dependent on the\\npast information and the attention is bidirectional. An example\\nof a non-causal attention mask is shown in Figure 4.\\nMixture-of-Experts: It is a variant of transformer architecture\\nwith parallel independent experts and a router to route tokens\\nto experts. These experts are feed-forward layers after the at-\\ntention block [90]. Mixture-of-Experts (MoE) is an efficient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='tention block [90]. Mixture-of-Experts (MoE) is an efficient\\nsparse architecture that offers comparable performance to dense\\nmodels and allows increasing the model size without increas-\\ning the computational cost by activating only a few experts at a\\ntime [91, 92].\\n2.10. Pre-Training Objectives\\nThis section describes LLMs pre-training objectives.\\nFor\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 4}, page_content='For\\nmore details see the paper [93].\\nFull Language Modeling: An autoregressive language model-\\ning objective where the model is asked to predict future tokens\\ngiven the previous tokens, an example is shown in Figure 5.\\nPrefix Language Modeling: A non-causal training objective,\\nwhere a prefix is chosen randomly and only remaining target\\ntokens are used to calculate the loss. An example is shown in\\nFigure 5.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\\ndifferent training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\\n“RLHF” represents reinforcement learning with human feedback.\\nMasked Language Modeling: In this training objective, tokens'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='Masked Language Modeling: In this training objective, tokens\\nor spans (a sequence of tokens) are masked randomly and the\\nmodel is asked to predict masked tokens given the past and\\nfuture context. An example is shown in Figure 5.\\nUnified Language Modeling: Unified language modeling [94]\\nis a combination of causal, non-causal, and masked language\\ntraining objectives. Here in masked language modeling, the\\nattention is not bidirectional but unidirectional, attending either'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='attention is not bidirectional but unidirectional, attending either\\nleft-to-right or right-to-left context.\\n2.11. LLMs Scaling Laws\\nScaling laws study the optimal combination of model param-\\neters, dataset size, and computational resources that predict the\\nimprovement in the model performance. It has been shown\\nthat the loss scales according to the power-law with model size,\\ndataset size, and compute resources [95]. This study suggests'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='dataset size, and compute resources [95]. This study suggests\\nlarger models are more important than big data for better perfor-\\nmance. Another variant of scaling law [96] suggests the model\\nsize and the number of training tokens should be scaled equally.\\n2.12. LLMs Adaptation Stages\\nThis section discusses the fundamentals of LLMs adaptation\\nstages, from pre-training to fine-tuning for downstream tasks\\nand utilization. An example of different training stages and in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='and utilization. An example of different training stages and in-\\nference in LLMs is shown in Figure 6. In this paper, we refer\\nto alignment-tuning as aligning with human preferences, while\\noccasionally the literature uses the term alignment for different\\npurposes.\\n2.12.1. Pre-Training\\nIn the very first stage, the model is trained in a self-\\nsupervised manner on a large corpus to predict the next to-\\nkens given the input. The design choices of LLMs vary from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 5}, page_content='kens given the input. The design choices of LLMs vary from\\nencoder-decoder to decoder-only architectures with different\\nbuilding blocks and loss functions in sections 2.5, 2.4, 2.10.\\n2.12.2. Fine-Tuning\\nThere are different styles to fine-tune an LLM. This section\\nbriefly discusses fine-tuning approaches.\\nTransfer Learning: The pre-trained LLMs perform well for\\nvarious tasks [6, 15]. However, to improve the performance for\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='a downstream task, pre-trained models are fine-tuned with the\\ntask-specific data [10, 11], known as transfer learning.\\nInstruction-tuning: To enable a model to respond to user\\nqueries effectively, the pre-trained model is fine-tuned on in-\\nstruction formatted data i.e., instruction and an input-output\\npair. Instructions generally comprise multi-task data in plain\\nnatural language, guiding the model to respond according to the\\nprompt and the input. This type of fine-tuning improves zero-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='prompt and the input. This type of fine-tuning improves zero-\\nshot generalization and downstream task performance. Details\\non formatting instruction data and its various styles are avail-\\nable in [16, 50, 97].\\nAlignment-tuning: LLMs are prone to generating false, biased,\\nand harmful text. To make them helpful, honest, and harmless,\\nmodels are aligned using human feedback. Alignment involves\\nasking LLMs to generate unexpected responses and then updat-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='asking LLMs to generate unexpected responses and then updat-\\ning their parameters to avoid such responses [20, 21, 98].\\nIt ensures LLMs operate according to human intentions and\\nvalues. A model is defined to be an “aligned” model if the\\nmodel fulfills three criteria of helpful, honest, and harmless or\\n“HHH” [99].\\nResearchers employ reinforcement learning with human feed-\\nback (RLHF) [100] for model alignment. In RLHF, a fine-tuned\\nmodel on demonstrations is further trained with reward model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='model on demonstrations is further trained with reward model-\\ning (RM) and reinforcement learning (RL), shown in Figure 6.\\nBelow we briefly discuss RM and RL pipelines in RLHF.\\nReward modeling: trains a model to rank generated responses\\naccording to human preferences using a classification objec-\\ntive. To train the classifier humans annotate LLMs generated\\nresponses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='responses based on the HHH criteria.\\nReinforcement learning: in combination with the reward model\\nis used for alignment in the next stage. The previously trained\\nreward model ranks LLM-generated responses into preferred\\nvs. non-preferred, which is used to align the model with proxi-\\nmal policy optimization (PPO). This process repeats iteratively\\nuntil convergence.\\n2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='2.12.3. Prompting/Utilization\\nPrompting is a method to query trained LLMs for generating\\nresponses, as illustrated in Figure 6. LLMs can be prompted in\\nvarious prompt setups, where they can be adapted to the instruc-\\ntions without fine-tuning and in other cases with fine-tuning on\\ndata containing different prompt styles [16, 101, 102]. A good\\nguide on prompt engineering is available at [32]. Below, we\\nwill discuss various widely used prompt setups.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='will discuss various widely used prompt setups.\\nZero-Shot Prompting: LLMs are zero-shot learners and ca-\\npable of answering queries never seen before. This style of\\nprompting requires LLMs to answer user questions without see-\\ning any examples in the prompt.\\nIn-context Learning: Also known as few-shot learning, here,\\nmultiple input-output demonstration pairs are shown to the\\nmodel to generate the desired response. This adaptation style'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='model to generate the desired response. This adaptation style\\nis also called few-shot learning. A discussion on formatting in-\\ncontext learning (ICL) templates is available in [54, 50, 18, 16].\\nReasoning in LLMs: LLMs are zero-shot reasoners and can\\nbe provoked to generate answers to logical problems, task\\nplanning, critical thinking, etc. with reasoning.\\nGenerating\\nreasons is possible only by using different prompting styles,\\nwhereas to improve LLMs further on reasoning tasks many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='whereas to improve LLMs further on reasoning tasks many\\nmethods [16, 97] train them on reasoning datasets. We discuss\\nvarious prompting techniques for reasoning below.\\nChain-of-Thought (CoT): A special case of prompting where\\ndemonstrations contain reasoning information aggregated with\\ninputs and outputs so that the model generates outcomes with\\nstep-by-step reasoning. More details on CoT prompts are avail-\\nable in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='able in [55, 103, 101].\\nSelf-Consistency:\\nImproves CoT performance by generat-\\ning multiple responses and selecting the most frequent an-\\nswer [104].\\nTree-of-Thought (ToT): Explores multiple reasoning paths\\nwith possibilities to look ahead and backtrack for problem-\\nsolving [105].\\nSingle-Turn Instructions: In this prompting setup, LLMs are\\nqueried only once with all the relevant information in the\\nprompt. LLMs generate responses by understanding the con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='prompt. LLMs generate responses by understanding the con-\\ntext either in a zero-shot or few-shot setting.\\nMulti-Turn Instructions: Solving a complex task requires mul-\\ntiple interactions with LLMs, where feedback and responses\\nfrom the other tools are given as input to the LLM for the next\\nrounds. This style of using LLMs in the loop is common in\\nautonomous agents.\\n3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='3. Large Language Models\\nThis section reviews LLMs, briefly describing their architec-\\ntures, training objectives, pipelines, datasets, and fine-tuning\\ndetails.\\n3.1. Pre-Trained LLMs\\nHere, we provide summaries of various well-known pre-\\ntrained LLMs with significant discoveries, changing the course\\nof research and development in NLP. These LLMs have consid-\\nerably improved the performance in NLU and NLG domains,\\nand are widely fine-tuned for downstream tasks. Moreover, We'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='and are widely fine-tuned for downstream tasks. Moreover, We\\nalso identify key findings and insights of pre-trained LLMs in\\nTable 1 and 2 that improve their performance.\\n3.1.1. General Purpose\\nT5 [10]: An encoder-decoder model employing a unified text-\\nto-text training for all NLP problems is shown in Figure 7. T5\\nplaces layer normalization outside the residual path in a conven-\\ntional transformer model [64]. It uses masked language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='tional transformer model [64]. It uses masked language mod-\\neling as a pre-training objective where spans (consecutive to-\\nkens) are replaced with a single mask instead of separate masks\\nfor each token. This type of masking speeds up the training as\\nit produces shorter sequences. After pre-training, the model is\\nfine-tuned using adapter layers [106] for downstream tasks.\\nGPT-3 [6]: The GPT-3 architecture is the same as the GPT-\\n2 [5] but with dense and sparse attention in transformer layers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 6}, page_content='2 [5] but with dense and sparse attention in transformer layers\\nsimilar to the Sparse Transformer [67]. It shows that large mod-\\nels can train on larger batch sizes with a lower learning rate to\\ndecide the batch size during training, GPT-3 uses the gradient\\nnoise scale as in [107]. Overall, GPT-3 increases model param-\\neters to 175B showing that the performance of large language\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='Figure 7: Unified text-to-text training example, source image from [10].\\nFigure 8: The image is the article of [108], showing an example of PanGu-α\\narchitecture.\\nmodels improves with the scale and is competitive with the fine-\\ntuned models.\\nmT5 [11]: A multilingual T5 model [10] trained on the mC4\\ndataset with 101 languages. The dataset is extracted from the\\npublic common crawl scrape. The model uses a larger vocab-\\nulary size of 250,000 to cover multiple languages. To avoid'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='ulary size of 250,000 to cover multiple languages. To avoid\\nover-fitting or under-fitting for a language, mT5 employs a data\\nsampling procedure to select samples from all languages. The\\npaper suggests using a small amount of pre-training datasets,\\nincluding all languages when fine-tuning for a task using En-\\nglish language data. This allows the model to generate correct\\nnon-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='non-English outputs.\\nPanGu-α [108]: An autoregressive model that has a query\\nlayer at the end of standard transformer layers, example shown\\nin Figure 8, to predict the next token. Its structure is similar to\\nthe transformer layer but with an additional embedding for the\\nnext position in the attention mechanism, given in Eq. 3.\\na = pnWq\\nhWk\\nhTHT\\nL\\n(3)\\nCPM-2 [12]: Cost-efficient Pre-trained language Models\\n(CPM-2) pre-trains bilingual (English and Chinese) 11B and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='(CPM-2) pre-trains bilingual (English and Chinese) 11B and\\n198B mixture-of-experts (MoE) models on the WuDaoCor-\\npus [109] dataset. The tokenization process removes “_” white\\nspace tokens in the sentencepiece tokenizer. The models are\\ntrained with knowledge inheritance, starting with only the Chi-\\nnese language in the first stage and then adding English and\\nChinese data. This trained model gets duplicated multiple times\\nto initialize the 198B MoE model. Moreover, to use the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='to initialize the 198B MoE model. Moreover, to use the model\\nfor downstream tasks, CPM-2 experimented with both com-\\nplete fine-tuning and prompt fine-tuning as in [40] where only\\nprompt-related parameters are updated by inserting prompts at\\nvarious positions, front, middle, and back. CPM-2 also pro-\\nposes the INFMOE, a memory-efficient framework with a strat-\\negy to dynamically offload parameters to the CPU for inference\\nat a 100B scale. It overlaps data movement with inference com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='at a 100B scale. It overlaps data movement with inference com-\\nputation for lower inference time.\\nERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\\ntask learning to build a modular architecture using Transformer-\\nXL [111] as the backbone. The universal representation mod-\\nule is shared by all the tasks, which serve as the basic block\\nfor task-specific representation modules, which are all trained\\njointly for natural language understanding, natural language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='jointly for natural language understanding, natural language\\ngeneration, and knowledge extraction. This LLM is primar-\\nily focused on the Chinese language. It claims to train on the\\nlargest Chinese text corpora for LLM training, and achieved\\nstate-of-the-art in 54 Chinese NLP tasks.\\nJurassic-1 [112]: A pair of auto-regressive language mod-\\nels, including a 7B-parameter J1-Large model and a 178B-\\nparameter J1-Jumbo model.\\nThe training vocabulary of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='parameter J1-Jumbo model.\\nThe training vocabulary of\\nJurassic-1 comprise word pieces, complete words, and multi-\\nword expressions without any word boundaries, where possible\\nout-of-vocabulary instances are interpreted as Unicode bytes.\\nCompared to the GPT-3 counterparts, the Jurassic-1 models\\napply a more balanced depth-to-width self-attention architec-\\nture [113] and an improved tokenizer for a faster prediction\\nbased on broader resources, achieving a comparable perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='based on broader resources, achieving a comparable perfor-\\nmance in zero-shot learning tasks and a superior performance in\\nfew-shot learning tasks given the ability to feed more examples\\nas a prompt.\\nHyperCLOVA [114]: A Korean language model with GPT-3\\narchitecture.\\nYuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\\nhigh-quality text collected from the Internet. A Massive Data\\nFiltering System (MDFS) built on Spark is developed to pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='Filtering System (MDFS) built on Spark is developed to pro-\\ncess the raw data via coarse and fine filtering techniques. To\\nspeed up the training of Yuan 1.0 to save energy expenses and\\ncarbon emissions, various factors that improve the performance\\nof distributed training are incorporated in architecture and train-\\ning: like increasing the hidden state size improves pipeline and\\ntensor parallelism performance, larger micro batches improve'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='tensor parallelism performance, larger micro batches improve\\npipeline parallelism performance, and larger global batch size\\nimprove data parallelism performance. In practice, the Yuan 1.0\\nmodel performs well on text classification, Winograd Schema,\\nnatural language inference, and reading comprehension tasks.\\nGopher [116]: The Gopher family of models ranges from\\n44M to 280B parameters in size to study the effect of scale\\non the LLMs performance. The 280B model beats GPT-3 [6],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='on the LLMs performance. The 280B model beats GPT-3 [6],\\nJurrasic-1 [112], MT-NLG [117], and others on 81% of the\\nevaluated tasks.\\nERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\\nby training a larger model with 26x the number of parameters\\nof the latter. This bigger model outperformed other state-of-the-\\nart models in 68 NLP tasks. LLMs produce text with incorrect\\nfacts. In order to have control of the generated text with fac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 7}, page_content='facts. In order to have control of the generated text with fac-\\ntual consistency, ERNIE 3.0 Titan adds another task, Credible\\nand Controllable Generations, to its multi-task learning setup.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='It introduces additional self-supervised adversarial and control-\\nlable language modeling losses to the pre-training step, which\\nenables ERNIE 3.0 Titan to beat other LLMs in their manually\\nselected Factual QA task set evaluations.\\nGPT-NeoX-20B [118]: An auto-regressive model that largely\\nfollows GPT-3 with a few deviations in architecture design,\\ntrained on the Pile dataset without any data deduplication. GPT-\\nNeoX has parallel attention and feed-forward layers in a trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='NeoX has parallel attention and feed-forward layers in a trans-\\nformer block, given in Eq. 4, that increases throughput by 15%.\\nIt uses rotary positional embedding [66], applying it to only\\n25% of embedding vector dimension as in [119]. This reduces\\nthe computation without performance degradation. As opposed\\nto GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\\nuses only dense layers. The hyperparameter tuning at this scale\\nis difficult; therefore, the model chooses hyperparameters from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='is difficult; therefore, the model chooses hyperparameters from\\nthe method [6] and interpolates values between 13B and 175B\\nmodels for the 20B model. The model training is distributed\\namong GPUs using both tensor and pipeline parallelism.\\nx + Attn(LN1(x)) + FF(LN2(x))\\n(4)\\nOPT [14]: It is a clone of GPT-3, developed to open-source\\na model that replicates GPT-3 performance. Training of OPT\\nemploys dynamic loss scaling [120] and restarts from an earlier'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='employs dynamic loss scaling [120] and restarts from an earlier\\ncheckpoint with a lower learning rate whenever loss divergence\\nis observed. Overall, the performance of OPT-175B models is\\ncomparable to the GPT3-175B model.\\nBLOOM [13]: A causal decoder model trained on the ROOTS\\ncorpus to open-source an LLM. The architecture of BLOOM is\\nshown in Figure 9, with differences like ALiBi positional em-\\nbedding, an additional normalization layer after the embedding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='bedding, an additional normalization layer after the embedding\\nlayer as suggested by the bitsandbytes1 library. These changes\\nstabilize training with improved downstream performance.\\nGLaM [91]: Generalist Language Model (GLaM) represents a\\nfamily of language models using a sparsely activated decoder-\\nonly mixture-of-experts (MoE) structure [121, 90].\\nTo gain\\nmore model capacity while reducing computation, the experts\\nare sparsely activated where only the best two experts are used'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='are sparsely activated where only the best two experts are used\\nto process each input token. The largest GLaM model, GLaM\\n(64B/64E), is about 7× larger than GPT-3 [6], while only part of\\nthe parameters are activated per input token. The largest GLaM\\n(64B/64E) model achieves better overall results as compared\\nto GPT-3 while consuming only one-third of GPT-3’s training\\nenergy.\\nMT-NLG [117]: A 530B causal decoder based on the GPT-\\n2 architecture that has roughly 3× GPT-3 model parameters.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='2 architecture that has roughly 3× GPT-3 model parameters.\\nMT-NLG is trained on filtered high-quality data collected from\\nvarious public datasets and blends various types of datasets in a\\nsingle batch, which beats GPT-3 on several evaluations.\\nChinchilla [96]: A causal decoder trained on the same dataset\\nas the Gopher [116] but with a little different data sampling\\ndistribution (sampled from MassiveText). The model architec-\\nture is similar to the one used for Gopher, with the exception of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='ture is similar to the one used for Gopher, with the exception of\\nAdamW optimizer instead of Adam. Chinchilla identifies the\\n1https://github.com/TimDettmers/bitsandbytes\\nFigure 9: The BLOOM architecture example sourced from [13].\\nrelationship that model size should be doubled for every dou-\\nbling of training tokens. Over 400 language models ranging\\nfrom 70 million to over 16 billion parameters on 5 to 500 bil-\\nlion tokens are trained to get the estimates for compute-optimal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='lion tokens are trained to get the estimates for compute-optimal\\ntraining under a given budget. The authors train a 70B model\\nwith the same compute budget as Gopher (280B) but with 4\\ntimes more data. It outperforms Gopher [116], GPT-3 [6], and\\nothers on various downstream tasks, after fine-tuning.\\nAlexaTM [122]: An encoder-decoder model, where encoder\\nweights and decoder embeddings are initialized with a pre-\\ntrained encoder to speed up training. The encoder stays frozen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='trained encoder to speed up training. The encoder stays frozen\\nfor the initial 100k steps and is later unfrozen for end-to-end\\ntraining. The model is trained on a combination of denoising\\nand causal language modeling (CLM) objectives, concatenat-\\ning a [CLM] token at the beginning for mode switching. Dur-\\ning training, the CLM task is applied for 20% of the time, which\\nimproves the in-context learning performance.\\nPaLM [15]: A causal decoder with parallel attention and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='PaLM [15]: A causal decoder with parallel attention and\\nfeed-forward layers similar to Eq. 4, speeding up training by\\na factor of 15. Additional changes to the conventional trans-\\nformer model include SwiGLU activation, RoPE embeddings,\\nmulti-query attention that saves computation cost during decod-\\ning, and shared input-output embeddings. During training, loss\\nspiking was observed, and to fix it, model training was restarted\\nfrom a 100-step earlier checkpoint by skipping 200-500 batches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='from a 100-step earlier checkpoint by skipping 200-500 batches\\naround the spike. Moreover, the model was found to memo-\\nrize around 2.4% of the training data at the 540B model scale,\\nwhereas this number was lower for smaller models.\\nPaLM-2 [123]: A smaller multi-lingual variant of PaLM,\\ntrained for larger iterations on a better quality dataset. PaLM-\\n2 shows significant improvements over PaLM, while reducing\\ntraining and inference costs due to its smaller size. To lessen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='training and inference costs due to its smaller size. To lessen\\ntoxicity and memorization, it appends special tokens with a\\nfraction of pre-training data, which shows a reduction in gener-\\nating harmful responses.\\nU-PaLM [124]: This method trains PaLM for 0.1% addi-\\ntional compute with the UL2 (also named as UL2Restore) ob-\\njective [125], using the same dataset it outperforms the baseline\\nsignificantly on various NLP tasks, including zero-shot, few-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 8}, page_content='significantly on various NLP tasks, including zero-shot, few-\\nshot, commonsense reasoning, CoT, etc. Training with UL2R\\ninvolves converting a causal decoder PaLM to a non-causal de-\\ncoder PaLM and employing 50% sequential denoising, 25%\\nregular denoising, and 25% extreme denoising loss functions.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='UL2 [125]: An encoder-decoder architecture trained using a\\nmixture of denoisers (MoD) objective. Denoisers include 1)\\nR-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\\nrupts consecutive tokens of a large sequence and 3) X-Denoiser:\\nwhich corrupts a large number of tokens randomly. During pre-\\ntraining, UL2 includes a denoiser token from R, S, X to rep-\\nresent a denoising setup. It helps improve fine-tuning perfor-\\nmance for downstream tasks that bind the task to one of the up-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='mance for downstream tasks that bind the task to one of the up-\\nstream training modes. This MoD style of training outperforms\\nthe T5 model on many benchmarks.\\nGLM-130B [33]: GLM-130B is a bilingual (English and Chi-\\nnese) model trained using an auto-regressive mask infilling pre-\\ntraining objective similar to the GLM [126]. This training style\\nmakes the model bidirectional as compared to GPT-3, which is\\nunidirectional. As opposed to GLM, the training of GLM-130B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='unidirectional. As opposed to GLM, the training of GLM-130B\\nincludes a small amount of multi-task instruction pre-training\\ndata (5% of the total data) along with self-supervised mask in-\\nfilling. To stabilize the training, it applies embedding layer gra-\\ndient shrink.\\nLLaMA [127, 21]: A set of decoder-only language models\\nvarying from 7B to 70B parameters. LLaMA models series is\\nthe most famous among the community for parameter efficiency\\nand instruction tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='the most famous among the community for parameter efficiency\\nand instruction tuning.\\nLLaMA-1 [127]: Implements efficient causal attention [128]\\nby not storing and computing masked attention weights and\\nkey/query scores. Another optimization is reducing the number\\nof activations recomputed in the backward pass, as in [129].\\nLLaMA-2 [21]: This work is more focused on fine-tuning a\\nsafer and better LLaMA-2-Chat model for dialogue generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='safer and better LLaMA-2-Chat model for dialogue generation.\\nThe pre-trained model has 40% more training data with a larger\\ncontext length and grouped-query attention.\\nLLaMA-3/3.1 [130]: A collection of models trained on a\\nseven times larger dataset as compared to LLaMA-2 with dou-\\nble the context length, outperforming its previous variants and\\nother models.\\nPanGu-Σ [92]: An autoregressive model with parameters\\ncopied from PanGu-α and extended to a trillion scale with Ran-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='copied from PanGu-α and extended to a trillion scale with Ran-\\ndom Routed Experts (RRE), the architectural diagram is shown\\nin Figure 10. RRE is similar to the MoE architecture, with\\ndistinctions at the second level, where tokens are randomly\\nrouted to experts in a domain instead of using a learnable gat-\\ning method. The model has bottom layers densely activated and\\nshared across all domains, whereas top layers are sparsely ac-\\ntivated according to the domain. This training style allows for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='tivated according to the domain. This training style allows for\\nextracting task-specific models and reduces catastrophic forget-\\nting effects in the case of continual learning.\\nMixtral8x22b [131]: A mixture-of-experts (MoE) model with\\neight distinct experts routes each token to two experts at each\\nlayer and combines the outputs additively.\\nSnowflake Arctic [132]: Arctic LLM is a hybrid of dense and\\nmixture-of-experts (MoE) architecture. The MoE (128×3.66B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='mixture-of-experts (MoE) architecture. The MoE (128×3.66B\\nMLP experts) is parallel to the dense transformer (10B) with\\nonly two experts activated. The model has many experts, com-\\npared to other MoE LLMs [131, 133], to increase the model\\ncapacity and provide an opportunity to choose among many ex-\\nperts for a diverse configuration. The model has 480B param-\\neters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='eters, and only 17B are active during a forward pass, reducing\\nthe computation significantly.\\nGrok [133, 134]: Grok is a family of LLMs including Grok-1\\nand Grok-1.5, released by XAI.\\nGrok-1 [133]: Grok-1 is a 314B parameters language MoE\\nmodel (eight experts), where two experts are activated per to-\\nken.\\nGrok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\\ncontext length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='context length and improved performance.\\nGemini [135, 136]: Gemini replaces Bard (based on PaLM)\\nwith multi-modal capabilities and significant language model-\\ning performance improvements.\\nGemini-1 [135]: The first-ever auto-regressive model to\\nachieve human-level capabilities on the MMLU benchmark.\\nGemini-1.5 [136]: A multi-modal LLM with MoE architec-\\nture builds on the findings of Gemini-1. The model has a 2M\\ncontext window and can reason over information up to 10M'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='context window and can reason over information up to 10M\\ntokens. Such large context windows were never achieved pre-\\nviously and shown to have a huge impact on performance gain.\\nNemotron-4 340B [137]: A decoder-only model that has been\\naligned on 98% synthetic data and only 2% manually annotated\\ndata. Utilizing synthetic data at a large proportion improves the\\nmodel performance significantly. The paper suggested intro-\\nducing alignment data with a smaller subset of previously seen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='ducing alignment data with a smaller subset of previously seen\\ndata during the late stage of the model pre-training, enabling the\\nsmooth transition from the pre-trained stage to the final train-\\ning stage. To train better instruction-following models, weaker\\nmodels are trained into stronger models iteratively. The syn-\\nthetic data generated by the weaker instruction-tuned model is\\nused to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='used to train a base model which is later supervised fine-tuned\\noutperforming the weaker model.\\nDeepSeek [138]: DeepSeek studies the LLMs scaling laws\\nin detail to determine the optimal non-embedding model size\\nand training data. The experiments were performed for 8 bud-\\ngets ranging from 1e17 to 3e20 training FLOPs. Each compute\\nbudget was tested against ten different models/data scales. The\\nbatch size and learning rates were also fitted for the given com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='batch size and learning rates were also fitted for the given com-\\npute budget finding that the batch size should increase with\\nthe increased compute budget while decreasing the learning\\nrate. Following are the equations for the optimal batch-size (B),\\nlearning rate (η), model size (M), and data (D):\\nBopt = 0.2920.C0.3271\\nηopt = 0.3118.C−0.1250\\nMopt = Mbase.Ca\\nDopt = Dbase.Cb\\nMbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\\n(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 9}, page_content='(5)\\nDeepSeek-v2 [139]: An MoE model that introduces multi-\\nhead latent attention (MLA) to reduce inference costs, by com-\\npressing Key-Value (KV) cache into a latent vector.\\nMLA\\nachieves better performance than multi-head attention (MHA),\\nand other efficient attention mechanisms such as grouped query\\nattention (GQA), multi-query attention (MQA), etc. Because\\nof MLA, DeepSeek-v2 achieves 5.76 times faster inference\\nthroughput as compared to DeepSeek [138].\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='3.1.2. Coding\\nCodeGen [140]:\\nCodeGen has a similar architecture to\\nPaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\\nbeddings. The model is trained on both natural language and\\nprogramming language data sequentially (trained on the first\\ndataset, then the second, and so on) on the following datasets\\n1) PILE, 2) BIGQUERY, and 3) BIGPYTHON. CodeGen pro-\\nposed a multi-step approach to synthesizing code. The purpose\\nis to simplify the generation of long sequences where the previ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='is to simplify the generation of long sequences where the previ-\\nous prompt and generated code are given as input with the next\\nprompt to generate the next code sequence. CodeGen open-\\nsource a Multi-Turn Programming Benchmark (MTPB) to eval-\\nuate multi-step program synthesis.\\nCodex [141]: This LLM is trained on a subset of public Python\\nGithub repositories to generate code from docstrings. Com-\\nputer programming is an iterative process where the programs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='puter programming is an iterative process where the programs\\nare often debugged and updated before fulfilling the require-\\nments. Similarly, Codex generates 100 versions of a program\\nby repetitive sampling for a given description, which produces\\na working solution for 77.5% of the problems passing unit tests.\\nIts powerful version powers Github Copilot2.\\nAlphaCode [142]: A set of large language models, ranging\\nfrom 300M to 41B parameters, designed for competition-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='from 300M to 41B parameters, designed for competition-level\\ncode generation tasks. It uses the multi-query attention [143] to\\nreduce memory and cache costs. Since competitive program-\\nming problems highly require deep reasoning and an under-\\nstanding of complex natural language algorithms, the Alpha-\\nCode models are pre-trained on filtered GitHub code in popular\\nlanguages and then fine-tuned on a new competitive program-\\nming dataset named CodeContests. The CodeContests dataset'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='ming dataset named CodeContests. The CodeContests dataset\\nmainly contains problems, solutions, and test cases collected\\nfrom the Codeforces platform3. The pre-training employs stan-\\ndard language modeling objectives, while GOLD [144] with\\ntempering [145] serves as the training objective for the fine-\\ntuning on CodeContests data. To evaluate the performance of\\nAlphaCode, simulated programming competitions are hosted\\non the Codeforces platform: overall, AlphaCode ranks at the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='on the Codeforces platform: overall, AlphaCode ranks at the\\ntop 54.3% among over 5000 competitors, where its Codeforces\\nrating is within the top 28% of recently participated users.\\nCodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with\\nshallow encoder and deep decoder, trained in multiple stages\\ninitially unimodal data (code) and later bimodal data (text-code\\npairs). Each training stage has different training objectives and\\nactivates different model blocks encoder, decoder, or both ac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='activates different model blocks encoder, decoder, or both ac-\\ncording to the task. The unimodal pre-training includes span\\ndenoising and CLM objectives, whereas bimodal pre-training\\nobjectives contain contrastive learning, matching, and CLM for\\ntext-code pairs. CodeT5+ adds special tokens with the text to\\nenable task modes, for example, [CLS ] for contrastive loss,\\n[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='[Match] for text-code matching, etc.\\nStarCoder [147]: A decoder-only model with the SantaCoder\\narchitecture, employing Flash attention to scale up the context\\nlength to 8k. The StarCoder trains an encoder to filter names,\\n2https://github.com/features/copilot\\n3https://codeforces.com/\\nemails, and other personal data from the training data. Its fine-\\ntuned variant outperforms PaLM, LLaMA, and LAMDA on\\nHumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='HumanEval and MBPP benchmarks.\\n3.1.3. Scientific Knowledge\\nGalactica [148]: A large curated corpus of human scientific\\nknowledge with 48 million papers, textbooks, lecture notes,\\nmillions of compounds and proteins, scientific websites, en-\\ncyclopedias, and more are trained using the metaseq library3,\\nwhich is built on PyTorch and fairscale [149]. The model wraps\\nreasoning datasets with the < work > token to provide step-by-\\nstep reasoning context to the model, which has been shown to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='step reasoning context to the model, which has been shown to\\nimprove the performance on reasoning tasks.\\n3.1.4. Dialog\\nLaMDA [150]: A decoder-only model pre-trained on pub-\\nlic dialog data, public dialog utterances, and public web doc-\\numents, where more than 90% of the pre-training data is in\\nEnglish. LaMDA is trained with the objective of producing re-\\nsponses that exhibit high levels of quality, safety, and grounded-\\nness. To achieve this, discriminative and generative fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='ness. To achieve this, discriminative and generative fine-tuning\\ntechniques are incorporated to enhance the model’s safety and\\nquality aspects. As a result, the LaMDA models can be utilized\\nas a general language model performing various tasks.\\n3.1.5. Finance\\nBloombergGPT [151]: A non-causal decoder model trained\\nusing both financial (“FINPILE” from the Bloomberg archive)\\nand general-purpose datasets. The model’s architecture is sim-\\nilar to the BLOOM [13] and OPT [14]. It allocates 50B param-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='ilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\\neters to different blocks of the model using the approach [113].\\nFor effective training, BloombergGPT packs documents to-\\ngether with < |endoftext| > to use the maximum sequence\\nlength, uses warmup batch size starting from 1024 to 2048, and\\nmanually reduces the learning rate multiple times during the\\ntraining.\\nXuan Yuan 2.0 [152]: A Chinese financial chat model with\\nBLOOM’s [13] architecture trained on a combination of general'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='BLOOM’s [13] architecture trained on a combination of general\\npurpose, financial, general purpose instructions, and financial\\ninstitutions datasets. Xuan Yuan 2.0 combined the pre-training\\nand fine-tuning stages to avoid catastrophic forgetting.\\n3.2. Fine-Tuned LLMs\\nPre-trained LLMs have excellent generalization abilities to\\nunseen tasks. However, because they are generally trained with\\nthe objective of next token prediction, LLMs have limited ca-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='the objective of next token prediction, LLMs have limited ca-\\npacity to follow user intent and are prone to generate unethical,\\ntoxic or inaccurate responses [20]. For their effective utiliza-\\ntion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\\ngenerate safe responses [20], which also results in increasing\\nzero-shot, few-shot, and cross-task generalization [97, 16, 18],\\nwith minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 10}, page_content='with minimal compute increment, e.g., 0.2% of the total pre-\\ntraining for PaLM 540B [16].\\nWe review various fine-tuned LLMs and strategies for effective\\nfine-tuning in this section.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\\nModels\\nFindings & Insights\\nT5\\n• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\\n• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\\nclassification layers\\nGPT-3\\n• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\\nlearners\\nmT5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='learners\\nmT5\\n• Large multi-lingual models perform equivalently to single language models on downstream tasks.\\nHowever, smaller multi-lingual models perform worse\\nPanGu-α\\n• LLMs have good few shot capabilities\\nCPM-2\\n• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\\nble to full model fine-tuning\\n• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\\n• Inserting prompt tokens in-between sentences can allow the model to understand relations between\\nsentences and long sequences\\n• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\\n(aggregate information with the input text) for the model\\nERNIE 3.0\\n• A modular LLM architecture with a universal representation module and task-specific representa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• A modular LLM architecture with a universal representation module and task-specific representa-\\ntion module helps in the finetuning phase\\n• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\\nan efficient way to take advantage of the powerful pre-trained model\\nJurassic-1\\n• The performance of LLM is highly related to the network size\\n• To improve runtime performance, more operations can be performed in parallel (width) rather than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• To improve runtime performance, more operations can be performed in parallel (width) rather than\\nsequential (depth)\\n• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\\ncabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\\nbenefits in few-shot learning tasks\\nHyperCLOVA\\n• By employing prompt-based tuning, the performances of models can be improved, often surpassing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='• By employing prompt-based tuning, the performances of models can be improved, often surpassing\\nthose of state-of-the-art models when the backward gradients of inputs are accessible\\nYuan 1.0\\n• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\\nbehavior in zero-shot and few-shot learning\\nGopher\\n• Relative encodings enable the model to evaluate for longer sequences than training.\\nERNIE 3.0 Titan'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='ERNIE 3.0 Titan\\n• Additional self-supervised adversarial loss to distinguish between real and generated text improves\\nthe model performance as compared to ERNIE 3.0\\nGPT-NeoX-20B\\n• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\\nlayers\\n• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations\\nfrom growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 11}, page_content='from growing with increasing depth and width\\n• Training on Pile outperforms GPT-3 on five-shot\\nTable Continued on Next Page\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='Models\\nFindings & Insights\\nOPT\\n• Restart training from an earlier checkpoint with a lower learning rate if loss diverges\\n• Model is prone to generate repetitive text and stuck in a loop\\nGalactica\\n• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\\ndomain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\\nresearch on LLMs\\n• A working memory token approach can achieve strong performance over existing methods on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='• A working memory token approach can achieve strong performance over existing methods on\\nmathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\\ntasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\\nGLaM\\n• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\\nin each transformer layer with a mixture-of-experts (MoE)\\n• The model trained on filtered data shows consistently better performances on both NLG and NLU'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='• The model trained on filtered data shows consistently better performances on both NLG and NLU\\ntasks, where the effect of filtering is more significant on the former tasks\\n• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\\nthe downstream tasks\\n• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\\nthe MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\\nmance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='mance\\nLaMDA\\n• The model can be fine-tuned to learn to call different external information resources and tools\\nAlphaCode\\n• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed\\nwith a shallower encoder and a deeper decoder\\n• To achieve better performances, it is necessary to employ strategies such as massively scaling\\nupsampling, followed by the filtering and clustering of samples into a compact set'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='upsampling, followed by the filtering and clustering of samples into a compact set\\n• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-\\nscale sampling is crucial\\n• Simplifying problem descriptions can effectively improve the model’s performance\\nChinchilla\\n• The model size and the number of training tokens should be scaled proportionately: for each dou-\\nbling of the model size, the number of training tokens should be doubled as well\\nPaLM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='bling of the model size, the number of training tokens should be doubled as well\\nPaLM\\n• English-centric models produce better translations when translating to English as compared to non-\\nEnglish\\n• Generalized models can have equivalent performance for language translation to specialized small\\nmodels\\n• Larger models have a higher percentage of training data memorization\\n• Performance has not yet saturated even at 540B scale, which means larger models are likely to\\nperform better\\nAlexaTM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 12}, page_content='perform better\\nAlexaTM\\n• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\\ncontext than decoder-only\\n• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\\nlearning\\n• Placing layer norm at the beginning of each transformer layer improves the training stability\\nTable Continued on Next Page\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='Models\\nFindings & Insights\\nU-PaLM\\n• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\\n• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\\ndiversity\\nUL2\\n• Mode switching training enables better performance on downstream tasks\\n• CoT prompting outperforms standard prompting for UL2\\nGLM-130B\\n• Pre-training data with a small proportion of multi-task instruction data improves the overall model\\nperformance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='performance\\nCodeGen\\n• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\\neration\\nLLaMA\\n• A constant performance improvement is observed when scaling the model\\n• Smaller models can achieve good performances with more training data and computing time\\nPanGu-Σ\\n• Sparse models provide the benefits of large models at a lower computation cost\\n• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for\\ncontinual learning\\n• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\\ncost-efficient while maintaining a performance similar to the original\\nBloombergGPT\\n• Pre-training with general-purpose and task-specific data improves task performance without hurt-\\ning other model capabilities\\nXuanYuan 2.0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='ing other model capabilities\\nXuanYuan 2.0\\n• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\\nCodeT5+\\n• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\\n• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\\nfor better performance\\nStarCoder\\n• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\\nLLaMA-2\\n• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\\nfine-tuning\\n• Model trained on unfiltered data requires fewer samples for safety alignment\\nPaLM-2\\n• Data quality is important to train better models\\n• Model and data size should be scaled with 1:1 proportions\\n• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='• Smaller models trained for larger iterations outperform larger models\\nLLaMA-3/3.1\\n• Increasing batch size gradually stabilizes the training without loss spikes\\n• High-quality data at the final stages of training improves the model performance\\n• Increasing model context length windows step-wise allows it to better adapt to various sequence\\nlengths\\nNemotron-40B\\n• Model aligned iteratively on synthetic data with data generated from the previously aligned model\\nachieves competitive performance'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 13}, page_content='achieves competitive performance\\nDeepSeek\\n• Batch size should increase with the increase in compute budget while decreasing the learning rate\\nDeepSeek-v2\\n• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring\\na significantly smaller KV cache, therefore achieving faster data generation\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\\nModels\\nFindings & Insights\\nT0\\n• Multi-task prompting enables zero-shot generalization and outperforms baselines\\n• Even a single prompt per dataset task is enough to improve performance\\nWebGPT\\n• To aid the model in effectively filtering and utilizing relevant information, human labelers play a\\ncrucial role in answering questions regarding the usefulness of the retrieved documents'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='crucial role in answering questions regarding the usefulness of the retrieved documents\\n• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\\nend-to-end retrieval and synthesis via imitation learning and reinforcement learning\\n• Generating answers with references can make labelers easily judge the factual accuracy of answers\\nTk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='Tk-INSTRUCT\\n• Instruction tuning leads to a stronger generalization of unseen tasks\\n• More tasks improve generalization whereas only increasing task instances does not help\\n• Supervised trained models are better than generalized models\\n• Models pre-trained with instructions and examples perform well for different types of inputs\\nmT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='mT0 and BLOOMZ\\n• Instruction tuning enables zero-shot generalization to tasks never seen before\\n• Multi-lingual training leads to even better zero-shot generalization for both English and non-\\nEnglish\\n• Training on machine-translated prompts improves performance for held-out tasks with non-English\\nprompts\\n• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\\nother pre-trained language tasks\\nOPT-IML'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='other pre-trained language tasks\\nOPT-IML\\n• Creating a batch with multiple task examples is important for better performance\\n• Only example proportional sampling is not enough, training datasets should also be proportional\\nfor better generalization/performance\\n• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\\nwhereas fully supervised tasks have no effect\\n• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\\n• Only 1% reasoning data improves the performance, adding more deteriorates performance\\n• Adding dialogue data makes the performance worse\\nSparrow\\n• Labelers’ judgment and well-defined alignment rules help the model generate better responses\\n• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\\nraters'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='raters\\n• The combination of reinforcement learning (RL) with reranking yields optimal performance in\\nterms of preference win rates and resilience against adversarial probing\\nFlan\\n• Finetuning with CoT improves performance on held-out tasks\\n• Fine-tuning along with CoT data improves reasoning abilities\\n• CoT tuning improves zero-shot reasoning\\n• Performance improves with more tasks\\n• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\\n• Improving the model’s performance with instruction tuning is compute-efficient\\n• Multitask prompting enables zero-shot generalization abilities in LLM\\nWizardCoder\\n• Fine-tuning with re-written instruction-tuning data into a complex set improves performance\\nLLaMA-2-Chat\\n• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 14}, page_content='• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\\nRLHF step further improves model safety and make it less prone to jailbreak attacks\\nLIMA\\n• Less high quality data is enough for fine-tuned model generalization\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Figure 10: This example illustrates the PanGu-P architecture, as depicted in\\nthe image sourced from [92].\\n3.2.1. Instruction-Tuning with Manually Created Datasets\\nNumerous hand-crafted instruction-tuning datasets with\\ndifferent design choices are proposed in the literature to\\ninstruction-tune LLMs. The performance of fine-tuned LLMs\\ndepends on multiple factors, such as dataset, instruction diver-\\nsity, prompting templates, model size, and training objectives.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='sity, prompting templates, model size, and training objectives.\\nKeeping this in view, diverse fine-tuned models have emerged\\nin the literature using manually created datasets.\\nThe models T0 [17] and mT0 (multi-lingual) [154] employ\\ntemplates to convert existing datasets into prompt datasets.\\nThey have shown improvements in generalization to zero-shot\\nand held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\\nwith in-context instructions to study generalization on unseen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='with in-context instructions to study generalization on unseen\\ntasks when given in-context instructions during test time. The\\nmodel outperformed Instruct-GPT, despite being smaller in\\nsize, i.e., 11B parameters as compared to 175B of GPT-3.\\nIncreasing Tasks and Prompt Setups: Zero-shot and few-shot\\nperformance improves significantly by expanding task collec-\\ntion and prompt styles. OPT-IML [97] and Flan [16] curated\\nlarger 2k and 1.8k task datasets, respectively. While increasing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='larger 2k and 1.8k task datasets, respectively. While increasing\\ntask size alone is not enough, OPT-IML and Flan add more\\nprompting setups in their datasets, zero-shot, few-shot, and\\nCoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\\nfurther on 1.88M CoT samples. Another method [102] uses\\nsymbolic tasks with tasks in T0, Flan, etc.\\n3.2.2. Instruction-Tuning with LLMs Generated Datasets\\nGenerating an instruction-tuning dataset requires carefully'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Generating an instruction-tuning dataset requires carefully\\nwriting instructions and input-output pairs, which are often\\nwritten by humans, smaller in size, and less diverse. To over-\\ncome this, self-instruct [19] proposed an approach to prompt\\navailable LLMs to generate instruction-tuning datasets. Self-\\ninstruct outperformed models trained on manually created\\ndataset SUPER-NATURALINSTRUCTIONS (a dataset with\\n1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\\ninstruction, and 1 sample per task and iteratively generates new\\ninstructions (52k) and instances (82k input-output pairs) using\\nFigure 11: An example image shows an instance of the Flan training paradigm,\\ntaken from [16].\\nGPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data\\nof datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='of datasets on Huggingface to prompt LLMs to generate multi-\\nple task instruction-tuning datasets.\\nLLaMA Tuned: Various models in the literature instruction-\\ntune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-\\nated datasets.\\nAmong these, Alpaca [158], Vicuna [159],\\nand LLaMA-GPT-4 [160] are a few general-purpose fine-tuned\\nmodels, where Alpaca is trained on 52k samples from text-\\ndavinci-003, Vicuna on 70k samples from ShareGPT.com, and\\nLLaMA-GPT-4 by re-creating Alpaca instructions from GPT-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\\n4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million\\nsamples) by generating data from ChatGPT and outperforms\\nGPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the\\nLLaMA’s consistent tokenization of numbers. HuaTuo [162] is\\na medical knowledge model, fine-tuned with a generated QA\\ndataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='dataset of 8k instructions.\\nComplex Instructions: Evol-Instruct [163, 164] prompts LLMs\\nto convert given instructions into a more complex set. The in-\\nstructions are iteratively evolved with re-writing instructions in\\ncomplex wording and creating new instructions. With this style\\nof automated instruction generation, WizardLM [163] (fine-\\ntuned LLaMA on 250k instructions), outperforms Vicuna and\\nAlpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\\nClaude-Plus, Bard, and others.\\n3.2.3. Aligning with Human Preferences\\nIncorporating human preferences into LLMs presents a\\nsignificant advantage in mitigating undesirable behaviors and\\nensuring accurate outputs. The initial work on alignment, such\\nas InstructGPT [20] aligns GPT-3 using a 3-step approach,\\ninstruction-tuning, reward modeling, and fine-tuning with\\nreinforcement learning (RL). The supervised fine-tuned GPT-3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='reinforcement learning (RL). The supervised fine-tuned GPT-3\\non demonstrations is queried to generate responses, which\\nhuman labelers rank according to human values, and a reward\\nmodel is trained on the ranked data. Lastly, the GPT-3 is trained\\nwith proximal policy optimization (PPO) using rewards on the\\ngenerated data from the reward model. LLaMA 2-Chat [21]\\nimproves alignment by dividing reward modeling into help-\\nfulness and safety rewards and using rejection sampling in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 15}, page_content='fulness and safety rewards and using rejection sampling in\\naddition to PPO. The initial four versions of LLaMA 2-Chat\\nare fine-tuned with rejection sampling and then with PPO on\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='top of rejection sampling.\\nAligning with Supported Evidence: This style of alignment\\nallows the model to generate responses with proofs and facts,\\nreduces hallucination, and assists humans more effectively,\\nwhich increases trust in the model’s output.\\nSimilar to\\nthe RLHF training style, a reward model is trained to rank\\ngenerated responses containing web citations in answers\\nto questions, which is later used to train the model, as in\\nGopherCite [165], WebGPT [166], and Sparrow [167]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='GopherCite [165], WebGPT [166], and Sparrow [167]. The\\nranking model in Sparrow [167] is divided into two branches,\\npreference reward and rule reward, where human annotators\\nadversarial probe the model to break a rule. These two rewards\\ntogether rank a response to train with RL.\\nAligning Directly with SFT: The PPO in the RLHF pipeline\\nis complex, memory-intensive, and unstable, requiring mul-\\ntiple models, reward, value, policy, and reference models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='tiple models, reward, value, policy, and reference models.\\nAvoiding this sophisticated alignment pipeline is possible by\\nincorporating minimal changes in the supervised fine-tuning\\n(SFT) pipeline as in [168, 169, 170], with better or compa-\\nrable performance to PPO. Direct preference optimization\\n(DPO) [168] trains a model directly on the human-preferred\\nresponses to maximize the likelihood of preferred against\\nunpreferred responses, with per-sample importance weight.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='unpreferred responses, with per-sample importance weight.\\nReward ranked fine-tuning RAFT [169] fine-tunes the model\\non ranked responses by the reward model. Preference ranking\\noptimization (PRO) [171] and RRHF [170] penalize the model\\nto rank responses with human preferences and supervised loss.\\nOn the other hand, chain-of-hindsight (CoH) [172] provides\\nfeedback to the model in language rather than reward, to learn\\ngood versus bad responses.\\nAligning with Synthetic Feedback:\\nAligning LLMs with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='good versus bad responses.\\nAligning with Synthetic Feedback:\\nAligning LLMs with\\nhuman feedback is slow and costly. The literature suggests a\\nsemi-automated process to align LLMs by prompting LLMs to\\ngenerate helpful, honest, and ethical responses to the queries,\\nand fine-tuning using the newly created dataset. Constitutional\\nAI [173] replaces human feedback in RLHF with AI, calling\\nit RL from AI feedback (RLAIF). AlpacaFarm [174] designs\\nprompts to imitate human feedback using LLMs APIs. Oppo-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='prompts to imitate human feedback using LLMs APIs. Oppo-\\nsite to constitutional AI, AlpacaFarm injects noise in feedback\\nto replicate human mistakes.\\nSelf-Align [98] prompts the\\nLLM with ICL examples, instructing the LLM about what the\\nresponse should contain to be considered useful and ethical.\\nThe same LLM is later fine-tuned with the new dataset.\\nAligning with Prompts: LLMs can be steered with prompts to\\ngenerate desirable responses without training [175, 176]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='generate desirable responses without training [175, 176]. The\\nself-correction prompting in [176] concatenates instructions\\nand CoT with questions, guiding the model to answer its\\ninstruction following a strategy to ensure moral safety before\\nthe actual answer. This strategy is shown to reduce the harm in\\ngenerated responses significantly.\\nRed-Teaming/Jailbreaking/Adversarial\\nAttacks:\\nLLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='Attacks:\\nLLMs\\nexhibit harmful behaviors, hallucinations, leaking personal in-\\nformation, and other shortcomings through adversarial probing.\\nThe models are susceptible to generating harmful responses\\neven though they are aligned for safety [177, 178].\\nRed-\\nteaming is a common approach to address illicit outputs, where\\nthe LLMs are prompted to generate harmful outputs [178, 179].\\nThe dataset collected through red-teaming is used to fine-tune'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='The dataset collected through red-teaming is used to fine-tune\\nmodels for safety. While red-teaming largely relies on human\\nannotators, another work [180] red-team LLMs to find prompts\\nthat lead to harmful outputs for other LLMs.\\n3.2.4. Continue Pre-Training\\nAlthough fine-tuning boosts a model’s performance, it leads\\nto catastrophic forgetting of previously learned information.\\nConcatenating fine-tuning data with a few randomly selected'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='Concatenating fine-tuning data with a few randomly selected\\npre-training samples in every iteration avoids network forget-\\nting [181, 152]. This is also effective in adapting LLMs for\\ncases where fine-tuning data is small and the original capac-\\nity is to be maintained. Prompt-based continued pre-training\\n(PCP) [182] trains the model with text and instructions related\\nto tasks and then finally instruction-tunes the model for down-\\nstream tasks.\\n3.2.5. Sample Efficiency'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='stream tasks.\\n3.2.5. Sample Efficiency\\nWhile fine-tuning data is generally many-fold smaller than\\nthe pre-training data, it still has to be large enough for accept-\\nable performance [16, 97, 18] and requires proportional com-\\nputing resources. Studying the effects on performance with less\\ndata, existing literature [183, 184] finds that models trained\\non less data can outperform models trained with more data.\\nIn [183], 25% of the total downstream data is found enough'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='In [183], 25% of the total downstream data is found enough\\nfor state-of-the-art performance. Selecting coreset-based 0.5%\\nof the total instruction-tuning data improves the model perfor-\\nmance by 2% in [184], as compared to the complete data tun-\\ning. Less is more for alignment (LIMA) [185] uses only 1000\\ncarefully created demonstrations to fine-tune the model and has\\nachieved comparable performance to GPT-4.\\n3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='3.3. Increasing Context Window\\nLLMs are trained with limited context windows due to ex-\\npensive attention and high memory requirements.\\nA model\\ntrained on limited sequence lengths fails to generalize to unseen\\nlengths at inference time [186, 49]. Alternatively, LLMs with\\nALiBi [65] positional encodings can perform zero-shot length\\nextrapolation. However, ALiBi has less expressive power [66]\\nand inferior performance on multiple benchmarks [46], and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='and inferior performance on multiple benchmarks [46], and\\nmany LLMs use RoPE positional embedding that is unable to\\nperform zero-shot extrapolation. A larger context length has\\nbenefits such as a better understanding of longer documents,\\nmore samples in in-context learning, execution of bigger rea-\\nsoning processes, etc. Expanding context length during fine-\\ntuning is slow, inefficient, and computationally expensive [49].\\nTherefore, researchers employ various context window extrap-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='Therefore, researchers employ various context window extrap-\\nolation techniques discussed below.\\nPosition Interpolation: Rather than extrapolating, [49] shows\\nthat interpolating position encodings within the pre-trained con-\\ntext window are more effective. The work demonstrates that\\nonly 1000 steps of fine-tuning are enough to achieve better re-\\nsults on larger windows without reducing performance com-\\npared to the original context size. Giraffe [46] uses power scal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 16}, page_content='pared to the original context size. Giraffe [46] uses power scal-\\ning in RoPE, and YaRN [47] proposed NTK-aware interpola-\\ntion.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='Efficient Attention Mechanism:\\nDense global attention is\\none of the major constraints in training larger context win-\\ndow LLMs.\\nUsing efficient attention variants, such as lo-\\ncal, sparse, and dilated attention, reduces the computation cost\\nsignificantly.\\nLongT5 [48] proposes transient global atten-\\ntion (TGlobal), applying attention to local and global tokens\\n(windowed token averaging).\\nThe model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='The model replaces attention\\nin T5 [10] with TGlobal attention, pre-trains the model on\\n4098 sequence length, fine-tunes on larger window sizes, as\\nlarge as 16k, and improves task performance on longer inputs.\\nThis shows the extrapolation ability of TGlobal attention with\\nonly fine-tuning. COLT5 [187] uses two branches, one with\\nlightweight and the other with heavyweight attention and feed-\\nforward layers. All tokens are processed from the lightweight'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='forward layers. All tokens are processed from the lightweight\\nbranch, and only important tokens are routed to the heavy-\\nweight branch. LongNet [188] replaces standard attention with\\ndilated attention, expanding sequence length to 1 billion tokens.\\nLongLoRA [189] proposes shift-short attention, used during\\nfine-tuning to reduce dense attention costs. However, the model\\nduring inference uses dense attention and achieves similar per-\\nformance as full attention fine-tuning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='formance as full attention fine-tuning.\\nExtrapolation without Training: LM-Infinite [186] and par-\\nallel context windows (PCW) [190] show length extrapolation\\nis possible using pre-trained LLMs. LM-Infinite suggested Λ-\\nshaped attention applied within the original context window\\nlimits. Likewise, PCW chunks larger inputs into the pre-trained\\ncontext lengths and applies the same positional encodings to\\neach chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='each chunk.\\n3.4. Augmented LLMs\\nLLMs are capable of learning from the examples concate-\\nnated with the input, known as context augmentation, in-\\ncontext learning (ICL), or few-shot prompting. They show ex-\\ncellent generalization to unseen tasks with few-shot prompt-\\ning, enabling LLMs to answer queries beyond the capacity ac-\\nquired during training [6, 55]. These emergent abilities allow\\nfor adapting the model without fine-tuning—a costly process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='for adapting the model without fine-tuning—a costly process.\\nAside from this, hallucination, producing inaccurate, unsafe,\\nor factually incorrect responses, is common for LLMs, which is\\navoided by augmenting contextual data. While the user can pro-\\nvide in-context samples in the query [54, 32], here we specifi-\\ncally refer to the methods that access external storage program-\\nmatically, calling them augmented LLMs.\\nThe literature suggests various external memory designs to aug-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='The literature suggests various external memory designs to aug-\\nment LLMs, long-term [191, 192, 193, 194], short-term [195],\\nsymbolic [196], and non-symbolic [197, 198]. The memory\\ncan be maintained in different formats such as documents, vec-\\ntors, or databases. A few systems maintain intermediate mem-\\nory representations to retain information across multiple iter-\\nations [194, 192], while others extract important information\\nfrom the datasets and save it in memory for recall [199]. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='from the datasets and save it in memory for recall [199]. The\\nmemory read and write operations are performed either with\\nor without LLMs cooperation [192, 200, 194, 201], acting as\\na feedback signal in [195]. We discuss different types of aug-\\nmented LLMs below.\\nFigure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\\ntracts a similar context to the input and forwards it to the LLM either in simple\\nlanguage or encoded through Fusion-in-Decoder (FiD). Depending on the task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='language or encoded through Fusion-in-Decoder (FiD). Depending on the task,\\nretrieval and generation may repeat multiple times.\\n3.4.1. Retrieval Augmented LLMs\\nLLMs may have limited memory and outdated information,\\nleading to inaccurate responses. Retrieving relevant informa-\\ntion from external up-to-date storage enables the LLMs to\\naccurately answer with references and utilize more informa-\\ntion. With retrieval augmentation, smaller models have been'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='tion. With retrieval augmentation, smaller models have been\\nshown to perform at par with larger models. For instance, the\\n11B model can become competitive to 540B PaLM in [25] and\\n7.5B to 280B Gopher in [193]. Retrieval augmented language\\nmodeling (RALM) has two major components, shown in\\nFigure 12, namely: 1) retriever and 2) language model.\\nIn\\nRALM, the retriever plays a crucial role in driving LLM\\nresponse, where incorrect information can steer LLMs to false'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='response, where incorrect information can steer LLMs to false\\nbehavior. This leads to the development of various methods to\\nretrieve accurate information and fuse with the query for better\\nperformance.\\nZero-Shot Retrieval Augmentation: This kind of augmen-\\ntation keeps the original LLM architecture and weights\\nunchanged and uses BM25 [202], nearest neighbors, or frozen\\npre-trained models like Bert [7] as a retriever. The retrieved\\ninformation is provided as input to the model for response'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='information is provided as input to the model for response\\ngeneration, shown to improve performance over LLMs without\\nretrieval [198, 203].\\nIn some scenarios, multiple retrieval\\niterations are required to complete the task.\\nThe output\\ngenerated in the first iteration is forwarded to the retriever\\nto fetch similar documents. Forward-looking active retrieval\\n(FLARE) [197] initially generates the response and corrects\\nthe output by retrieving relevant documents if the response'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='the output by retrieving relevant documents if the response\\ncontains low-confidence tokens. Similarly, RepoCoder [204]\\nfetches code snippets recursively for code completion.\\nTraining with Retrieval Augmentation: To reduce failures in\\nretrieval augmentation generation (RAG), researchers train or\\nfine-tune retrievers and LLMs with a retrieval augmentation\\npipeline. We discuss the literature below based on their focus\\non the respective training processes of the pipeline.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 17}, page_content='on the respective training processes of the pipeline.\\nTraining LLM: Retrieval-enhanced transformer (RETRO) [193]\\nshows pre-training smaller LLMs with RAG pipeline outper-\\nforms larger LLMs, such as GPT-3 trained without RAG.\\nRETRO uses a 2-trillion token subset of MassiveText as\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='a database.\\nThe retrieval pipeline divides the input query\\ninto subsets and retrieves relevant chunks from the database\\nfor each subset, encoded together with input intermediate\\nrepresentations for generating tokens. It uses cross-chunked\\nattention to attend to previous chunks auto-regressively.\\nA\\nstudy on RETRO [205] shows models pre-trained without RAG\\nbut fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='but fine-tuned using RAG lack the performance gains obtained\\nby pre-training with RAG.\\nTraining Retriever: Quality of responses generated by LLMs\\nis highly dependent on the in-context examples.\\nThere-\\nfore, [206, 207, 208, 209] train retrievers to retrieve accurate\\nfew-shot samples while keeping the LLM frozen for gener-\\nation.\\nRetrieved samples are ranked to build ground-truth\\ndata to train retrievers with contrastive learning in [206, 208].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='data to train retrievers with contrastive learning in [206, 208].\\nRoBERTa is trained for downstream tasks in [207] for ICL\\nsamples retrieval.\\nREPLUG [209] trains the retriever with\\nsupervised signals from the frozen LLM-generated outputs.\\nTraining Retriever and LLM: Further benefits are achieved by\\ntraining both the retriever and the model in [25, 210, 211]. In\\nthis case, the error propagates back to the retriever, updating\\nboth the language model and the retriever.\\nWhile masked'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='both the language model and the retriever.\\nWhile masked\\nlanguage modeling (MLM) is a common pre-training objec-\\ntive [25, 211], retrieval pre-trained transformer (RPT) [210]\\nused document chunk prediction as a pre-training objective for\\nlong text modeling.\\nEncoded Context Augmentation:\\nConcatenating retrieved\\ndocuments with the query becomes infeasible as the sequence\\nlength and sample size grow. Encoding the context and fusing\\nit with the decoder (Fusion-in-Decoder) using cross-attention'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='it with the decoder (Fusion-in-Decoder) using cross-attention\\nmakes it possible to augment more samples without increasing\\ncomputation costs significantly [212, 193, 210, 25].\\nWeb Augmented:\\nLocally stored memory, but external to\\nLLM, has limited information. However, a large amount of\\ninformation is available on the internet, which gets updated\\nregularly.\\nRather than storing information locally, various\\nmethods retrieve query-related context through a web search'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='methods retrieve query-related context through a web search\\nand forward it to LLMs [213, 214, 166].\\n3.4.2. Tool Augmented LLMs\\nWhile RAG relies on the retriever to provide context to the\\nLLM to answer queries, tool augmented LLMs capitalize on the\\nreasoning abilities of LLMs to iteratively plan by dividing tasks\\ninto sub-tasks, selecting necessary tools, and taking actions to\\ncomplete the task [215, 216, 217, 27]. A generic pipeline of\\ntool-augmented LLMs is shown in Figure 13, where different'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='tool-augmented LLMs is shown in Figure 13, where different\\nmodules in Figure 13 are selected in a loop until the task com-\\npletion.\\nZero-Shot Tool Augmentation: LLMs in-context learning and\\nreasoning abilities enable them to interact with tools with-\\nout training. Automatic reasoning and tool-use (ART) [217]\\nbuilds a task library with demonstrations of reasoning steps and\\ncalling external tools. It retrieves similar task examples and\\nprovides the context to the LLM for inference. Aside from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='provides the context to the LLM for inference. Aside from\\nthis, [218] shows tool documentation is enough to teach LLMs\\nto use tools without demonstrations. RestGPT [219] integrates\\nLLMs with RESTful APIs by decomposing tasks into planning\\nFigure 13: A basic flow diagram of tool augmented LLMs. Given an input and\\na set of available tools, the model generates a plan to complete the task. The\\ntool augmented LLMs utilize different modules iteratively, such as retriever,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='tool augmented LLMs utilize different modules iteratively, such as retriever,\\ntool execution, read-write to memory, feedback, etc., depending on the task.\\nand API selection steps. The API selector understands the API\\ndocumentation to select a suitable API for the task and plan the\\nexecution. ToolkenGPT [220] uses tools as tokens by concate-\\nnating tool embeddings with other token embeddings. During\\ninference, the LLM generates the tool tokens representing the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='inference, the LLM generates the tool tokens representing the\\ntool call, stops text generation, and restarts using the tool exe-\\ncution output.\\nTraining with Tool Augmentation: LLMs are trained to inter-\\nact with diverse tools, enhancing planning abilities to overcome\\nthe limitations of zero-shot tool augmentation [221, 27, 222,\\n223]. Gorilla [221] instruction-tunes LLaMA with information\\nretrieval from API documentation. It uses the self-instruct [19]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='retrieval from API documentation. It uses the self-instruct [19]\\ndata generation pipeline with GPT-4 by providing in-context\\nexamples retrieved from API documentation. Tool augmented\\nlanguage model (TALM) [27] fine-tunes T5 [10] for tool use\\nwith a self-play approach, where it iteratively completes tool\\nmanipulation tasks and includes them back in the training set.\\nToolLLM [223] collects 16k APIs from RapidAPI. It samples\\nAPIs from the list to generate an instruction-tuning dataset us-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='APIs from the list to generate an instruction-tuning dataset us-\\ning ChatGPT in single-tool and multi-tool scenarios. For high-\\nquality datasets, ToolLLM suggested a depth-first search-based\\ndecision tree (DFSDT) method to generate ground-truths with\\ndiverse reasoning and planning.\\nMultimodal Tool Augmentation: The compositional reasoning\\ncapacity of LLMs allows them to manipulate tools in multi-\\nmodal settings [215, 216, 224]. Following the pipeline shown'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 18}, page_content='modal settings [215, 216, 224]. Following the pipeline shown\\nin Figure 13, the LLM outlines a plan, generally executing in a\\nsequence: Plan →Tool selection →Execute →Inspect →\\nGenerate, to respond to the user query. Here, the database of\\ntools is rich in modalities, including text, images, etc. Many of\\nthe multimodal tool augmentation systems employ multimodal\\nLLMs [31, 225, 224, 216], while others utilize single modality\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='LLMs and generate a plan on using different modality tools to\\nsolve multimodal queries [226].\\n3.5. LLMs-Powered Agents\\nAI agents are autonomous entities, capable of planning,\\ndecision-making, and performing actions to achieve complex\\ngoals.\\nIn the early days, AI agents were rule-based, de-\\nsigned for narrow tasks, and had limited capabilities, such\\nas Clippy [227] and Deep Blue [228].\\nIn contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='In contrast to this,\\nLLMs abilities to respond to dynamic scenarios have made it\\npossible to incorporate them in diverse applications, includ-\\ning LLMs-powered agents [224, 216], where LLMs behave\\nas the brain of agents. LLMs have been incorporated in web\\nagents [166, 167], coding agents [229], tool agents [27, 223],\\nembodied agents [26], and conversational agents [195], requir-\\ning minimal to no fine-tuning\". Below we summarize the re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='ing minimal to no fine-tuning\". Below we summarize the re-\\nsearch in LLMs-based autonomous agents. For a more detailed\\ndiscussion, please refer to [230, 231].\\nLLMs Steering Autonomous Agents: LLMs are the cognitive\\ncontrollers of the autonomous agents. They generate plans, rea-\\nson about tasks, incorporate memory to complete tasks, and\\nadapt the outline depending on the feedback from the environ-\\nment. Depending on the acquired capabilities of LLMs, many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='ment. Depending on the acquired capabilities of LLMs, many\\nmethods fine-tune, propose a better prompting approach, or uti-\\nlize different modules to enhance agents’ performance. Mod-\\nules and strategies employed in autonomous agents are briefly\\ndiscussed below.\\nPlanning and Reasoning: Completing a complex task requires\\nhuman-like logical thinking, planning necessary steps, and\\nreasoning current and future directions. Prompting methods'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='reasoning current and future directions. Prompting methods\\nlike chain-of-thoughts [103], tree-of-thoughts [105], and self-\\nconsistency [104] are central to agents, eliciting LLMs to rea-\\nson its actions and choose among different paths for task com-\\npletion. When LLMs are prompted with a task description and\\na sequence of actions, they can accurately generate plan ac-\\ntions without any fine-tuning [232]. Reasoning via planning\\n(RAP) [233] incorporates a re-purposed LLM as a world model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='(RAP) [233] incorporates a re-purposed LLM as a world model\\nto reason about future outcomes and explore alternative paths\\nfor task completion. Retroformer [234] uses a retrospective\\nLLM to improve main LLM planning and reasoning capabil-\\nities by providing helpful task cues.\\nFeedback: LLMs in open-loop systems generate plans and as-\\nsume that the agent will complete them successfully. However,\\nthe actual scenario is different with failures and variable re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='the actual scenario is different with failures and variable re-\\nsponses from the environment. To correctly complete tasks,\\nmany methods use LLMs in a closed-loop where the action re-\\nsponse is provided as feedback to the LLMs to re-assess and\\nupdate the plan as required [235, 236, 237, 195]. Another di-\\nrection of research exploits LLMs as reward functions to train\\nreinforcement learning (RL) policies instead of humans [238].\\nMemory: LLMs can learn from the context provided in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='Memory: LLMs can learn from the context provided in the\\nprompt. In addition to internal memory, various systems em-\\nploy external memory to save the response history.\\nReflex-\\nion [195] maintains an episodic memory to use previous re-\\nsponses as feedback to improve future decision-making. Retro-\\nformer [234] improves its responses by employing short-term\\nand long-term memory, where short-term memory contains re-\\ncent responses and long-term memory keeps summarized failed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='cent responses and long-term memory keeps summarized failed\\nattempts to add in the prompt as reflection.\\nMulti-Agents Systems: LLMs can play user-defined roles and\\nbehave like a specific domain expert. In multi-agent systems,\\neach LLM is assigned a unique role, simulating human behav-\\nior and collaborating with other agents to complete a complex\\ntask [229, 239].\\nLLMs in Physical Environment:\\nLLMs are good at\\ninstruction-following, however, utilizing them for physically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='LLMs are good at\\ninstruction-following, however, utilizing them for physically\\ngrounded tasks requires adaptation, as they lack real-world\\nknowledge. This could lead to generating illogical responses\\nfor a particular physical situation [240, 26].\\nSayCan [240]\\nmake LLMs aware of the available low-level task operations.\\nLLM (Say) builds a high-level plan to complete the task and\\na learned affordance function (Can) explores the possibility of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='a learned affordance function (Can) explores the possibility of\\nexecuting the plan in the real world. SayCan uses RL to train\\nthe language-conditioned affordance function. PaLM-E enables\\nthe LLM to solve grounded tasks by training multi-modal LLM\\nfeeding inputs directly from the sensors.\\nManipulation: In the area of manipulation [236, 241], LLMs\\nenhance a robot’s dexterity and adaptability, excelling in tasks\\nlike object recognition, grasping, and collaboration. They ana-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='like object recognition, grasping, and collaboration. They ana-\\nlyze visual and spatial information to determine the most effec-\\ntive approach to interact with objects.\\nNavigation: LLMs enhance a robot’s ability to navigate com-\\nplex environments with precision and adaptability [242, 243,\\n244, 245]. They generate feasible paths and trajectories for\\nrobots, accounting for intricate environmental details [246].\\nThis ability is valuable in scenarios requiring precise and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='This ability is valuable in scenarios requiring precise and\\ndynamically adaptable navigation in environments like ware-\\nhouses, transport, healthcare facilities, and residences.\\n3.6. Efficient LLMs\\nDeploying LLMs in production is expensive. Reducing their\\nrunning costs while preserving performance is an appealing\\narea of research. This section summarizes the approaches sug-\\ngested to enhance LLMs’ efficiency.\\n3.6.1. Parameter Efficient Fine-Tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='gested to enhance LLMs’ efficiency.\\n3.6.1. Parameter Efficient Fine-Tuning\\nFine-tuning LLMs with tens or hundreds of billions of pa-\\nrameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\\n(540B), etc., is computationally intensive and time-consuming.\\nTo avoid complete model fine-tuning, numerous parameter-\\nefficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try\\nto achieve acceptable model fine-tuning performance at reduced\\ncosts. As compared to full fine-tuning [248], PEFT performs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='costs. As compared to full fine-tuning [248], PEFT performs\\nbetter in low-resource setups, achieves comparable perfor-\\nmance on medium-resource scenarios, and performs worse than\\nfull fine-tuning under high-resource availability. An overview\\nof different PEFT approaches is shown in Figure 14.\\nAdapter Tuning: Adds a few trainable parameters within the\\ntransformer block. The adapter layer is a sequence of feature\\ndownscaling, non-linearity, and upscaling [106]. Variants of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 19}, page_content='downscaling, non-linearity, and upscaling [106]. Variants of\\nadapter tuning inject adapter layers sequentially [106] and in\\nparallel [38], whereas the mixture of adapter (AdaMix) [249]\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\\nthe adapter tuning category.\\nemploys multiple adapter modules in a single layer. AdaMix\\nroutes input instances randomly to one of the multiple down-\\nscale and upscale modules. The mixture of adapters is averaged\\nout for inference to avoid additional latency. Low-Rank Adap-\\ntation (LoRA) [250] learns low-rank decomposed matrices to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='tation (LoRA) [250] learns low-rank decomposed matrices to\\nfreeze original weights. The learned weights are fused with the\\noriginal weights for inference, avoiding latency.\\nPrompt Tuning: Prompting is an effective way to adapt a\\npre-trained LLM for the downstream task. However, manual\\nprompts bring uncertainty in the model’s prediction, where a\\nchange in a single word drops the performance [247]. Prompt\\ntuning alleviates this problem by fine-tuning only 0.001%-3%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='tuning alleviates this problem by fine-tuning only 0.001%-3%\\nadditional parameters [251]. It concatenates trainable prompt\\nparameters with the model embeddings [247, 40, 251]. Task-\\nspecific fixed discrete prompts are concatenated with input em-\\nbeddings in [40]. As discrete prompts bring instability, prompts\\nare encoded through a learnable mapping in P-Tuning [247],\\nnaming continuous prompts, which are appended with the dis-\\ncrete prompts.\\nOnly the prompt encoder is trainable in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='crete prompts.\\nOnly the prompt encoder is trainable in the\\nmodel. In an extension of P-Tuning, continuous prompts are\\nconcatenated with each layer of the network in [251]. Progres-\\nsive prompts [252] avoid catastrophic forgetting and transfer\\npreviously learned knowledge by sequentially adding trainable\\nprompt embeddings to the previously frozen task embeddings.\\nPrefix Tuning: A set of trainable task-specific prefix vectors\\nare appended to the frozen transformer layers in prefix tun-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='are appended to the frozen transformer layers in prefix tun-\\ning [41]. The prefix vectors are virtual tokens attended by the\\ncontext tokens on the right. In addition, adaptive prefix tun-\\ning [253] applies a gating mechanism to control the information\\nfrom the prefix and actual tokens.\\nBias Tuning: Fine-tuning only bias terms in small to medium\\ntraining data has been found effective in BitFit [254].\\nThis\\nmethod achieves full fine-tuning performance for tasks with less'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='This\\nmethod achieves full fine-tuning performance for tasks with less\\ntraining data and comparable performance with more training\\ndata.\\n3.6.2. Quantization\\nLLMs require extensive computing and memory for infer-\\nence.\\nDeploying a 175B parameter GPT-3 model needs at\\nleast five 80GB A100 GPUs and 350GB of memory to store in\\nFP16 format [44]. Such demanding requirements for deploying\\nLLMs make it harder for smaller organizations to utilize them.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='LLMs make it harder for smaller organizations to utilize them.\\nModel compression is an effective solution but comes at the cost\\nof degraded performance, especially at large scales greater than\\n6B. These models exhibit very large magnitude outliers that do\\nnot exist in smaller models [255], making it challenging and re-\\nquiring specialized methods for quantizing LLMs [44, 256].\\nPost-Training Quantization: Minimal or no training is re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='Post-Training Quantization: Minimal or no training is re-\\nquired in this type of quantization, without significantly com-\\npromising the model performance. LLM-8-bit [255] uses full-\\nprecision matrix multiplication for weights associated with out-\\nlier features and 8-bit for remaining features. The lower pre-\\ncision multiplication outputs are converted to FP-16 and con-\\ncatenated with others. The quantized models have homogenous\\nword embeddings, which may degrade their performance. To'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='word embeddings, which may degrade their performance. To\\nfix this, token-level knowledge distillation is employed in [45]\\nalong with independent quantization scaling factors for each\\nmodule due to varying weight distribution. Feature distribu-\\ntions are asymmetric and appear in different channels; outlier\\nsuppression [257] shifts and scales per-channel activation dis-\\ntributions for effective quantization. SmoothQuant [44] quan-\\ntizes activations and weights to INT8 format by smoothing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='tizes activations and weights to INT8 format by smoothing\\nactivations and migrating the quantization difficulty toward\\nweights. It multiplies the inverse of the smoothing factor with\\nweights, which introduces a few outliers in the weights but is\\neasier to quantify than unsmoothed activations. OPTQ [256]\\nuses the optimal brain compression (OBC) [258] algorithm to\\nquantize the model layer-by-layer and update weights to com-\\npensate for quantization error.\\nTo improve speed and per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='pensate for quantization error.\\nTo improve speed and per-\\nformance, OPTQ updates weights in arbitrary order, employs\\nlazy updates, and uses better Cholesky kernels. Outlier-aware\\nweight quantization (OWQ) [259] uses the OPTQ algorithm for\\nquantization but assigns higher precision to vulnerable weights,\\ncausing outliers and lower precision for others.\\nQuantization-Aware Training:\\nTo compensate for perfor-\\nmance degradation,\\na quantized model is fine-tuned in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 20}, page_content='To compensate for perfor-\\nmance degradation,\\na quantized model is fine-tuned in\\nquantization-aware training (QAT) [260, 261, 262].\\nAl-\\npha Tuning quantizes the model using binary coding quan-\\ntization (BCQ) [263] and fine-tunes only quantization scal-\\ning factors.\\nThis approach improves performance over\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='parameter-efficient fine-tuning of the pre-trained model. Sim-\\nilarly, parameter-efficient and quantization-aware adaptation\\n(PEQA) [264] reduces the precision of fully-connected layers\\nand fine-tunes only quantization scaling parameters.\\nLLM-\\nQAT [262] generates training data from the pre-trained network\\nand trains a quantized student model with knowledge distilla-\\ntion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM\\nwith LoRA [250] using a 4-bit normal float, which shows better'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='with LoRA [250] using a 4-bit normal float, which shows better\\nperformance over a 4-bit integer and float.\\n3.6.3. Pruning\\nPruning is an alternative approach to quantization to com-\\npress model size, thereby reducing LLMs deployment costs\\nsignificantly. Compared to task-agnostic pruning, task-specific\\npruning is easily achievable with good performance, where a\\nmodel is fine-tuned on the downstream task and pruned for\\nfaster inference. It is possible to prune LLMs for individual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='faster inference. It is possible to prune LLMs for individual\\ntasks, but the cost of pruning and deploying task-specific mod-\\nels is high. To overcome this, many structured and unstructured\\npruning methods for LLMs have been proposed to maintain rea-\\nsonable performance across all tasks while shrinking the model\\nsize [265, 42, 266].\\nUnstructured Pruning: This kind of pruning removes less im-\\nportant weights without maintaining any structure.\\nExisting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='portant weights without maintaining any structure.\\nExisting\\nLLM pruning methods take advantage of the unique charac-\\nteristics of LLMs, uncommon for smaller models, where a\\nsmall subset of hidden states are activated with large magni-\\ntude [255]. Pruning by weights and activations (Wanda) [265]\\nprunes weights in every row based on importance, calculated\\nby multiplying the weights with the norm of input. The pruned\\nmodel does not require fine-tuning, thereby saving computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='model does not require fine-tuning, thereby saving computa-\\ntional costs. Outlier weighed layerwise sparsity (OWL) [267]\\nextends Wanda with non-uniform layer pruning. It shows that\\nthe number of outliers varies for different layers; therefore, the\\nmodel should have variable pruning ratios for better perfor-\\nmance for every layer. Contrastive pruning (CAP) [43] itera-\\ntively prunes the model by training the sparse model using con-\\ntrastive loss between pre-trained, fine-tuned, and snapshots of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='trastive loss between pre-trained, fine-tuned, and snapshots of\\nprevious sparse models to learn task-specific and task-agnostic\\nknowledge.\\nStructured Pruning: Here, the parameters are removed in\\ngroups, rows, columns, or matrices, which speeds up the\\ninference because of effective hardware tensor core utiliza-\\ntion [265].\\nLLM-Pruner [42] employs a 3-stage structured\\npruning strategy, identifying the groups of hidden states caus-\\ning each other to activate during the forward-pass, keeping im-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='ing each other to activate during the forward-pass, keeping im-\\nportant groups and removing less important ones, and fine-\\ntuning the pruned model with LoRA. Sparsity-induced mask\\nlearning (SIMPLE) [268] prunes the network using learnable\\nmasks. Similarly, another method prunes LLMs by learning\\nmasks and removing unimportant rank-1 components of the\\nfactorized weight matrix [266].\\n3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='3.7. Multimodal LLMs\\nInspired by the success of LLMs in natural language process-\\ning applications, an increasing number of research works are\\nnow facilitating LLMs to perceive different modalities of infor-\\nmation like image [269, 270, 271], video [272, 273, 274], au-\\ndio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present\\nsubstantial benefits compared to standard LLMs that process\\nonly text. By incorporating information from various modal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='only text. By incorporating information from various modal-\\nities, MLLMs can achieve a deeper understanding of context,\\nleading to more intelligent responses infused with a variety of\\nexpressions. Importantly, MLLMs align closely with human\\nperceptual experiences, leveraging the synergistic nature of our\\nmultisensory inputs to form a comprehensive understanding of\\nthe world [276, 26]. Coupled with a user-friendly interface,\\nMLLMs can offer intuitive, flexible, and adaptable interactions,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='MLLMs can offer intuitive, flexible, and adaptable interactions,\\nallowing users to engage with intelligent assistants through a\\nspectrum of input methods. According to the ways of construct-\\ning models, current MLLMs can be generally divided into three\\nstreams: pre-training, fine-tuning, and prompting. In this sec-\\ntion, we will discuss more details of these main streams, as well\\nas the important application of MLLMs in visual reasoning.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='as the important application of MLLMs in visual reasoning.\\nPre-training: This stream of MLLMs intends to support differ-\\nent modalities using unified end-to-end models. For instance,\\nFlamingo [269] applies gated cross-attention to fuse vision and\\nlanguage modalities, which are collected from pre-trained and\\nfrozen visual encoder and LLM, respectively. Moreover, BLIP-\\n2 [270] proposes a two-stage strategy to pre-train a Querying\\nTransformer (Q-Former) for the alignment between vision and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='Transformer (Q-Former) for the alignment between vision and\\nlanguage modalities: in the first stage, vision-language repre-\\nsentation learning is bootstrapped from a frozen visual encoder;\\nand in the second stage, a frozen LLM bootstraps vision-to-\\nlanguage generative learning for zero-shot image-to-text gen-\\neration. Similarly, MiniGPT-4 [277] deploys pre-trained and\\nfrozen ViT [278], Q-Former and Vicuna LLM [159], only train-\\ning the linear projection layer for vision and language modali-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='ing the linear projection layer for vision and language modali-\\nties alignment.\\nFine-tuning: Derived from instruction tuning [16] for NLP\\ntasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\\nusing multimodal instructions. Following this method, LLMs\\ncan be easily and effectively extended as multimodal chat-\\nbots [277, 271, 29] and multimodal task solvers [279, 30, 280].\\nThe key issue of this stream of MLLMs is to collect multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='The key issue of this stream of MLLMs is to collect multi-\\nmodal instruction-following data for fine-tuning [58]. To ad-\\ndress this issue, the solutions of benchmark adaptation [279,\\n281, 282], self-instruction [19, 31, 283], and hybrid composi-\\ntion [284, 280] are employed, respectively. To mitigate the gap\\nbetween the original language modality and additional modal-\\nities, the learnable interface is introduced to connect differ-\\nent modalities from frozen pre-trained models.\\nParticularly,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='ent modalities from frozen pre-trained models.\\nParticularly,\\nthe learnable interface is expected to work in a parameter-\\nefficient tuning manner: e.g., LLaMA-Adapter [285] applies\\nan efficient transformer-based adapter module for training,\\nand LaVIN [284] dynamically learns the multimodal feature\\nweights using a mixture-of-modality adapter. Different from\\nthe learnable interface, the expert models can directly convert\\nmultimodalities into language: e.g., VideoChat-Text [272] in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 21}, page_content='multimodalities into language: e.g., VideoChat-Text [272] in-\\ncorporates Whisper [286], a speech recognition expert model,\\nto generate the captions of given videos for the understanding\\nof following LLMs.\\nPrompting:\\nDifferent from the fine-tuning technique that\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='directly updates the model parameters given task-specific\\ndatasets, the prompting technique provides certain context, ex-\\namples, or instructions to the model, fulfilling specialized tasks\\nwithout changing the model parameters. Since prompting can\\nsignificantly reduce the need for large-scale multimodal data,\\nthis technique is widely used to construct MLLMs. Particularly,\\nto solve multimodal Chain of Thought (CoT) problems [103],\\nLLMs are prompted to generate both the reasoning process and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='LLMs are prompted to generate both the reasoning process and\\nthe answer given multimodal inputs [287]. On this front, differ-\\nent learning paradigms are exploited in practice: for example,\\nMultimodal-CoT [287] involves two stages of rationale genera-\\ntion and answer inference, where the input of the second stage\\nis a combination of the original input and the output of the first\\nstage; and CoT-PT [288] applies both prompt tuning and spe-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='stage; and CoT-PT [288] applies both prompt tuning and spe-\\ncific visual bias to generate a chain of reasoning implicitly. In\\naddition to CoT problems, LLMs can also be prompted with\\nmultimodal descriptions and tools, effectively dividing complex\\ntasks into sub-tasks [289, 290].\\nVisual Reasoning Application: Recent visual reasoning sys-\\ntems [291, 292, 216, 293] tend to apply LLMs for better visual\\ninformation analysis and visual-language integration. Differ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='information analysis and visual-language integration. Differ-\\nent from previous works [294, 295] that rely on limited VQA\\ndatasets and small-scale neural networks, current LLM-aided\\nmethods offer benefits of stronger generalization ability, emer-\\ngent ability, and interactivity [58]. To realize visual reasoning\\nwith the help of LLMs, prompting and fine-tuning techniques\\ncan also be utilized: for example, PointClip V2 [292] applies\\nLLMs to generate 3D-specific prompts, which are encoded as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='LLMs to generate 3D-specific prompts, which are encoded as\\ntextual features and then combined with visual features for\\n3D recognition; and GPT4Tools [31] employs LoRA [250] to\\nfine-tune LLMs following tool-related instructions.\\nServing\\nas a controller [293], decision maker [296], or semantics re-\\nfiner [291, 297], LLMs significantly facilitates the progress of\\nvisual reasoning research.\\n3.8. Summary and Discussion\\n3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='3.8.1. Architecture\\nDue to the gigantic scale of LLMs, minor changes in archi-\\ntecture and training strategies have a big impact on performance\\nand stability. Here, we summarize key architectural modules\\nused in various LLMs, leading to better performance, reduced\\ntraining time and memory, and better training stability.\\nLayer Normalization: The performance and training stability\\nof LLMs are affected significantly by layer normalization. Pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='of LLMs are affected significantly by layer normalization. Pre-\\nnorm, that is normalizing inputs rather than outputs, is more\\ncommon among LLMs stabilizing the training [6, 127, 108].\\nBLOOM [13] and AlexaTM [122] utilize an additional layer\\nnormalization before embedding layer to stabilize the training\\nof large-scale models, while the model’s zero-shot generaliza-\\ntion ability can be negatively impacted [13]. However, another\\nstudy [33] finds that pre-norm degrades fine-tuned model per-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='study [33] finds that pre-norm degrades fine-tuned model per-\\nformance as compared to post-norm, and there are no stability\\nbenefits of pre-norm beyond the 100B scale. Therefore, GLM-\\n130B [33] used deep-norm which is a variant of post-norm for\\nbetter downstream task performance after fine-tuning.\\nPositional Encoding: Like other building blocks of the model,\\npositional encoding also affects the performance and training\\nstability of LLMs.\\nBLOOM [13] finds ALiBi outperforms'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='stability of LLMs.\\nBLOOM [13] finds ALiBi outperforms\\nlearned and rotary positional encodings.\\nContrary to this,\\nGLM-130B [33] identifies rotary positional encoding as being\\nbetter than ALiBi. So, there is no conclusion in the literature\\nabout positional encodings yet.\\nParallel Attention: In this type of attention, feed-forward and\\nattention layers are parallel to each other rather than sequen-\\ntial in a transformer block. It has been shown to reduce train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='tial in a transformer block. It has been shown to reduce train-\\ning time by 15%. There is no evidence of performance drop\\ndue to this change in the literature and it is used by the models\\nPaLM [15], GPT-NeoX [118], and CodeGen [140].\\nMulti-Query Attention It has shared key and value attention\\nheads in a transformer block while query attention heads are\\nprojected as usual. This reduces memory usage and speeds up\\nsampling in autoregressive decoding. No performance degrada-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='sampling in autoregressive decoding. No performance degrada-\\ntion has been observed with this change and it makes the train-\\ning efficient allowing larger batch sizes. Multi-query attention\\nis used in [15, 142].\\nMixture of Experts: This type of architecture enables eas-\\nily scaling models to trillions of parameters [92, 91]. Only a\\nfew experts are activated during the computation making them\\ncompute-efficient. The performance of MoE models is better'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='compute-efficient. The performance of MoE models is better\\nthan dense models for the same amount of data and requires less\\ncomputation during fine-tuning to achieve performance similar\\nto dense models as discussed in [91]. MoE architectures are\\nless prone to catastrophic forgetting, therefore are more suited\\nfor continual learning [92]. Extracting smaller sub-models for\\ndownstream tasks is possible without losing any performance,\\nmaking MoE architecture hardware-friendly [92].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='making MoE architecture hardware-friendly [92].\\nSparse vs Dense Activated: GPT-3 [6] uses sparse transform-\\ners [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\\narchitectures to lower computational costs and increase the\\nmodel size and capacity. According to the literature, sparse\\nmodules do not degrade the model’s performance [67]. How-\\never, more experiments are required to verify this statement.\\n3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='3.8.2. Training Strategies\\nTraining models at a huge scale require tricks to reduce train-\\ning costs, avoid loss divergence, and achieve better perfor-\\nmance. We summarize and discuss some of these key tricks\\nused in different LLMs.\\nMixed Precision: It is a famous method for LLMs to reduce\\nmemory usage and improve training efficiency. In mixed pre-\\ncision, forward and backward passes are performed in FP16\\nformat whereas optimizer states and master weights are kept'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='format whereas optimizer states and master weights are kept\\nin FP32 format [120]. A drawback associated with this for-\\nmat change is training instability due to a smaller value range\\nresulting in loss spikes [33]. An alternative to FP16 is BF16\\nwhich has a comparatively larger range and performs precision-\\nsensitive operations like gradient accumulation and softmax in\\nFP32 [13]. BF16 has better performance and training stability\\nbut uses more memory and is supported on specific hardware,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 22}, page_content='but uses more memory and is supported on specific hardware,\\nfor example, A100 GPUs. Therefore, its adoption in LLMs is\\nlimited.\\nTraining Instability: Loss divergence or spiking is a common\\nissue in LLMs that occurs multiple times during training. This\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='happens in the presence of gradient clipping [15]. To mitigate\\nthis problem, many approaches suggest restarting training from\\nan earlier checkpoint [15, 33, 91], skipping 200-500 earlier\\ndata batches at the point of divergence in [15] and re-shuffling\\nbatches in [91]. The embedding layer gradient shrink proves to\\nfurther stabilize the training as its gradient norm is significantly\\nlarger than the other layers [33]. Another suggestion to improve'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='larger than the other layers [33]. Another suggestion to improve\\ntraining stability for larger models is not to use biases in dense\\nand norm layers as in [15].\\nWeight Initialization: It plays a significant role in model con-\\nvergence and training stability.\\nGPT-NeoX [118] initializes\\nfeed-forward layers before residuals with\\n2\\nL\\n√\\nd as in [153] and\\nother layers with the small initialization scheme [298]. This\\navoids activations growing exponentially with increasing depth.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='avoids activations growing exponentially with increasing depth.\\nMT-NLG [117] found higher variance for weight initialization\\nleads to unstable training, hence validating small initialization\\nscheme [298]. Various models perform random weight initial-\\nization which can cause bad initialization, Galactica [148] sug-\\ngests a longer warmup to negate the effect.\\nLearning Rate: A suitable learning rate is important for sta-\\nble training. It is suggested to use a lower value [13, 15, 124]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='ble training. It is suggested to use a lower value [13, 15, 124]\\nwith warmup and decay (cosine or linear). Usually, the learn-\\ning rate is within the range 1e−4 to 8e−4. Moreover, MT-NLG\\n(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\\ning learning rates based on the model size using the GPT-3 [6]\\nmodels ranging between 13B and 175B. This avoids tuning the\\nlearning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='learning rate hyperparameter.\\nTraining Parallelism: 3D parallelism, a combination of data,\\npipeline, and tensor parallelism, is the most utilized training\\nparallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\\nIn addition to 3D parallelism, BLOOM [13] uses a zero op-\\ntimizer [37] to shard optimizer states.\\nPanGu-α [108] and\\nPanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\\nlelism which additionally contains optimizer parallelism and\\nrematerialization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='lelism which additionally contains optimizer parallelism and\\nrematerialization.\\nMode Switching: It adds task-related tokens at the beginning\\nof the text during training. These tokens refer to the natural\\nlanguage understanding and natural language generation tasks\\nwhich are shown to improve downstream task performance\\nin [125, 124, 122]. During fine-tuning and inference, tokens\\nare appended based on the downstream tasks.\\nControllable Text Generation: Generating credible and con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='Controllable Text Generation: Generating credible and con-\\ntrolled text from a pre-trained model is challenging. GPT-3 [6]\\nand other LLMs use in-context learning to control generated\\ntext. While in-context learning helps in controlling the gener-\\nated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\\nto rank its generated text for credibility and soft prompts such as\\ngenre, topic, keywords, sentiment, and length for better control\\non generated text.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='genre, topic, keywords, sentiment, and length for better control\\non generated text.\\n3.8.3. Supervised Models vs Generalized Models\\nAlthough generalized models are capable of performing di-\\nverse tasks with good performance they have not yet outper-\\nformed models trained in supervised settings. The supervised\\ntrained models are still state-of-the-art in various NLP tasks by\\na large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='a large margin as shown in [6, 15, 18].\\n3.8.4. Zero-Shot vs Few-Shot\\nLLMs perform well in zero-shot and few-shot settings. But\\nthe performance difference between zero-shot and few-shot is\\nlarge for pre-trained models [6, 15], naming LLMs as meta-\\nlearners [6]. LLMs zero-shot evaluations underperform unsu-\\npervised methods in neural machine translation [6]. The liter-\\nature shows pre-training is not enough for good zero-shot per-\\nformance [15, 16]. To improve the zero-shot performance the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='formance [15, 16]. To improve the zero-shot performance the\\nliterature suggests using instruction fine-tuning that improves\\nthe zero-shot performance significantly and outperforms base-\\nlines. Instruction fine-tuning has also been shown to improve\\nzero-shot generalization to unseen tasks. Another model, Flan-\\nPaLM [16], unlocks zero-shot reasoning with CoT training.\\n3.8.5. Encoder vs Decoder vs Encoder-Decoder\\nTraditionally, these architectures perform well for different'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='Traditionally, these architectures perform well for different\\ntasks, for example, encoder-only for NLU tasks, decoder-only\\nfor NLG, and encoder-decoder for sequence2sequence model-\\ning. Encoder-only models are famous for smaller models such\\nas Bert [7], RoBERTa [299], etc., whereas LLMs are either\\ndecoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\\nWhile decoder-only models are good at NLG tasks, various\\nLLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\\nLLaMA [156], are decoder-only models with significant per-\\nformance gains on both NLU and NLG tasks. In contradic-\\ntion to this, T5 [10] and UL2 [125] identify encoder-decoder\\nmodels out-performing decoder-only models. In another study,\\nPaLM [15] finds increasing the size of decoder-only models\\ncan reduce the performance gap between decoder-only and\\nencoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='encoder-decoder architectures.\\nAlthough decoder-only architectures have become a trend for\\nLLMs, many recently proposed approaches [125, 122] use\\nmode-switching tokens in text with encoder-decoder architec-\\ntures to enable task-specific modes. Similarly, CodeT5+ [34]\\nuses an encoder-decoder architecture with multiple training ob-\\njectives for different tasks, activating the encoder, decoder, or\\nboth according to the tasks. These variations in architecture'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='both according to the tasks. These variations in architecture\\nand training objectives allow a model to perform well in differ-\\nent settings. Because of this dynamic configuration, the future\\nof LLMs can be attributed to encoder-decoder architectures.\\n4. Model Configurations\\nWe provide different statistics of pre-trained and instruction-\\ntuned models in this section. This includes information such as\\npublication venue, license type, model creators, steps trained,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='publication venue, license type, model creators, steps trained,\\nparallelism, etc in Table 3 and Table 4. Architecture details\\nof pre-trained LLMs are available in Table 5. Providing these\\ndetails for instruction-tuned models is unnecessary because it\\nfine-tunes pre-trained models for instruction datasets. Hence,\\narchitectural details are the same as the baselines. Moreover,\\noptimization settings for various LLMs are available in Table 6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 23}, page_content='optimization settings for various LLMs are available in Table 6\\nand Table 7. We do not include details on precision, warmup,\\nand weight decay in Table 7. These details are not as important\\nas others to mention for instruction-tuned models, and are not\\nprovided by the papers.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='Table 3: Summary of pre-trained LLMs (>10B). Only the LLMs discussed individually in the previous sections are summarized. “Data/Tokens” is the model’s\\npre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\\n(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\\nre-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\\n(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"“DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\\nModels\\nPublication\\nVenue\\nLicense\\nType\\nModel\\nCreators Purpose\\nNo. of\\nParams\\nCommercial\\nUse\\nSteps\\nTrained\\nData/\\nTokens\\nData\\nCleaning\\nNo. of\\nProcessing Units\\nProcessing\\nUnit Type\\nTraining\\nTime\\nCalculated\\nTrain. Cost\\nTraining\\nParallelism\\nLibrary\\nT5 [10]\\nJMLR'20\\nApache-2.0\\nGoogle\\nGeneral\\n11B\\n✓\\n1M\\n1T\\nHeur+Dedup\\n1024\\nTPU v3\\n-\\n-\\nD+M\\nMesh TensorFlow\\nGPT-3 [6]\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Apache-2.0\\nGoogle\\nGeneral\\n11B\\n✓\\n1M\\n1T\\nHeur+Dedup\\n1024\\nTPU v3\\n-\\n-\\nD+M\\nMesh TensorFlow\\nGPT-3 [6]\\nNeurIPS'20\\n-\\nOpenAI\\nGeneral\\n175B\\n×\\n-\\n300B\\nDedup+QF\\n-\\nV100\\n-\\n-\\nM\\n-\\nmT5 [11]\\nNAACL'21\\nApache-2.0\\nGoogle\\nGeneral\\n13B\\n✓\\n1M\\n1T\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nPanGu-α [108]\\narXiv'21\\nApache-2.0\\nHuawei\\nGeneral\\n200B\\n✓\\n260k\\n1.1TB\\nHeur+Dedup\\n2048\\nAscend 910\\n-\\n-\\nD+OP+P+O+R\\nMindSpore\\nCPM-2 [12]\\nAI Open'21\\nMIT\\nTsinghua\\nGeneral\\n198B\\n✓\\n1M\\n2.6TB\\nDedup\\n-\\n-\\n-\\n-\\nD+M\\nJAXFormer\\nCodex [141]\\narXiv'21\\n-\\nOpenAI\\nCoding\\n12B\\n×\\n-\\n100B\\nHeur\\n-\\n-\\n-\\n-\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"1M\\n2.6TB\\nDedup\\n-\\n-\\n-\\n-\\nD+M\\nJAXFormer\\nCodex [141]\\narXiv'21\\n-\\nOpenAI\\nCoding\\n12B\\n×\\n-\\n100B\\nHeur\\n-\\n-\\n-\\n-\\n-\\n-\\nERNIE 3.0 [110]\\narXiv'21\\n-\\nBaidu\\nGeneral\\n10B\\n×\\n120k∗\\n375B\\nHeur+Dedup\\n384\\nV100\\n-\\n-\\nM∗\\nPaddlePaddle\\nJurassic-1 [112]\\nWhite-Paper'21 Apache-2.0\\nAI21\\nGeneral\\n178B\\n✓\\n-\\n300B\\n-\\n800\\nGPU\\n-\\n-\\nD+M+P\\nMegatron+DS\\nHyperCLOVA [114]\\nEMNLP'21\\n-\\nNaver\\nGeneral\\n82B\\n×\\n-\\n300B\\nClf+Dedup+PF\\n1024\\nA100\\n321h\\n1.32 Mil\\nM\\nMegatron\\nYuan 1.0 [115]\\narXiv'21\\nApache-2.0\\n-\\nGeneral\\n245B\\n✓\\n26k∗\\n180B Heur+Clf+Dedup\\n2128\\nGPU\\n-\\n-\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Megatron\\nYuan 1.0 [115]\\narXiv'21\\nApache-2.0\\n-\\nGeneral\\n245B\\n✓\\n26k∗\\n180B Heur+Clf+Dedup\\n2128\\nGPU\\n-\\n-\\nD+T+P\\n-\\nGopher [116]\\narXiv'21\\n-\\nGoogle\\nGeneral\\n280B\\n×\\n-\\n300B\\nQF+Dedup\\n4096\\nTPU v3\\n920h\\n13.19 Mil\\nD+M\\nJAX+Haiku\\nERNIE 3.0 Titan [35]\\narXiv'21\\n-\\nBaidu\\nGeneral\\n260B\\n×\\n-\\n300B\\nHeur+Dedup\\n-\\nAscend 910\\n-\\n-\\nD+M+P+D*\\nPaddlePaddle\\nGPT-NeoX-20B [118] BigScience'22\\nApache-2.0\\nEleutherAI\\nGeneral\\n20B\\n✓\\n150k\\n825GB\\nNone\\n96\\n40G A100\\n-\\n-\\nM\\nMegatron+DS+PyTorch\\nOPT [14]\\narXiv'22\\nMIT\\nMeta\\nGeneral\\n175B\\n✓\\n150k\\n180B\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"None\\n96\\n40G A100\\n-\\n-\\nM\\nMegatron+DS+PyTorch\\nOPT [14]\\narXiv'22\\nMIT\\nMeta\\nGeneral\\n175B\\n✓\\n150k\\n180B\\nDedup\\n992\\n80G A100\\n-\\n-\\nD+T\\nMegatron\\nBLOOM [13]\\narXiv'22\\nRAIL-1.0\\nBigScience\\nGeneral\\n176B\\n✓\\n-\\n366B\\nDedup+PR\\n384\\n80G A100\\n2520h\\n3.87 Mil\\nD+T+P\\nMegatron+DS\\nGalactica [148]\\narXiv'22\\nApache-2.0\\nMeta\\nScience\\n120B\\n×\\n225k\\n106B\\nDedup\\n128\\n80GB A100\\n-\\n-\\n-\\nMetaseq\\nGLaM [91]\\nICML'22\\n-\\nGoogle\\nGeneral\\n1.2T\\n×\\n600k∗\\n600B\\nClf\\n1024\\nTPU v4\\n-\\n-\\nM\\nGSPMD\\nLaMDA [150]\\narXiv'22\\n-\\nGoogle\\nDialog\\n137B\\n×\\n3M\\n2.81T\\nFiltered\\n1024\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"600B\\nClf\\n1024\\nTPU v4\\n-\\n-\\nM\\nGSPMD\\nLaMDA [150]\\narXiv'22\\n-\\nGoogle\\nDialog\\n137B\\n×\\n3M\\n2.81T\\nFiltered\\n1024\\nTPU v3\\n1384h\\n4.96 Mil\\nD+M\\nLingvo\\nMT-NLG [117]\\narXiv'22\\nApache-v2.0 MS.+Nvidia General\\n530B\\n×\\n-\\n270B\\n-\\n4480\\n80G A100\\n-\\n-\\nD+T+P\\nMegatron+DS\\nAlphaCode [142]\\nScience'22\\nApache-v2.0\\nGoogle\\nCoding\\n41B\\n✓\\n205k\\n967B\\nHeur+Dedup\\n-\\nTPU v4\\n-\\n-\\nM\\nJAX+Haiku\\nChinchilla [96]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n70B\\n×\\n-\\n1.4T\\nQF+Dedup\\n-\\nTPUv4\\n-\\n-\\n-\\nJAX+Haiku\\nPaLM [15]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n255k\\n780B\\nHeur\\n6144\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"QF+Dedup\\n-\\nTPUv4\\n-\\n-\\n-\\nJAX+Haiku\\nPaLM [15]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n255k\\n780B\\nHeur\\n6144\\nTPU v4\\n-\\n-\\nD+M\\nJAX+T5X\\nAlexaTM [122]\\narXiv'22\\nApache v2.0\\nAmazon\\nGeneral\\n20B\\n×\\n500k\\n1.1T\\nFiltered\\n128\\nA100\\n2880h\\n1.47 Mil\\nM\\nDS\\nU-PaLM [124]\\narXiv'22\\n-\\nGoogle\\nGeneral\\n540B\\n×\\n20k\\n-\\n-\\n512\\nTPU v4\\n120h\\n0.25 Mil\\n-\\n-\\nUL2 [125]\\nICLR'23\\nApache-2.0\\nGoogle\\nGeneral\\n20B\\n✓\\n2M\\n1T\\n-\\n512\\nTPU v4\\n-\\n-\\nM\\nJAX+T5X\\nGLM [33]\\nICLR'23\\nApache-2.0\\nMultiple\\nGeneral\\n130B\\n×\\n-\\n400B\\n-\\n768\\n40G A100\\n1440h\\n3.37 Mil\\nM\\n-\\nCodeGen [140]\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"ICLR'23\\nApache-2.0\\nMultiple\\nGeneral\\n130B\\n×\\n-\\n400B\\n-\\n768\\n40G A100\\n1440h\\n3.37 Mil\\nM\\n-\\nCodeGen [140]\\nICLR'23\\nApache-2.0\\nSalesforce\\nCoding\\n16B\\n✓\\n650k\\n577B\\nHeur+Dedup\\n-\\nTPU v4\\n-\\n-\\nD+M\\nJAXFormer\\nLLaMA [127]\\narXiv'23\\n-\\nMeta\\nGeneral\\n65B\\n×\\n350k\\n1.4T Clf+Heur+Dedup\\n2048\\n80G A100\\n504h\\n4.12 Mil\\nD+M\\nxFormers\\nPanGuΣ [92]\\narXiv'23\\n-\\nHuawei\\nGeneral 1.085T\\n×\\n-\\n329B\\n-\\n512\\nAscend 910\\n2400h\\n-\\nD+OP+P+O+R\\nMindSpore\\nBloombergGPT [151]\\narXiv23\\n-\\nBloomberg\\nFinance\\n50B\\n×\\n139k\\n569B\\nDedup\\n512\\n40G A100\\n1272h\\n1.97 Mil\\nM\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"BloombergGPT [151]\\narXiv23\\n-\\nBloomberg\\nFinance\\n50B\\n×\\n139k\\n569B\\nDedup\\n512\\n40G A100\\n1272h\\n1.97 Mil\\nM\\nPyTorch\\nXuan Yuan 2.0 [152]\\narXiv23\\nRAIL-1.0\\nDu Xiaoman Finance\\n176B\\n✓\\n-\\n366B\\nFiltered\\n-\\n80GB A100\\n-\\n-\\nP\\nDS\\nCodeT5+ [34]\\narXiv'23\\nBSD-3\\nSalesforce\\nCoding\\n16B\\n✓\\n110k\\n51.5B\\nDedup\\n16\\n40G A100\\n-\\n-\\n-\\nDS\\nStarCoder [147]\\narXiv'23\\nOpenRAIL-M BigCode\\nCoding\\n15.5B\\n✓\\n250k\\n1T\\nDedup+QF+PF\\n512\\n80G A100\\n624h\\n1.28 Mil\\nD+T+P\\nMegatron-LM\\nLLaMA-2 [21]\\narXiv'23\\nLLaMA-2.0\\nMeta\\nGeneral\\n70B\\n✓\\n500k\\n2T\\nMinimal Filtering\\n-\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"D+T+P\\nMegatron-LM\\nLLaMA-2 [21]\\narXiv'23\\nLLaMA-2.0\\nMeta\\nGeneral\\n70B\\n✓\\n500k\\n2T\\nMinimal Filtering\\n-\\n80G A100\\n1.7Mh\\n-\\n-\\n-\\nPaLM-2 [123]\\narXiv'23\\n-\\nGoogle\\nGeneral\\n-\\n×\\n-\\n-\\nDdedup+PF+QF\\n-\\n-\\n-\\n-\\n-\\n-\\nLLaMA-3.1 [130]\\narXiv'24\\nLLaMA-3.0\\nMeta\\nGeneral\\n405B\\n✓\\n1.2M\\n15T\\nDedup+QF\\n16k\\n80G H100 30.84Mh\\n-\\nD+T+P+C\\nPyTorch\\nMixtral 8x22B [131]\\nweb'24\\nApache-2.0\\nMistral AI\\nGeneral\\n141B\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSnowflake Arctic [132] web'24\\nApache-2.0\\nSnowflake\\nGeneral\\n480B\\n✓\\n-\\n3.5T\\n-\\n-\\n-\\n-\\nT+P\\nDS\\nNemotron-4 340B [137]web'24\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"Apache-2.0\\nSnowflake\\nGeneral\\n480B\\n✓\\n-\\n3.5T\\n-\\n-\\n-\\n-\\nT+P\\nDS\\nNemotron-4 340B [137]web'24\\nNvidia\\nNvidia\\nGeneral\\n340B\\n✓\\n-\\n9T\\n-\\n6144\\n80G H100\\n-\\n-\\nD+T+P\\n-\\nDeepSeek [138]\\narXiv'24\\nMIT\\nDeepSeek\\nGeneral\\n67B\\n✓\\n-\\n2T\\nDedup+QF\\n-\\n-\\n300.6Kh\\n-\\nD+T+P\\nDS\\nDeepSeek-v2 [139]\\narXiv'24\\nMIT\\nDeepSeek\\nGeneral\\n67B\\n✓\\n-\\n8.1T\\nQF\\n-\\nH800\\n172.8Kh\\n-\\nD+P\\nHAI-LLM\\nTable 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"of training samples.\\nModels\\nPublication\\nVenue\\nLicense\\nType\\nModel\\nCreators\\nPurpose\\nNo. of\\nParams\\nCommercial\\nUse\\nPre-trained\\nModels\\nSteps\\nTrained\\nData/\\nTokens\\nNo. of\\nProcessing Units\\nProcessing\\nUnit Type\\nTrain.\\nTime\\nCalculated\\nTrain. Cost\\nTrain.\\nParallelism\\nLibrary\\nWebGPT [166]\\narXiv'21\\n-\\nOpenAI\\nGeneral\\n175B\\n×\\nGPT-3\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nT0 [17]\\nICLR'22\\nApache-2.0\\nBigScience\\nGeneral\\n11B\\n✓\\nT5\\n-\\n250B\\n512\\nTPU v3\\n270h\\n0.48 Mil\\n-\\n-\\nTk-Instruct [18]\\nEMNLP'22\\nMIT\\nAI2+\\nGeneral\\n11B\\n✓\\nT5\\n1000\\n-\\n256\\nTPU v3\\n4h\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"TPU v3\\n270h\\n0.48 Mil\\n-\\n-\\nTk-Instruct [18]\\nEMNLP'22\\nMIT\\nAI2+\\nGeneral\\n11B\\n✓\\nT5\\n1000\\n-\\n256\\nTPU v3\\n4h\\n0.0036 Mil\\n-\\nGoogle T5\\nOPT-IML [97]\\narXiv'22\\n-\\nMeta\\nGeneral\\n175B\\n×\\nOPT\\n8k\\n2B\\n128\\n40G A100\\n-\\n-\\nD+T\\nMegatron\\nFlan-U-PaLM [16] ICLR'22\\nApache-2.0\\nGoogle\\nGeneral\\n540B\\n✓\\nU-PaLM\\n30k\\n-\\n512\\nTPU v4\\n-\\n-\\n-\\nJAX+T5X\\nmT0 [154]\\nACL'23\\nApache-2.0 HuggingFace+ General\\n13B\\n✓\\nmT5\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nSparrow [167]\\narXiv'22\\n-\\nGoogle\\nDialog\\n70B\\n×\\nChinchilla\\n-\\n-\\n64\\nTPU v3\\n-\\n-\\nM\\n-\\nWizardCoder [164] arXiv'23\\nApache-2.0\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"-\\nGoogle\\nDialog\\n70B\\n×\\nChinchilla\\n-\\n-\\n64\\nTPU v3\\n-\\n-\\nM\\n-\\nWizardCoder [164] arXiv'23\\nApache-2.0\\nHK Bapt.\\nCoding\\n15B\\n×\\nStarCoder\\n200\\nS-78k\\n-\\n-\\n-\\n-\\n-\\n-\\nAlpaca [158]\\nGithub'23\\nApache-2.0\\nStanford\\nGeneral\\n13B\\n✓\\nLLaMA\\n3-Epoch\\nS-52k\\n8\\n80G A100\\n3h\\n600\\nFSDP\\nPyTorch\\nVicuna [159]\\nGithub'23\\nApache-2.0\\nLMSYS\\nGeneral\\n13B\\n✓\\nLLaMA\\n3-Epoch S-125k\\n-\\n-\\n-\\n-\\nFSDP\\nPyTorch\\nLIMA [185]\\narXiv'23\\n-\\nMeta+\\nGeneral\\n65B\\n-\\nLLaMA\\n15-Epoch S-1000\\n-\\n-\\n-\\n-\\n-\\n-\\nKoala [300]\\nGithub'23\\nApache-2.0\\nUC-Berkley\\nGeneral\\n13B\\n×\\nLLaMA\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content=\"-\\nLLaMA\\n15-Epoch S-1000\\n-\\n-\\n-\\n-\\n-\\n-\\nKoala [300]\\nGithub'23\\nApache-2.0\\nUC-Berkley\\nGeneral\\n13B\\n×\\nLLaMA\\n2-Epoch S-472k\\n8\\nA100\\n6h\\n100\\n-\\nJAX/FLAX\\n5. Datasets and Evaluation\\nGenerating training and evaluation datasets is expensive be-\\ncause of the large-scale data demand of LLMs. Hence, datasets\\nfor training and benchmarking these models are topics of key\\nimportance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='importance. A summary of datasets commonly used by LLMs\\nis provided next.\\n5.1. Training Datasets\\nThe performance of LLMs largely depends on the training\\ndata’s quality, size, and diversity. Preparing training datasets\\nof high quality at a large scale is laborious. Researchers have\\nsuggested various pre-training and fine-tuning datasets to en-\\nhance LLMs capabilities. We summarize these efforts in Ta-\\nble 8. While numerous training datasets are available in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 24}, page_content='ble 8. While numerous training datasets are available in the\\nliterature, we cover the most widely used ones in our summary.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\\nsize of hidden states.\\nModels\\nType\\nTraining\\nObjective\\nAttention\\nVocab\\nTokenizer\\nNorm\\nPE\\nActivation\\nBias\\nnL\\nnH\\nHS\\nT5 (11B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n×\\n24\\n128\\n1024\\nGPT3 (175B)\\nCausal-Dec\\nNext Token\\nDense+Sparse\\n-\\n-\\nLayer\\nLearned\\nGeLU\\n✓\\n96\\n96\\n12288\\nmT5 (13B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Dense+Sparse\\n-\\n-\\nLayer\\nLearned\\nGeLU\\n✓\\n96\\n96\\n12288\\nmT5 (13B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n-\\n-\\n-\\n-\\nPanGu-α (200B)\\nCausal-Dec\\nNext Token\\nStandard\\n40k\\nBPE\\nLayer\\n-\\n-\\n-\\n64\\n128\\n16384\\nCPM-2 (198B)\\nEnc-Dec\\nSpan Corruption\\nStandard\\n250k\\nSentencePiece\\nPre-RMS\\nRelative\\nReLU\\n-\\n24\\n64\\n-\\nCodex (12B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE+\\nPre-Layer\\nLearned\\nGeLU\\n-\\n96\\n96\\n12288\\nERNIE 3.0 (10B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n64'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='12288\\nERNIE 3.0 (10B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n64\\n4096\\nJurassic-1 (178B)\\nCausal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece∗\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n76\\n96\\n13824\\nHyperCLOVA (82B)\\nCausal-Dec\\nNext Token\\nDense+Sparse\\n-\\nBPE*\\nPre-Layer\\nLearned\\nGeLU\\n-\\n64\\n80\\n10240\\nYuan 1.0 (245B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\n-\\n-\\n-\\n-\\n-\\n76\\n-\\n16384\\nGopher (280B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n128\\n16384\\nERNIE 3.0 Titan (260B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Next Token\\nStandard\\n32k\\nSentencePiece\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n128\\n16384\\nERNIE 3.0 Titan (260B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nWordPiece\\nPost-Layer\\nRelative\\nGeLU\\n-\\n48\\n192\\n12288\\nGPT-NeoX-20B\\nCausal-Dec\\nNext Token\\nParallel\\n50k\\nBPE\\nLayer\\nRotary\\nGeLU\\n✓\\n44\\n64\\n-\\nOPT (175B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE\\n-\\n-\\nReLU\\n✓\\n96\\n96\\n-\\nBLOOM (176B)\\nCausal-Dec\\nNext Token\\nStandard\\n250k\\nBPE\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n112\\n14336\\nGalactica (120B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE+custom\\nLayer\\nLearned\\nGeLU\\n×\\n96'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='70\\n112\\n14336\\nGalactica (120B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE+custom\\nLayer\\nLearned\\nGeLU\\n×\\n96\\n80\\n10240\\nGLaM (1.2T)\\nMoE-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\nLayer\\nRelative\\nGeLU\\n✓\\n64\\n128\\n32768\\nLaMDA (137B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nLayer\\nRelative\\nGeGLU\\n-\\n64\\n128\\n8192\\nMT-NLG (530B)\\nCausal-Dec\\nNext Token\\nStandard\\n50k\\nBPE\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n105\\n128\\n20480\\nAlphaCode (41B)\\nEnc-Dec\\nNext Token\\nMulti-query\\n8k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n128\\n6144\\nChinchilla (70B)\\nCausal-Dec'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Enc-Dec\\nNext Token\\nMulti-query\\n8k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n128\\n6144\\nChinchilla (70B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n80\\n64\\n8192\\nPaLM (540B)\\nCausal-Dec\\nNext Token\\nParallel+Multi-query\\n256k\\nSentencePiece\\nLayer\\nRoPE\\nSwiGLU\\n×\\n118\\n48\\n18432\\nAlexaTM (20B)\\nEnc-Dec\\nDenoising\\nStandard\\n150k\\nSentencePiece\\nPre-Layer\\nLearned\\nGeLU\\n✓\\n78\\n32\\n4096\\nSparrow (70B)\\nCausal-Dec\\nPref.&Rule RM\\n-\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n16∗\\n64\\n8192\\nU-PaLM (540B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Causal-Dec\\nPref.&Rule RM\\n-\\n32k\\nSentencePiece-NFKC\\nPre-RMS\\nRelative\\nGeLU\\n✓\\n16∗\\n64\\n8192\\nU-PaLM (540B)\\nNon-Causal-Dec\\nMoD\\nParallel+Multi-query\\n256k\\nSentencePiece\\nLayer\\nRoPE\\nSwiGLU\\n×\\n118\\n48\\n18432\\nUL2 (20B)\\nEnc-Dec\\nMoD\\nStandard\\n32k\\nSentencePiece\\n-\\n-\\n-\\n-\\n64\\n16\\n4096\\nGLM (130B)\\nNon-Causal-Dec\\nAR Blank Infilling\\nStandard\\n130k\\nSentencePiece\\nDeep\\nRoPE\\nGeGLU\\n✓\\n70\\n96\\n12288\\nCodeGen (16B)\\nCausal-Dec\\nNext Token\\nParallel\\n-\\nBPE\\nLayer\\nRoPE\\n-\\n-\\n34\\n24\\n-\\nLLaMA (65B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nPre-RMS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Parallel\\n-\\nBPE\\nLayer\\nRoPE\\n-\\n-\\n34\\n24\\n-\\nLLaMA (65B)\\nCausal-Dec\\nNext Token\\nStandard\\n32k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n80\\n64\\n8192\\nPanGu-Σ (1085B)\\nCausal-Dec\\nNext Token\\nStandard\\n-\\nBPE\\nFused Layer\\n-\\nFastGeLU\\n-\\n40\\n40\\n5120\\nBloombergGPT (50B)\\nCausal-Dec\\nNext Token\\nStandard\\n131k\\nUnigram\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n40\\n7680\\nXuan Yuan 2.0 (176B)\\nCausal-Dec\\nNext Token\\nSelf\\n250k\\nBPE\\nLayer\\nALiBi\\nGeLU\\n✓\\n70\\n112\\n14336\\nCodeT5+ (16B)\\nEnc-Dec\\nSC+NT+Cont.+Match\\nStandard\\n-\\nCode-Specific\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nStarCoder (15.5B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='CodeT5+ (16B)\\nEnc-Dec\\nSC+NT+Cont.+Match\\nStandard\\n-\\nCode-Specific\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nStarCoder (15.5B)\\nCausal-Dec\\nFIM\\nMulti-query\\n49k\\nBPE\\n-\\nLearned\\n-\\n-\\n40\\n48\\n6144\\nLLaMA-2 (70B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n32k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLUE\\n-\\n-\\n-\\n-\\nPaLM-2\\n-\\nMoD\\nParallel\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nLLaMA-3.1 (405B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n128k\\nBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n126\\n128\\n16384\\nNemotron-4 (340B)\\nCausal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\n-\\nRoPE\\nReLU\\n×\\n96\\n96\\n18432\\nDeepSeek (67B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='Causal-Dec\\nNext Token\\nStandard\\n256k\\nSentencePiece\\n-\\nRoPE\\nReLU\\n×\\n96\\n96\\n18432\\nDeepSeek (67B)\\nCausal-Dec\\nNext Token\\nGrouped-query\\n100k\\nBBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n95\\n64\\n8192\\nDeepSeek-v2 (67B)\\nMoE-Dec\\nNext Token\\nMulti-Head Latent\\n100k\\nBBPE\\nPre-RMS\\nRoPE\\nSwiGLU\\n-\\n60\\n128\\n5120\\n5.2. Evaluation Datasets and Tasks\\nThe evaluation of LLMs is important in gauging their profi-\\nciency and limitations. This process measures the model’s abil-\\nity to comprehend, generate, and interact with human language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='ity to comprehend, generate, and interact with human language\\nacross a spectrum of tasks. Evaluating a language model (LM)\\nis divided into two broader categories: 1) natural language un-\\nderstanding (NLU) and 2) natural language generation (NLG).\\nIt is emphasized that tasks in NLU and NLG are softly catego-\\nrized and are often used interchangeably in the literature.\\nNatural Language Understanding: It measures the language\\nunderstanding capacity of LMs. It encompasses multiple tasks,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='understanding capacity of LMs. It encompasses multiple tasks,\\nincluding sentiment analysis, text classification, natural lan-\\nguage inference (NLI), question answering (QA), common-\\nsense reasoning (CR), mathematical reasoning (MR), reading\\ncomprehension (RC), etc.\\nNatural Language Generation: It assesses the language gener-\\nation capabilities of LLMs by understanding the provided input\\ncontext. It includes tasks such as summarization, sentence com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='context. It includes tasks such as summarization, sentence com-\\npletion, machine translation (MT), dialogue generation, etc.\\nNumerous datasets are proposed for each task, evaluating\\nLLMs against different characteristics. To provide an overview\\nof evaluation datasets, we briefly discuss a few famous datasets\\nwithin each category and offer a comprehensive list of datasets\\nin Table 9. Moreover, we show a detailed overview of the train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='in Table 9. Moreover, we show a detailed overview of the train-\\ning datasets and evaluation tasks and benchmarks used by vari-\\nous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\\nble 11. We also compare the top-performing LLMs in various\\nNLP tasks in Table 12.\\n5.2.1. Multi-task\\nMMLU [307]: A benchmark that measures the knowledge\\nacquired by models during pretraining and evaluates models in\\nzero-shot and few-shot settings across 57 subjects, testing both'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='zero-shot and few-shot settings across 57 subjects, testing both\\nworld knowledge and problem-solving ability.\\nSuperGLUE [2]: A more challenging and diverse successor\\nto the GLUE [309] benchmark, SuperGLUE includes a variety\\nof language understanding tasks, such as question answering,\\nnatural language inference, and co-reference resolution. It is\\ndesigned to provide a rigorous test of language understanding\\nand requires significant progress in areas like sample-efficient,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='and requires significant progress in areas like sample-efficient,\\ntransfer, multi-task, and unsupervised or self-supervised learn-\\ning.\\nBIG-bench [308]: The BIG-bench (Behavior of Intelligent\\nGenerative Models Benchmark) is a large-scale benchmark de-\\nsigned to test the abilities of LLMs across a wide range of\\ntasks, including reasoning, creativity, ethics, and understanding\\nof specific domains.\\nGLUE [309]: The General Language Understanding Evalua-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 25}, page_content='of specific domains.\\nGLUE [309]: The General Language Understanding Evalua-\\ntion (GLUE) benchmark is a collection of resources for train-\\ning, evaluating, and analyzing natural language understanding\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\\nfor most of the LLMs.\\nSequence\\nLR\\nOptimizers\\nPrecision\\nWeight\\nGrad\\nModels\\nBatch Size\\nLength\\nLR\\nWarmup\\nDecay\\nAdaFactorAdam AdamWFP16 BF16 Mixed Decay\\nClip\\nDropout\\nT5 (11B)\\n211\\n512\\n0.01\\n×\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nGPT3 (175B)\\n32K\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nmT5 (13B)\\n1024\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='32K\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nmT5 (13B)\\n1024\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nPanGu-α (200B)\\n-\\n1024\\n2e-5\\n-\\n-\\n-\\n-\\n-\\n-\\n✓\\n-\\n-\\n-\\n-\\nCPM-2 (198B)\\n1024\\n1024\\n0.001\\n-\\n-\\n✓\\n-\\n-\\n-\\n-\\n-\\n✓\\nCodex (12B)\\n-\\n-\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n-\\n-\\nERNIE 3.0 (12B)\\n6144\\n512\\n1e-4\\n✓\\nlinear\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nJurassic-1 (178B)\\n3.2M\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nHyperCLOVA (82B)\\n1024\\n-\\n6e-5\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nYuan 1.0 (245B)\\n<10M\\n2048\\n1.6e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nGopher (280B)\\n3M\\n2048\\n4e-5\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='-\\nYuan 1.0 (245B)\\n<10M\\n2048\\n1.6e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n-\\n-\\nGopher (280B)\\n3M\\n2048\\n4e-5\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n-\\n✓\\n-\\nERNIE 3.0 Titan (260B)\\n-\\n512\\n1e-4\\n✓\\nlinear\\n✓\\n✓\\n✓\\n✓\\n-\\nGPT-NeoX-20B\\n1538\\n2048\\n0.97e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nOPT (175B)\\n2M\\n2048\\n1.2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n✓\\n✓\\nBLOOM (176B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nGalactica (120B)\\n2M\\n2048\\n7e-6\\n✓\\nlinear decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n✓\\nGLaM (1.2T)\\n1M\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\nFP32 + ✓\\n-\\n✓\\n×\\nLaMDA (137B)\\n256K\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='1M\\n1024\\n0.01\\n-\\ninverse square root\\n✓\\nFP32 + ✓\\n-\\n✓\\n×\\nLaMDA (137B)\\n256K\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMT-NLG (530B)\\n1920\\n2048\\n5e-5\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n✓\\n✓\\n-\\nAlphaCode (41B)\\n2048\\n1536+768\\n1e-4\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n✓\\n✓\\n-\\nChinchilla (70B)\\n1.5M\\n2048\\n1e-4\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n-\\n-\\n-\\nPaLM (540B)\\n2048\\n2048\\n0.01\\n-\\ninverse square root\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n×\\nAlexaTM (20B)\\n2M\\n1024\\n1e-4\\n-\\nlinear decay to 5%\\n✓\\n✓\\n✓\\n-\\n✓\\nU-PaLM (540B)\\n32\\n2048\\n1e-4\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\nUL2 (20B)\\n1024\\n1024\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='✓\\n✓\\n✓\\n-\\n✓\\nU-PaLM (540B)\\n32\\n2048\\n1e-4\\n-\\ncosine\\n✓\\n-\\n-\\n-\\n-\\n-\\n-\\nUL2 (20B)\\n1024\\n1024\\n-\\n-\\ninverse square root\\n-\\n-\\n-\\n-\\n-\\n-\\n×\\n-\\n-\\nGLM (130B)\\n4224\\n2048\\n8e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeGen (16B)\\n2M\\n2048\\n5e-5\\n✓\\ncosine\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nLLaMA (65B)\\n4M Tokens\\n2048\\n1.5e-4\\n✓\\ncosine decay to 10%\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nPanGu-Σ (1.085T)\\n512\\n1024\\n2e-5\\n✓\\n-\\n✓\\n✓\\n-\\n-\\n-\\nBloombergGPT (50B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n×\\nXuan Yuan 2.0 (176B)\\n2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nCodeT5+ (16B)\\n2048\\n1024\\n2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='2048\\n2048\\n6e-5\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nCodeT5+ (16B)\\n2048\\n1024\\n2e-4\\n-\\nlinear\\n✓\\n✓\\n✓\\n-\\n-\\nStarCoder (15.5B)\\n512\\n8k\\n3e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n-\\n-\\nLLaMA-2 (70B)\\n4M Tokens\\n4k\\n1.5e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nLLaMA-3.1 (405B)\\n16M\\n8192\\n8e-5\\n✓\\nlinear+cosine\\n✓\\n✓\\n-\\n-\\n-\\nNemotron-4 (340B)\\n2304\\n4096\\n-\\n-\\nlinear\\n-\\n-\\n-\\n✓\\n-\\n-\\n×\\nDeepSeek (67B)\\n4608\\n4096\\n3.2e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nDeepSeek-v2 (67B)\\n9216\\n4k\\n2.4e-4\\n✓\\nstep-decay\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='4608\\n4096\\n3.2e-4\\n✓\\ncosine\\n✓\\n✓\\n✓\\n✓\\n-\\nDeepSeek-v2 (67B)\\n9216\\n4k\\n2.4e-4\\n✓\\nstep-decay\\n✓\\n-\\n-\\n-\\n✓\\n✓\\n-\\nTable 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\\nno model uses weight decay for instruction tuning.\\nSequence\\nOptimizers\\nGrad\\nModels\\nBatch Size\\nLength\\nLR\\nWarmup\\nLR_Decay\\nAdaFactor\\nAdam\\nAdamW\\nClip\\nDropout\\nWebGPT (175B)\\nBC:512, RM:32\\n-\\n6e-5\\n-\\n-\\n✓\\n-\\n-\\nT0 (11B)\\n1024\\n1280\\n1e-3\\n-\\n-\\n✓\\n-\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='AdamW\\nClip\\nDropout\\nWebGPT (175B)\\nBC:512, RM:32\\n-\\n6e-5\\n-\\n-\\n✓\\n-\\n-\\nT0 (11B)\\n1024\\n1280\\n1e-3\\n-\\n-\\n✓\\n-\\n✓\\nTk-Instruct (11B)\\n1024\\n-\\n1e-5\\n-\\nconstant\\n-\\n-\\n-\\n-\\n-\\nOPT-IML (175B)\\n128\\n2048\\n5e-5\\n×\\nlinear\\n✓\\n✓\\n✓\\nFlan-U-PaLM (540B)\\n32\\n-\\n1e-3\\n-\\nconstant\\n✓\\n-\\n✓\\nSparrow (70B)\\nRM: 8+16, RL:16\\n-\\n2e-6\\n✓\\ncosine decay to 10%\\n✓\\n✓\\n×\\nWizardCoder (15B)\\n512\\n2048\\n2e-5\\n✓\\ncosine\\n-\\n-\\n-\\n-\\n-\\nAlpaca (13B)\\n128\\n512\\n1e-5\\n✓\\ncosine\\n-\\n-\\n✓\\n✓\\n×\\nVicuna (13B)\\n128\\n-2048\\n2e-5\\n✓\\ncosine\\n✓\\n-\\n×\\nLIMA (65B)\\n32\\n2048\\n1e-5\\n×\\nlinear\\n✓\\n-\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='cosine\\n-\\n-\\n✓\\n✓\\n×\\nVicuna (13B)\\n128\\n-2048\\n2e-5\\n✓\\ncosine\\n✓\\n-\\n×\\nLIMA (65B)\\n32\\n2048\\n1e-5\\n×\\nlinear\\n✓\\n-\\n✓\\nsystems. It includes a variety of tasks that test a wide range of\\nlinguistic phenomena, making it a comprehensive tool for eval-\\nuating language understanding in AI.\\n5.2.2. Language Understanding\\nWinoGrande [354]: A large-scale dataset inspired by the orig-\\ninal Winograd [357] Schema Challenge tests models on their\\nability to resolve pronoun ambiguity and encourages the devel-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='ability to resolve pronoun ambiguity and encourages the devel-\\nopment of models that understand the broad context in natural\\nlanguage text.\\nCoQA [316]: A conversational question-answering dataset,\\nCoQA challenges models with questions that rely on conver-\\nsation history and require free-form text answers. Its diverse\\ncontent from seven domains makes it a rigorous test for mod-\\nels’ ability to handle a wide range of topics and conversational\\ncontexts.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 26}, page_content='els’ ability to handle a wide range of topics and conversational\\ncontexts.\\nWiC [317]: This dataset assesses a model’s ability to dis-\\ncern word meanings based on context, aiding in tasks related\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\\nDataset\\nType\\nSize/Samples\\nTasks\\nSource\\nCreation\\nComments\\nC4 [10]\\nPretrain\\n806GB\\n-\\nCommon Crawl\\nAutomated\\nA clean, multilingual dataset with billions\\nof tokens\\nmC4 [11]\\nPretrain\\n38.49TB\\n-\\nCommon Crawl\\nAutomated\\nA multilingual extension of the C4\\ndataset, mC4 identifies over 100 lan-\\nguages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301]\\nPretrain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='guages using cld3 from 71 monthly web\\nscrapes of Common Crawl.\\nPILE [301]\\nPretrain\\n825GB\\n-\\nCommon Crawl, PubMed Central,\\nOpenWebText2, ArXiv, GitHub,\\nBooks3, and others\\nAutomated\\nA massive dataset comprised of 22 con-\\nstituent sub-datasets\\nROOTs [302]\\nPretrain\\n1.61TB\\n-\\n498 Hugging Face datasets\\nAutomated\\n46 natural and 13 programming lan-\\nguages\\nMassiveText [116]\\nPretrain\\n10.5TB\\n-\\nMassiveWeb, Books, News,\\nWikipedia, Github, C4\\nAutomated\\n99% of the data is in English\\nWikipedia [303]\\nPretrain\\n-\\n-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Wikipedia, Github, C4\\nAutomated\\n99% of the data is in English\\nWikipedia [303]\\nPretrain\\n-\\n-\\nWikipedia\\nAutomated\\nDump of wikipedia\\nRedPajama [304]\\nPretrain\\n5TB\\n-\\nCommonCrawl, C4, Wikipedia,\\nGithub, Books, StackExchange\\nAutomated\\nOpen-source replica of LLaMA dataset\\nPushShift.io Reddit\\nPretrain\\n21.1GB\\n-\\nReddit\\nAutomated\\nSubmissions and comments on Reddit\\nfrom 2005 to 2019\\nBigPython [140]\\nPretrain\\n5.5TB\\nCoding\\nGitHub\\nAutomated\\n-\\nPool of Prompt (P3) [17]\\nInstructions\\n12M\\n62\\nPromptSource\\nManual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='5.5TB\\nCoding\\nGitHub\\nAutomated\\n-\\nPool of Prompt (P3) [17]\\nInstructions\\n12M\\n62\\nPromptSource\\nManual\\nA Subset of PromptSource, created from\\n177 datasets including summarization,\\nQA, classification, etc.\\nxP3 [154]\\nInstructions\\n81M\\n71\\nP3+Multilingual datasets\\nManual\\nExtending P3 to total 46 languages\\nSuper-NaturalInstructions (SNI) [18]\\nInstructions\\n12.4M\\n1616\\nMultiple datasets\\nManual\\nExtending P3 with additional multi-\\nlingual datasets, total 46 languages\\nFlan [16]\\nInstructions\\n15M\\n1836'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='lingual datasets, total 46 languages\\nFlan [16]\\nInstructions\\n15M\\n1836\\nMuffin+T0-SF+NIV2\\nManual\\nTotal 60 languages\\nOPT-IML [97]\\nInstructions\\n18.1M\\n1667\\n-\\nManual\\n-\\nSelf-Instruct [19]\\nInstructions\\n82k\\n175\\n-\\nAutomated\\nGenerated 52k instructions with 82k sam-\\nples from 175 seed tasks using GPT-3\\nAlpaca [158]\\nInstructions\\n52k\\n-\\n-\\nAutomated\\nEmployed self-instruct method to gener-\\nate data from text-davinci-003\\nVicuna [159]\\nInstructions\\n125k\\n-\\nShareGPT\\nAutomated\\nConversations\\nshared\\nby\\nusers\\non'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='Vicuna [159]\\nInstructions\\n125k\\n-\\nShareGPT\\nAutomated\\nConversations\\nshared\\nby\\nusers\\non\\nShareGPT using public APIs\\nLLaMA-GPT-4 [160]\\nInstructions\\n52k\\n-\\nAlpaca\\nAutomated\\nRecreated Alpaca dataset with GPT-4 in\\nEnglish and Chinese\\nUnnatural Instructions [305]\\nInstructions\\n68k\\n-\\n15-Seeds (SNI)\\nAutomated\\n-\\nLIMA [185]\\nInstructions\\n1k\\n-\\nMultiple datasets\\nManual\\nCarefully created samples to test perfor-\\nmance with fine-tuning on less data\\nAnthropic-HH-RLHF [306]\\nAlignment\\n142k\\n-\\n-\\nManual'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='mance with fine-tuning on less data\\nAnthropic-HH-RLHF [306]\\nAlignment\\n142k\\n-\\n-\\nManual\\nAnthropic-HH-RLHF-2 [178]\\nAlignment\\n39k\\n-\\n-\\nManual\\nto Word Sense Disambiguation.\\nWikitext103 [318]:\\nWith over 100 million tokens from\\nWikipedia’s top articles, this dataset is a rich resource for tasks\\nthat require understanding long-term dependencies, such as lan-\\nguage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='guage modeling and translation.\\nPG19 [319]: This is a digital library of diverse books from\\nProject Gutenberg. It is specifically designed to facilitate re-\\nsearch in unsupervised learning and language modeling, with a\\nspecial focus on long-form content.\\nC4 [10]: A clean, multilingual dataset, C4 offers billions of to-\\nkens from web-crawled data. It is a comprehensive resource for\\ntraining advanced Transformer models on various languages.\\nLCQMC [320]: The Large-scale Chinese Question Matching'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='LCQMC [320]: The Large-scale Chinese Question Matching\\nCorpus (LCQMC) is a dataset for evaluating the performance\\nof models in semantic matching tasks. It contains pairs of ques-\\ntions in Chinese and their matching status, making it a valuable\\nresource for research in Chinese language understanding.\\n5.2.3. Story Cloze and Sentence Completion\\nStoryCloze [334]: It introduces a new “StoryCloze Test”, a\\ncommonsense reasoning framework for evaluating story under-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='commonsense reasoning framework for evaluating story under-\\nstanding, generation, and script learning. It considers a model’s\\nability to understand and generate coherent and sensible stories.\\nLAMBADA [335]: This dataset evaluates contextual text un-\\nderstanding through a word prediction task. Models must pre-\\ndict the last word of a passage, which is easy for humans when\\ngiven the whole passage, but not when given only the last sen-\\ntence.\\n5.2.4. Physical Knowledge and World Understanding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='tence.\\n5.2.4. Physical Knowledge and World Understanding\\nPIQA [340]: A dataset that probes the physical knowledge of\\nmodels, aiming to understand how well they are learning about\\nthe real world.\\nTriviaQA [341]: A dataset that tests models on reading com-\\nprehension and open domain question answering (QA) tasks,\\nwith a focus on Information Retrieval (IR)-style QA.\\nARC [342]: A larger version of the ARC-Challenge, this\\ndataset contains both easy and challenging grade-school level,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 27}, page_content='dataset contains both easy and challenging grade-school level,\\nmultiple-choice science questions. It is a comprehensive test of\\na model’s ability to understand and answer complex questions.\\nARC-Easy [342]:\\nA subset of the ARC dataset, ARC-\\nEasy, contains questions that are answered correctly by either\\na retrieval-based algorithm or a word co-occurrence algorithm.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Table 9: Categorized evaluation datasets used in evaluating LLMs.\\nType\\nDatasets/Benchmarks\\nMulti-Task\\nMMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-\\nCLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]\\nLanguage Understanding\\nCoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],\\nCB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\\nCLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]\\nStory Cloze and\\nSentence Completion\\nStoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-\\nFC [312]\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\\nBookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]\\nContextual Language\\nUnderstanding\\nRACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],\\ncMedQA [351],cMedQA2 [352], MATINF-QA [353]\\nCommonsense Reasoning\\nWinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\\nCLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]\\nReading Comprehension\\nSQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],\\nCMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-\\ntiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\\nDuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD\\n1.0 [380], CAIL2018-Task1 & Task2 [381]\\nMathematical Reasoning\\nMATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-\\nDiv [388], MAWPS [389], SVAMP [390]\\nProblem Solving\\nHumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]\\nNatural Language Inference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Natural Language Inference\\n& Logical Reasoning\\nANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],\\nANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-\\ngyQA [349]\\nCross-Lingual Understanding\\nMLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-\\nGoldP [403], MLSum [404]\\nTruthfulness and Fact Checking\\nTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Truthfulness and Fact Checking\\nTruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\\nBiases and Ethics in AI\\nETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]\\nToxicity\\nRealToxicityPrompts [413], CivilComments toxicity classification [414]\\nLanguage Translation\\nWMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]\\nScientific Knowledge\\nAminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral\\nGroups [148]\\nDialogue'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='Groups [148]\\nDialogue\\nWizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],\\nKdConv [421]\\nTopic Classification\\nTNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]\\nIt is a great starting point for models beginning to explore ad-\\nvanced question-answering.\\nARC-Challenge [342]:\\nA rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='A rigorous question-answering\\ndataset, ARC-Challenge includes complex, grade-school level\\nquestions that demand reasoning beyond simple retrieval, test-\\ning the true comprehension capabilities of models.\\n5.2.5. Contextual Language Understanding\\nRACE [347]: The RACE dataset is a reading comprehension\\ndataset collected from English examinations in China, which\\nbenchmarks AI models for understanding and answering ques-\\ntions on long and complex passages, simulating the challenge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='tions on long and complex passages, simulating the challenge\\nof a real-world examination.\\nRACE-Middle [347]: Another subset of the RACE [347]\\ndataset, RACE-Middle, contains middle school-level English\\nexam questions. It offers a slightly less challenging but academ-\\nically oriented evaluation of a model’s comprehension skills.\\nRACE-High [347]: A subset of the RACE [347] dataset,\\nRACE-High consists of high school-level English exam ques-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='RACE-High consists of high school-level English exam ques-\\ntions. It is designed to evaluate the comprehension ability of\\nmodels in a more academic and challenging context.\\nQuAC [348]: This dataset simulates an information-seeking\\ndialog between students and teachers using hidden Wikipedia\\ntext. It introduces unique challenges not found in machine com-\\nprehension datasets, making it a valuable resource for advanc-\\ning dialog systems.\\n5.2.6. Commonsense Reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='ing dialog systems.\\n5.2.6. Commonsense Reasoning\\nHellaSwag [355]: A dataset that challenges models to pick the\\nbest ending to a context uses Adversarial Filtering to create a\\n‘Goldilocks’ zone of complexity, where generated text is absurd\\nto humans but often misclassified by models.\\nCOPA [401]: This dataset evaluates a model’s progress in\\nopen-domain commonsense causal reasoning. Each question\\ncomprises a premise and two alternatives, and the model must'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 28}, page_content='comprises a premise and two alternatives, and the model must\\nselect the more plausible alternative, testing a model’s ability to\\nunderstand and reason about cause and effect.\\nWSC [357]: The Winograd Schema Challenge (WSC) is a\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\\nis natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\\n“Mem.” is memorization.\\nBenchmark\\nModels\\nTraining Dataset\\nBIG-\\nbench\\nMMLU\\nSuper\\nGLUE\\nQA\\nClf\\nNLI\\nMT\\nCloze/\\nCompletion\\nRC\\nCR\\nMR\\nCoding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5\\nC4 [10]\\n✓\\n✓\\n✓\\n✓'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='QA\\nClf\\nNLI\\nMT\\nCloze/\\nCompletion\\nRC\\nCR\\nMR\\nCoding\\nTruthful/\\nBias/\\nToxicity/\\nMem.\\nT5\\nC4 [10]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGPT-3\\nCommon Crawl, WebText, Books Cor-\\npora, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nmT5\\nmC4 [11]\\n✓\\n✓\\n✓\\nPanGu-α\\n1.1TB Chinese Text Corpus\\n✓\\n✓\\n✓\\n✓\\n✓\\nCPM-2\\nWuDaoCorpus [109]\\n✓\\n✓\\nCodex\\n54 million public repositories from Github\\n✓\\nERNIE-3.0\\nChinese text corpora, Baidu Search, Web\\ntext, QA-long, QA-short, Poetry and Cou-\\nplet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='plet Domain-specific data from medical,\\nlaw, and financial area Baidu knowledge\\ngraph with more than 50 million facts\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nJurassic-1\\nWikipedia, OWT, Books, C4, Pile [301],\\narXiv, GitHub\\n✓\\n✓\\n✓\\n✓\\nHyperCLOVA\\nKorean blogs, Community sites, News,\\nKiN Korean Wikipedia, Wikipedia (En-\\nglish and Japanese), Modu-Corpus: Mes-\\nsenger, News, Spoken and written lan-\\nguage corpus, Web corpus\\n✓\\nYuan 1.0\\nCommon Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓\\n✓\\n✓\\n✓\\nGopher'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='✓\\nYuan 1.0\\nCommon Crawl, SogouT, Sogou News,\\nBaidu Baike, Wikipedia, Books\\n✓\\n✓\\n✓\\n✓\\nGopher\\nsubsets of MassiveWeb Books, C4, News,\\nGitHub and Wikipedia samples from Mas-\\nsiveText\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nERNIE-3.0 TITAN\\nSame as ERNIE 3.0 and ERNIE 3.0 ad-\\nversarial dataset, ERNIE 3.0 controllable\\ndataset\\n✓\\n✓\\n✓\\n✓\\n✓\\nGPT-NeoX-20B\\nPile [301]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nOPT\\nRoBERTa [299], Pile [301], PushShift.io\\nReddit [423]\\n✓\\n✓\\n✓\\n✓\\nBLOOM\\nROOTs [13]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGalactica\\narXiv, PMC, Semantic Scholar, Wikipedia,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='✓\\n✓\\n✓\\n✓\\nBLOOM\\nROOTs [13]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGalactica\\narXiv, PMC, Semantic Scholar, Wikipedia,\\nStackExchange, LibreText, Open Text-\\nbooks, RefSeq Genome, OEIS, LIPID\\nMAPS, NASAExoplanet, Common Crawl,\\nScientificCC, AcademicCC, GitHub repos-\\nitories Khan Problems, GSM8K, OneS-\\nmallStep\\n✓\\n✓\\n✓\\n✓\\n✓\\nGLaM\\nFiltered Webpages, Social media conversa-\\ntions Wikipedia, Forums, Books, News\\n✓\\n✓\\n✓\\n✓\\n✓\\nLaMDA\\nInfiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG\\nTwo snapshots of Common Crawl and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='Infiniset : Public documents, Dialogs, Ut-\\nterances\\n✓\\nMT-NLG\\nTwo snapshots of Common Crawl and\\nBooks3, OpenWebText2, Stack Exchange,\\nPubMed Abstracts,\\nWikipedia,\\nPG-19\\n[242], BookCorpus2, NIH ExPorter, Pile,\\nCC-Stories, RealNews\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlphaCode\\nSelected GitHub repositories, CodeCon-\\ntests: Codeforces, Description2Code, Co-\\ndeNet\\n✓\\nChinchilla\\nMassiveWeb,\\nMassiveText Books,\\nC4,\\nNews, GitHub, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM\\nwebpages, books, Wikipedia, news, arti-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='C4,\\nNews, GitHub, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM\\nwebpages, books, Wikipedia, news, arti-\\ncles, source code, social media conversa-\\ntions\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlexaTM\\nWikipedia, mC4\\n✓\\n✓\\n✓\\n✓\\n✓\\nU-PaLM\\nSame as PaLM\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nUL2\\n-\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nGLM-130B\\n-\\n✓\\n✓\\n✓\\nCodeGen\\nPile, BigQuery, BigPython\\n✓\\nLLaMA\\nCommonCrawl, C4, Github, Wikipedia,\\nBooks, arXiv, StackExchange\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPanGu-Σ\\nWuDaoCorpora, CLUE, Pile, C4, Python\\ncode\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nBloombergGPT\\ninPile, Pile, C4, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeT5+'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 29}, page_content='code\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nBloombergGPT\\ninPile, Pile, C4, Wikipedia\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nCodeT5+\\nCodeSearchNet, Github Code\\n✓\\n✓\\nStarCoder\\nThe Stack v1.2\\n✓\\n✓\\n✓\\n✓\\nLLaMA-2\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nPaLM-2\\nWeb documents, Code, Books, Maths,\\nConversation\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\\nModels\\nTraining Dataset\\nBIG-\\nbench\\nMMLU\\nBBH\\nRAFT\\nFLAN\\nSNI\\nPromptSource\\nTyDiQA\\nHumanEval\\nMBPP\\nTruthful/\\nBias/\\nToxicity\\nT0\\nPool of Prompts\\n✓\\nWebGPT\\nELI5\\n[424],\\nELI5\\nfact-\\ncheck\\n[166],\\nTriviaQA\\n[341],\\nARC-Challenge\\n[342],\\nARC-\\nEasy\\n[342],\\nHand-written\\ndata,\\nDemonstrations of humans, Com-\\nparisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCT\\nSNI [18]\\n✓\\nmT0'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='parisons between model-generated\\nanswers\\n✓\\nTk-INSTRUCT\\nSNI [18]\\n✓\\nmT0\\nxP3 [154]\\nOPT-IML\\nPromptSource [17], FLAN [16],\\nSNI\\n[425],\\nUnifiedSKG\\n[426],\\nCrossFit\\n[427],\\nExMix\\n[428],\\nT5 [10], Reasoning\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nFlan\\nMuffin, T0-SF, NIv2, CoT\\n✓\\n✓\\n✓\\nWizardCoder\\nCode Alpaca\\n✓\\n✓\\nreading comprehension task in which a system must resolve\\nreferences in a text, often requiring world knowledge and rea-\\nsoning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='soning about the text.\\nCSQA [358]: The CommonsenseQA is a question-answering\\ndataset that requires commonsense knowledge to evaluate the\\nability of AI models to understand and answer questions.\\n5.2.7. Reading Comprehension\\nBoolQ [363]: A dataset derived from Google search queries,\\nBoolQ challenges models to answer binary (yes/no) questions.\\nThe questions are naturally occurring and are paired with a\\nparagraph from a Wikipedia article containing the answer. It'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='paragraph from a Wikipedia article containing the answer. It\\nis a test of reading comprehension and reasoning.\\nSQUADv2 [364]: The Stanford Question Answering Dataset\\n(SQuAD) [362] is a collection of questions posed by crowd\\nworkers on a set of Wikipedia articles, where the answer to ev-\\nery question is a segment of text from the corresponding reading\\npassage. SQuADv2 combines the original SQuAD1.1 dataset\\nwith over 50,000 unanswerable questions. The aim is to evalu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='with over 50,000 unanswerable questions. The aim is to evalu-\\nate a model’s ability to understand and answer questions based\\non a given context and to determine when a question is unan-\\nswerable.\\nDROP [365]: DROP, or Discrete Reasoning Over the con-\\ntent of Paragraphs, is designed to test a model’s ability to un-\\nderstand a wide variety of reading phenomena. It encourages\\ncomprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='comprehensive and reliable evaluation of reading comprehen-\\nsion capabilities.\\nRTE [366]:\\nThe Recognizing Textual Entailment (RTE)\\ndatasets come from a series of annual competitions on textual\\nentailment, predicting whether a given sentence logically fol-\\nlows from another and evaluating a model’s understanding of\\nlogical relationships in a text.\\nWebQA [367]: A dataset for open-domain question answering,\\nWebQA offers a large collection of web-based question-answer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='WebQA offers a large collection of web-based question-answer\\npairs. It is designed to assess the ability of AI models to under-\\nstand and answer questions based on web content.\\nCMRC2018 [369]: This dataset is a test of Chinese language\\nmodels’ ability to reason comprehensively and is designed with\\na challenging span-extraction format that pushes the boundaries\\nof machine performance.\\n5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='5.2.8. Mathematical Reasoning\\nMATH [382]: This dataset is a platform for evaluating the\\nmathematical problem-solving abilities of AI models. It con-\\ntains a diverse set of math problems, ranging from arithmetic\\nto calculus, and is designed to test the model’s ability to under-\\nstand and solve complex mathematical problems.\\nMath23k [383]: This one challenges a model’s ability to un-\\nderstand and solve mathematical word problems. It contains'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='derstand and solve mathematical word problems. It contains\\n23,000 Chinese arithmetic word problems that require models\\nto perform reasoning and computation based on the problem\\ndescription.\\nGSM8K [384]: A dataset of diverse grade school math word\\nproblems, testing a model’s ability to perform multi-step math-\\nematical reasoning.\\n5.2.9. Problem Solving and Logical Reasoning\\nANLI [393]: A large-scale dataset designed to test the robust-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='ANLI [393]: A large-scale dataset designed to test the robust-\\nness of machine learning models in Natural Language Inference\\n(NLI) is created through an iterative, adversarial process where\\nhumans try to generate examples that models cannot correctly\\nclassify.\\nHumanEval [141]: A dataset for evaluating the problem-\\nsolving ability of AI models, which includes a diverse set of\\ntasks that require various cognitive abilities, making it a com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='tasks that require various cognitive abilities, making it a com-\\nprehensive tool for assessing general intelligence in AI.\\nStrategyQA [349]: A question-answering dataset that re-\\nquires reasoning over multiple pieces of evidence to evaluate\\nthe strategic reasoning ability of AI models, pushing the bound-\\naries of what machines can understand and answer.\\n5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 30}, page_content='5.2.10. Cross-Lingual Understanding\\nXNLI [398]: A cross-lingual benchmark, XNLI extends the\\nMultiNLI [429] corpus to 15 languages, including low-resource\\nones like Urdu. It tests models on cross-lingual sentence under-\\nstanding, with 112,500 annotated pairs across three categories:\\nentailment, contradiction, and neutral.\\nPAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver-\\nsaries from Word Scrambling, is a multilingual version of the\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='PAWS [430] dataset for paraphrase identification. It includes\\nexamples in seven languages and is designed to evaluate the\\nperformance of cross-lingual paraphrase identification models.\\n5.2.11. Truthfulness\\nTruthful-QA [405]: A unique benchmark that measures a\\nlanguage model’s truthfulness when generating answers. The\\ndataset includes questions across various categories like health,\\nlaw, and politics, some designed to test the model against com-\\nmon human misconceptions.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='law, and politics, some designed to test the model against com-\\nmon human misconceptions.\\n5.2.12. Biases and Ethics in AI\\nETHOS [408]: ETHOS is a hate speech detection dataset\\nbuilt from YouTube and Reddit comments. It is a tool in the\\nfight against online hate speech, offering binary and multi-label\\nvariants for robust content moderation.\\nStereoSet [409]: StereoSet is a comprehensive dataset de-\\nsigned to measure and evaluate the presence of stereotypical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='signed to measure and evaluate the presence of stereotypical\\nbiases in language models. It focuses on four key domains:\\ngender, profession, race, and religion. Contrasting stereotypi-\\ncal bias against language modeling ability provides a valuable\\ntool for understanding and mitigating biases in large language\\nmodels.\\n6. Applications\\nApplying Large Language Models (LLMs) to a variety of\\ndownstream tasks has become a popular trend in both AI-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='downstream tasks has become a popular trend in both AI-\\nrelated research communities and industries, with many emerg-\\ning uses being discovered and explored daily. LLMs, which are\\ncapable of understanding and generating human-like text, have\\nfound meaningful applications across a variety of fields. This\\nsection provides an overview of LLM applications in medicine,\\neducation, science, mathematics, law, finance, robotics, and\\ncoding. While each of these domains pose different challenges,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='coding. While each of these domains pose different challenges,\\nLLMs open up opportunities to make significant contributions\\nto these domains through their generalizability.\\nGeneral Purpose:\\nLLMs are being widely considered as\\ngeneral-purpose tools for a wide variety of tasks [431]. This\\nis due to their inherent ability to understand, generate, and\\nmanipulate human-like text in a contextually relevant man-\\nner. This allows them to perform tasks ranging from simple'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='ner. This allows them to perform tasks ranging from simple\\nlanguage translation and question-answering to more complex\\ntasks like summarization, text generation, and even program-\\nming help [432]. The utility of LLMs is further enhanced by\\ntheir ability to adapt to the specific style and tone of the text\\nthey are processing, making the outputs more user-friendly and\\ncontext-aware. In everyday applications, LLMs can be used\\nas personal assistants, helping users draft emails or schedule'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='as personal assistants, helping users draft emails or schedule\\nappointments [433]; they can also be deployed in customer ser-\\nvice to handle common questions or applied to generate content\\nfor digital platforms like websites by creating human-like text\\nbased on given prompts [434]. Moreover, LLMs play a cru-\\ncial role in data analysis, where they can filter large volumes of\\ntext data, summarize key points, and find patterns that would'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='text data, summarize key points, and find patterns that would\\ntake humans much longer to identify [435]. Despite their wide-\\nranging applications, it is essential to remember that LLMs,\\nsimilar to any AI system, are only as good as the data they have\\nbeen trained on.\\nMedicine: The application of LLMs in the field of medicine is\\nreshaping healthcare delivery and research. For example, LLMs\\nare increasingly used in clinical decision support systems to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='are increasingly used in clinical decision support systems to\\nprovide physicians with evidence-based treatment recommen-\\ndations [436, 437, 438]. By analyzing patient data and medical\\nliterature, they can help identify potential diagnoses, suggest\\nappropriate tests, and recommend optimal treatment strategies.\\nMoreover, LLMs can also enhance patient interactions with\\nhealthcare systems; e.g., they can be used in chatbot applica-\\ntions [439, 440, 441] to answer patient queries about symptoms'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='tions [439, 440, 441] to answer patient queries about symptoms\\nor medications, schedule appointments, and even provide es-\\nsential health advice. For medical research, LLMs are used to\\nextract and filter information from a considerable amount of\\nmedical literature, identify relevant studies, summarize find-\\nings, and even predict future research trends [442, 443, 444].\\nFor medical education, LLMs can help create training mate-\\nrials, generate exam questions, provide detailed explanations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='rials, generate exam questions, provide detailed explanations\\nof complex medical topics, and offer personalized feedback to\\nstudents [445, 446, 447, 448]. They can also simulate patient\\ninteractions, enabling students to practice and improve their\\nclinical skills. At a broader level, LLMs can assist in public\\nhealth initiatives by analyzing media data to detect disease out-\\nbreaks, monitor public sentiment towards health policies, and\\ndisseminate health information in a clear and understandable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='disseminate health information in a clear and understandable\\nmanner [449]. LLMs can be employed to support public health\\ninitiatives, addressing related issues such as data privacy, the\\nnecessity for explainability, and the potential risk of propagat-\\ning biases [450, 451].\\nEducation: The integration of LLMs into the educational sec-\\ntor offers opportunities to enhance learning experiences, teacher\\nsupport, and educational content development. For students, by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='support, and educational content development. For students, by\\nanalyzing their learning styles, performance, and preferences,\\nLLMs can provide customized study materials and practice\\nquestions to develop personalized learning experiences [452].\\nFor teachers, LLMs can help to create lesson plans and grade\\nassignments and generate diverse and inclusive educational\\ncontent, significantly saving more time for teaching and student\\ninteraction [453, 454]. In language learning, LLMs serve as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='interaction [453, 454]. In language learning, LLMs serve as\\nadvanced conversational partners capable of simulating conver-\\nsations in multiple languages, correcting grammar, enhancing\\nvocabulary, and aiding pronunciation for the needs of fluency\\nin practice [455]. Furthermore, LLMs improve accessibility\\nin education by providing support for students with disabili-\\nties. They can generate real-time transcriptions for the hear-\\ning impaired, offer reading assistance for the visually impaired,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='ing impaired, offer reading assistance for the visually impaired,\\nand simplify complex texts for those with learning disabili-\\nties [451]. As LLMs continue to evolve, their applications in\\neducation can benefit more students and teachers from different\\nperspectives in practice.\\nScience: Similar to medical applications, LLMs can expedite\\nthe research process by quickly analyzing and summarizing sci-\\nentific literature. By briefing comprehensible and accessible re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 31}, page_content='entific literature. By briefing comprehensible and accessible re-\\nsearch summaries, LLMs can assist researchers in staying up-\\nto-date with the latest findings, even in fields outside their area\\nof expertise [456, 457]. In addition, LLMs can aid scientists\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\\nto the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\\nbenchmark.\\nTask\\nDataset/Benchmark\\nTop-1\\nTop-2\\nTop-3\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nMulti-Task\\nBIG-bench (B)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Score (N-shots)\\nModel (Size)\\nScore (N-shots)\\nModel (Size)\\nScore (N-shots)\\nMulti-Task\\nBIG-bench (B)\\nChinchilla (70B)\\n65.1 (5-shot)\\nGopher (280B)\\n53.97 (5-shot)\\nPaLM (540B)\\n53.7 (5-shot)\\nMMLU (B)\\nGPT-4 (-)\\n86.4 (5-shot)\\nGemini (Ultra)\\n83.7 (5-shot)\\nFlan-PaLM-2( f) (Large)\\n81.2 (5-shot)\\nLanguage Understanding\\nSuperGLUE (B)\\nERNIE 3.0 (12B)\\n90.6 (-)\\nPaLM(f) (540B)\\n90.4 (-)\\nT5 (11B)\\n88.9 (-)\\nStory Comprehension and\\nGeneration\\nHellaSwag\\nGPT-4 (-)\\n95.3 (10-shot)\\nGemini (Ultra)\\n87.8 (10-shot)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Story Comprehension and\\nGeneration\\nHellaSwag\\nGPT-4 (-)\\n95.3 (10-shot)\\nGemini (Ultra)\\n87.8 (10-shot)\\nPaLM-2 (Large)\\n86.8 (one shot)\\nStoryCloze\\nGPT3 (175B)\\n87.7 (few shot)\\nPaLM-2 (Large)\\n87.4 (one shot)\\nOPT (175B)\\n79.82 (-)\\nPhysical Knowledge and\\nWorld Understanding\\nPIQA\\nPaLM-2 (Large)\\n85.0 (one shot)\\nLLaMa (65B)\\n82.8 (zero shot)\\nMT-NLG (530B)\\n81.99 (zero shot)\\nTriviaQA\\nPaLM-2 (Large)\\n86.1 (one shot)\\nLLaMA-2 (70B)\\n85.0 (one shot)\\nPaLM (540B)\\n81.4 (one shot)\\nContextual Language\\nUnderstanding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='LLaMA-2 (70B)\\n85.0 (one shot)\\nPaLM (540B)\\n81.4 (one shot)\\nContextual Language\\nUnderstanding\\nLAMBADA\\nPaLM (540B)\\n89.7 (few shot)\\nMT-NLG (530B)\\n87.15 (few shot)\\nPaLM-2 (Large)\\n86.9 (one shot)\\nCommonsense Reasoning\\nWinoGrande\\nGPT-4 (-)\\n87.5 (5-shot)\\nPaLM-2 (Large)\\n83.0 (one shot)\\nPaLM (540B)\\n81.1 (zero shot)\\nSIQA\\nLLaMA (65B)\\n52.3 (zero shot)\\nChinchilla (70B)\\n51.3 (zero shot)\\nGopher (280B)\\n50.6 (zero shot)\\nReading Comprehension\\nBoolQ\\nPaLM(f) (540B)\\n92.2 (-)\\nT5 (11B)\\n91.2 (-)\\nPaLM-2 (Large)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Reading Comprehension\\nBoolQ\\nPaLM(f) (540B)\\n92.2 (-)\\nT5 (11B)\\n91.2 (-)\\nPaLM-2 (Large)\\n90.9 (one shot)\\nTruthfulness\\nTruthful-QA\\nLLaMA (65B)\\n57 (-)\\nMathematical Reasoning\\nMATH\\nGemini (Ultra)\\n53.2 (4-shot)\\nPaLM-2 (Large)\\n34.3 (4-shot)\\nLLaMa-2 (65B)\\n13.5 (4-shot)\\nGSM8K\\nGPT-4 (-)\\n92.0 (5-shot)\\nPaLM-2 (Large)\\n80.7 (8-shot)\\nU-PaLM (540B)\\n58.5 (-)\\nProblem Solving and\\nLogical Reasoning\\nHumanEval\\nGemini( f) (Ultra)\\n74.4 (zero shot)\\nGPT-4 (-)\\n67.0 (zero shot)\\nCode Llama (34B)\\n48.8 (zero shot)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='Gemini( f) (Ultra)\\n74.4 (zero shot)\\nGPT-4 (-)\\n67.0 (zero shot)\\nCode Llama (34B)\\n48.8 (zero shot)\\nin formulating new hypotheses and research questions since\\ntheir ability to process large-scale datasets allows them to un-\\nveil insights that might not be immediately apparent to human\\nresearchers [458]. Moreover, for scientific writing, LLMs can\\nhelp researchers draft documents, suggest improvements, and\\nensure adherence to specific formatting guidelines [459, 460].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='ensure adherence to specific formatting guidelines [459, 460].\\nThis not only saves time but also improves the clarity of scien-\\ntific communication, enabling interdisciplinary teams to work\\ntogether more effectively.\\nMaths: In addition to providing mathematical research and\\neducation support, LLMs can assist in solving mathematical\\nproblems by giving step-by-step explanations and guiding users\\nthrough complex proofs and calculations. They can help iden-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='through complex proofs and calculations. They can help iden-\\ntify errors in reasoning or computation and suggest corrections,\\nserving as an invaluable tool for both learning and verification\\npurposes [461, 462]. LLMs can be employed to check the valid-\\nity of mathematical proofs, offering a preliminary filter before\\nhuman review. While they are not a substitute for the meticu-\\nlous work of mathematicians, they can help simplify the process'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='lous work of mathematicians, they can help simplify the process\\nof proof verification [463, 464]. Moreover, LLMs enhance ac-\\ncessibility to mathematics by translating complex concepts and\\nfindings into understandable language for non-specialists [465],\\nwhere the gap between theoretical mathematics and applied\\ncontexts such as physics, engineering, and economics can be\\nbridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='bridged.\\nLaw: LLMs can assist with the thematic analysis of legal doc-\\numents, including generating initial coding for datasets, iden-\\ntifying themes, and classifying data according to these themes.\\nThis collaborative effort between legal experts and LLMs has\\nproved to be effective in analyzing legal texts such as court\\nopinions on theft, improving both the efficiency and quality of\\nthe research [466]. Additionally, LLMs have been evaluated for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='the research [466]. Additionally, LLMs have been evaluated for\\ntheir ability to generate explanations of legal terms, focusing\\non improving factual accuracy and relevance by incorporating\\nsentences from case law. By feeding relevant case law into the\\nLLM, the augmented models can generate higher-quality expla-\\nnations with less factually incorrect information [467]. More-\\nover, LLMs can be trained with specialized domain knowledge\\nto perform legal reasoning tasks [468] and answer legal ques-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='to perform legal reasoning tasks [468] and answer legal ques-\\ntions [469].\\nFinance: LLMs like BloombergGPT [151], trained on exten-\\nsive proprietary financial datasets, exhibit superior performance\\non financial tasks. This indicates the value of domain-specific\\ntraining in creating LLMs that can more accurately understand\\nand process industry-specific language and concepts. The intro-\\nduction of FinGPT [470] as an open-source model offers trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='duction of FinGPT [470] as an open-source model offers trans-\\nparent and accessible resources to develop novel applications\\nsuch as robo-advising, algorithmic trading, and low-code so-\\nlutions, ultimately expanding the capabilities of financial ser-\\nvices. Both BloombergGPT and FinGPT show the adaptabil-\\nity of LLMs to the financial domain, with the former showing\\nthe power of custom datasets and the latter emphasizing a data-\\ncentric approach and low-rank adaptation techniques for cus-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='centric approach and low-rank adaptation techniques for cus-\\ntomization. Moreover, LLMs demonstrate an ability to break\\ndown complex financial tasks into actionable plans, enabling\\nend-to-end solutions that were previously unfeasible with a sin-\\ngle model [471].\\nRobotics: In robotics research, LLMs have promising appli-\\ncations, such as enhancing human-robot interaction [28, 472,\\n473, 474], task planning [237], motion planning [246], nav-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='473, 474], task planning [237], motion planning [246], nav-\\nigation [246, 475], object manipulation [236], personalized\\nrobots [476], etc. LLMs enable robots to understand the en-\\nvironment effectively and generate plans to complete tasks col-\\nlaboratively [240, 26]. They can facilitate continuous learning\\nby allowing robots to access and integrate information from a\\nwide range of sources, helping robots acquire new skills, adapt\\nto changes, and refine their paths [224, 233, 234].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 32}, page_content='to changes, and refine their paths [224, 233, 234].\\n7. Challenges and Future Directions\\nLLMs such as GPT-4 and its predecessors have significantly\\nadvanced natural language processing. Nevertheless, they also\\nbring along a set of challenges. The computational cost, ad-\\nversarial robustness, and interpretability are among the tech-\\nnical challenges that are intrinsic to these models.\\nFurther-\\nmore, as these models are scaled up to handle more complex\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tasks or to operate in more complex or dynamic environments,\\nnew challenges in scalability, privacy, and real-time processing\\nemerge. On the frontier of foundational research, integrating\\nmulti-modality and the effectiveness of transfer learning are be-\\ning keenly explored. Additionally, the continuous learning as-\\npect of these models, which aims to have models that can adapt\\nto new information over time, presents a fresh set of challenges.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='to new information over time, presents a fresh set of challenges.\\nThese challenges not only underscore the technical intricacies\\ninvolved but also highlight the broader impact and the future\\ntrajectory of LLMs in real-world applications. The following\\nsections delve into these challenges, shedding light on the on-\\ngoing and potential efforts to address them.\\nComputational Cost: Training LLMs require extensive compu-\\ntational resources, which increases production costs and raises'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tational resources, which increases production costs and raises\\nenvironmental concerns due to substantial energy consump-\\ntion during large-scale training. Improved performance occurs\\nas computational resources increase, but the rate of improve-\\nment gradually decreases when both the model and dataset\\nsize remain fixed, following the power law of diminishing re-\\nturns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='turns [477].\\nBias and Fairness: LLMs can inherit and amplify societal bi-\\nases in their training data. These biases can manifest in the\\nmodel’s outputs, leading to potential ethical and fairness is-\\nsues [478].\\nOverfitting: Although LLMs possess substantial learning ca-\\npabilities, they are susceptible to overfitting noisy and peculiar\\npatterns within their extensive training data. Consequently, this\\nmay cause them to generate illogical responses [479]. The de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='may cause them to generate illogical responses [479]. The de-\\nbate about Memorization vs. Generalization in LLMs is about\\nfinding the right balance. Memorization allows the model to\\nremember specific details from its training data, ensuring it can\\nprovide accurate answers to precise questions. However, gen-\\neralization enables the model to make inferences and produce\\nresponses for inputs it has not seen before, which is essential\\nfor handling various real-world tasks. Striking the right bal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='for handling various real-world tasks. Striking the right bal-\\nance is the challenge: too much memorization can lead to over-\\nfitting, making the model inflexible and struggling with new\\ninputs [480].\\nEconomic and Research Inequality: The high cost of train-\\ning and deploying LLMs may make their development concen-\\ntrated within well-funded organizations, potentially worsening\\neconomic and research inequalities in AI [481].\\nReasoning and Planning: Some reasoning and planning tasks,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Reasoning and Planning: Some reasoning and planning tasks,\\neven as seemingly simple as common-sense planning, which\\nhumans find easy, remain well beyond the current capabilities\\nof LLMs evaluated using an assessment framework. This is not\\nentirely unexpected, considering that LLMs primarily generate\\ntext completions based on likelihood and offer no solid guaran-\\ntees in terms of reasoning abilities [482].\\nHallucinations: LLMs exhibit “hallucinations\", where they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Hallucinations: LLMs exhibit “hallucinations\", where they\\ngenerate responses that, while sounding plausible, are incorrect\\nor do not align with the provided information [483]. Hallucina-\\ntions can be categorized into three categories.\\n• Input-conflicting hallucination, wherein LLMs produce\\ncontent that diverges from the input given by users.\\n• Context-conflicting hallucination, where LLMs generate\\ncontent that contradicts information they have generated\\nearlier.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='content that contradicts information they have generated\\nearlier.\\n• Fact-conflicting hallucination involves LLM’s generation\\nof content that does not align with established world\\nknowledge.\\nPrompt Engineering: Prompts serve as inputs to LLMs, and\\ntheir syntax and semantics play a crucial role in determining\\nthe model’s output. The prompt variations, sometimes counter-\\nintuitive to humans, can result in significant changes in model\\noutput and are addressed through prompt engineering, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='output and are addressed through prompt engineering, which\\ninvolves designing natural language queries to guide LLMs\\nresponses effectively [484, 32].\\nLimited Knowledge: Information acquired during pretraining\\nis limited and may become obsolete after some time.\\nRe-\\ntraining the model using updated data is costly. To generate\\nfactually accurate responses, people use a retrieval augmen-\\ntation pipeline [198].\\nHowever, pre-trained models are not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='tation pipeline [198].\\nHowever, pre-trained models are not\\ntrained with retrieval augmentation generation (RAG) [6, 21];\\nhence, adapting the training pipeline is necessary [193, 25].\\nSafety and Controllability: Using LLMs comes with the risk\\nof generating harmful, misleading, or inappropriate content,\\nwhether by accident or when given specific prompts. Ensuring\\nthese models are safely utilized is a significant concern [485].\\nSecurity and Privacy: LLMs are prone to leaking personal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Security and Privacy: LLMs are prone to leaking personal\\ninformation and generating false, unethical, misaligned re-\\nsponses. Researchers have explored various security attacks,\\ni.e., backdoor attacks, jailbreaking, prompt injection, and data\\npoisoning, that lead to breaking LLMs security.\\nTherefore,\\ndeveloping better defense mechanisms is essential to ensure\\nLLMs are safe, reliable, and trustworthy for complex AI\\napplications [486].\\nMulti-Modality:\\nMulti-modal learning, where LLMs are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='applications [486].\\nMulti-Modality:\\nMulti-modal learning, where LLMs are\\ntrained on diverse data like text, images, and videos, aims to\\ncreate models with richer understanding but faces challenges\\nin data alignment, fusion strategies, and higher computational\\ndemands.\\nCatastrophic Forgetting:\\nLLMs are often pre-trained on\\nlarge datasets and then fine-tuned on domain-specific data,\\nreducing training resources.\\nHowever, they face issues like'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='reducing training resources.\\nHowever, they face issues like\\ndomain adaptation and catastrophic forgetting, which hinder\\nthe retention of original knowledge when learning new tasks.\\nAdversarial Robustness:\\nLarge Language Models (LLMs)\\nhave shown great capabilities in various tasks but are vul-\\nnerable to adversarial attacks, where slight, deliberate input\\nalterations can mislead them.\\nEspecially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='Especially with models like\\nBERT, adversarial fine-tuning can enhance robustness, al-\\nthough it sometimes compromises generalization [487].\\nAs\\nLLMs integrate more into complex systems, examining their\\nsecurity properties becomes crucial, given the emerging field\\nof adversarial attacks on LLMs within trustworthy ML [488].\\nThis vulnerability is notable in safety-critical domains, ne-\\ncessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 33}, page_content='cessitating robust adversarial evaluation tools to ensure LLM\\nreliability [489].\\nInterpretability and Explainability: The “black-box” nature\\nof LLMs poses challenges in understanding their decision-\\nmaking, which is crucial for broader acceptance and trust,\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='especially in sensitive domains.\\nDespite their advanced\\ncapabilities, the lack of insight into their operation limits their\\neffectiveness and trustworthiness [490, 491]. Efforts are being\\nmade to make LLMs more explainable to promote user trust\\nand to ensure responsible AI usage. Understanding the logic\\nbehind LLMs’ responses is essential for fostering trust and\\nensuring they align with human values and legal standards.\\nPrivacy Concerns:\\nPrivacy concerns in Large Language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='Privacy Concerns:\\nPrivacy concerns in Large Language\\nModels (LLMs) have escalated with their growth in complexity\\nand size, particularly around data sharing and potential misuse.\\nThere is a risk of malicious content creation, filter bypass,\\nand data privacy issues, especially in e-commerce, where\\nprotecting customer privacy is crucial. If models are trained\\non private data, additional concerns arise if such models are\\nmade publicly available. LLMs tend to memorize phrases from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='made publicly available. LLMs tend to memorize phrases from\\ntheir training sets, which an adversary could exploit to extract\\nsensitive data, posing a threat to personal privacy [492, 493].\\nReal-Time Processing: Real-time processing in Large Lan-\\nguage Models (LLMs) is pivotal for various applications,\\nespecially with the rising popularity of mobile AI applications\\nand concerns regarding information security and privacy.\\nHowever, LLMs often have hundreds of layers and millions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='However, LLMs often have hundreds of layers and millions\\nof parameters, which impede real-time processing due to the\\nhigh computational demands and limited weight storage on\\nhardware platforms, particularly in edge computing environ-\\nments [494].\\nWhile certain efforts like MobileBERT aim\\nto reduce memory requirements, they still face substantial\\nexecution overhead due to the large number of model layers,\\nleading to high inference latency.\\nLong-Term Dependencies:\\nLarge Language Models have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='leading to high inference latency.\\nLong-Term Dependencies:\\nLarge Language Models have\\nshown considerable progress in understanding and generating\\ntext, yet they often struggle with preserving context and\\nhandling long-term dependencies, particularly in complex,\\nmulti-turn conversations or long documents. This limitation\\ncan lead to incoherent or irrelevant responses.\\nHardware Acceleration: The growth of LLMs presents signif-\\nicant hardware challenges due to the increasing computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='icant hardware challenges due to the increasing computational\\nand memory demands associated with training and deploying\\nthese models. GPUs have played a crucial role in meeting the\\nhardware requirements for training LLMs, with the networking\\nindustry also evolving to optimize hardware for training\\nworkloads. However, the growing size of LLMs, which has\\nbeen outpacing hardware progress, makes model inference in-\\ncreasingly costly. Model quantization is a promising approach'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='creasingly costly. Model quantization is a promising approach\\nto bridge the widening gap between LLM size and hardware\\ncapacity [495].\\nAlthough specialized hardware acceleration\\nlike GPUs or TPUs can significantly reduce the computational\\ncost, making real-time applications more feasible, they may not\\nfully resolve all limitations, necessitating further advancements\\nin hardware technology.\\nRegulatory and Ethical Frameworks: The rapid advancements'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='in hardware technology.\\nRegulatory and Ethical Frameworks: The rapid advancements\\nin artificial intelligence have given rise to sophisticated Large\\nLanguage Models (LLMs) like OpenAI’s GPT-4 [157] and\\nGoogle’s Bard. These developments underscore the imperative\\nfor regulatory oversight to manage the ethical and social\\nchallenges accompanying LLMs’ widespread use [496]. For\\ninstance, LLMs can generate content that can be used posi-\\ntively or negatively, emphasizing the need for proactive ethical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='tively or negatively, emphasizing the need for proactive ethical\\nframeworks and policy measures to guide their responsible\\nuse and assign accountability for their outputs [497]. Auditing\\nis identified as a promising governance mechanism to ensure\\nthat AI systems, including LLMs, are designed and deployed\\nethically, legally, and technically robust [498].\\n8. Conclusion\\nThis article has comprehensively reviewed the develop-\\nments in LLMs.\\nIt contributes to summarizing significant'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='ments in LLMs.\\nIt contributes to summarizing significant\\nfindings of LLMs in the existing literature and provides a\\ndetailed analysis of the design aspects, including architec-\\ntures, datasets, and training pipelines.\\nWe identified crucial\\narchitectural components and training strategies employed by\\ndifferent LLMs.\\nThese aspects are presented as summaries\\nand discussions throughout the article.\\nMoreover, we have\\ndiscussed the performance differences of LLMs in zero-shot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='Moreover, we have\\ndiscussed the performance differences of LLMs in zero-shot\\nand few-shot settings, explored the impact of fine-tuning, and\\ncompared supervised and generalized models and encoder vs.\\ndecoder vs. encoder-decoder architectures. A comprehensive\\nreview of multi-modal LLMs, retrieval augmented LLMs,\\nLLMs-powered agents, efficient LLMs, datasets, evaluation,\\napplications, and challenges is also provided. This article is\\nanticipated to serve as a valuable resource for researchers,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='anticipated to serve as a valuable resource for researchers,\\noffering insights into the recent advancements in LLMs and\\nproviding fundamental concepts and details to develop better\\nLLMs.\\nAcknowledgement:\\nThe author/s would like to acknowl-\\nedge the support received from Saudi Data and AI Authority\\n(SDAIA) and King Fahd University of Petroleum and Miner-\\nals (KFUPM) under SDAIA-KFUPM Joint Research Center for\\nArtificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='Artificial Intelligence Grant No. JRC-AI-RFP-11.\\nReferences\\n[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\\ntory” for natural language processing?, in:\\nMachine Learning and\\nKnowledge Discovery in Databases. Research Track: European Con-\\nference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\\nProceedings, Part III 21, Springer, 2021, pp. 677–693. 1\\n[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\\nO. Levy, S. Bowman, Superglue: A stickier benchmark for general-\\npurpose language understanding systems, Advances in neural informa-\\ntion processing systems 32 (2019). 1, 26, 29\\n[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\\nZ. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human-\\nlike open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\\n[4] B. A. y Arcas, Do large language models understand us?, Daedalus\\n151 (2) (2022) 183–197. 2\\n[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\\nLanguage models are unsupervised multitask learners, OpenAI blog\\n1 (8) (2019) 9. 2, 7\\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 34}, page_content='A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\\nare few-shot learners, Advances in neural information processing sys-\\ntems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\\n[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\\nof deep bidirectional transformers for language understanding, arXiv\\npreprint arXiv:1810.04805 (2018). 2, 18, 24\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nL. Zettlemoyer, Deep contextualized word representations, in: NAACL-\\nHLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\\n2\\n[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV. Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehen-\\nsion, arXiv preprint arXiv:1910.13461 (2019). 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='sion, arXiv preprint arXiv:1910.13461 (2019). 2\\n[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\\na unified text-to-text transformer, The Journal of Machine Learning Re-\\nsearch 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31\\n[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-\\ntext transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\\n25, 28, 30\\n[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,\\nJ. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\\nguage models, AI Open 2 (2021) 216–224. 2, 8, 25\\n[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\\nR. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\\nparameter open-access multilingual language model, arXiv preprint\\narXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30\\n[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\\nM. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer\\nlanguage models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24,\\n25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='25\\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\\ning language modeling with pathways, arXiv preprint arXiv:2204.02311\\n(2022). 2, 6, 9, 11, 23, 24, 25\\n[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li,\\nX. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\\nlanguage models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='language models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\\n17, 22, 24, 25, 28, 31\\n[17] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\\nprompted training enables zero-shot task generalization, arXiv preprint\\narXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31\\n[18] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\\nA. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\\nSuper-naturalinstructions: Generalization via declarative instructions on\\n1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\\n11, 16, 17, 24, 25, 28, 31\\n[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\\njishirzi, Self-instruct: Aligning language model with self generated in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='jishirzi, Self-instruct: Aligning language model with self generated in-\\nstructions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28\\n[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\\nels to follow instructions with human feedback, Advances in Neural In-\\nformation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\\n22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='22\\n[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\\nfoundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\\n(2023). 2, 7, 10, 16, 25, 34\\n[22] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\\ngatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\\nlarge language models, arXiv preprint arXiv:2206.07682 (2022). 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='large language models, arXiv preprint arXiv:2206.07682 (2022). 2\\n[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\\nlanguage models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\\n[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\\nentific research capabilities of large language models, arXiv preprint\\narXiv:2304.05332 (2023). 2\\n[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\\nretrieval augmented language models, arXiv preprint arXiv:2208.03299\\n(2022). 2, 18, 19, 34\\n[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\\nA. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\\nmultimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='multimodal language model, arXiv preprint arXiv:2303.03378 (2023).\\n2, 20, 22, 33\\n[27] A. Parisi, Y. Zhao, N. Fiedel, Talm: Tool augmented language models,\\narXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\\n[28] B. Zhang, H. Soh, Large language models as zero-shot human models\\nfor human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\\n33\\n[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi,\\nY. Shi, et al., mplug-owl: Modularization empowers large language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='Y. Shi, et al., mplug-owl: Modularization empowers large language\\nmodels with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\\n22\\n[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\\nT. Lu, J. Zhou, Y. Qiao, et al., Visionllm: Large language model\\nis also an open-ended decoder for vision-centric tasks, arXiv preprint\\narXiv:2305.11175 (2023). 2, 22\\n[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:\\nTeaching large language model to use tools via self-instruction, arXiv\\npreprint arXiv:2305.18752 (2023). 2, 19, 22, 23\\n[32] E.\\nSaravia,\\nPrompt\\nEngineering\\nGuide,\\nhttps://github.com/dair-\\nai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\\n[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\\nW. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\\nmodel, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25\\n[34] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5+:\\nOpen code large language models for code understanding and genera-\\ntion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25\\n[35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\\nY. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\\nedge enhanced pre-training for language understanding and generation,\\narXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25\\n[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: System op-\\ntimizations enable training deep learning models with over 100 billion\\nparameters, in: Proceedings of the 26th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\\n3506. 2, 5\\n[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimiza-\\ntions toward training trillion parameter models, in: SC20: International\\nConference for High Performance Computing, Networking, Storage and\\nAnalysis, IEEE, 2020, pp. 1–16. 2, 4, 24\\n[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\\na unified view of parameter-efficient transfer learning, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='a unified view of parameter-efficient transfer learning, arXiv preprint\\narXiv:2110.04366 (2021). 2, 20, 21\\n[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\\nria, Llm-adapters: An adapter family for parameter-efficient fine-tuning\\nof large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\\n20\\n[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\\nefficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\\n20, 21\\n[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\\ngeneration, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\\n[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\\nlarge language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\\n[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\\nFrom dense to sparse: Contrastive pruning for better pre-trained lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='From dense to sparse: Contrastive pruning for better pre-trained lan-\\nguage model compression, in: Proceedings of the AAAI Conference on\\nArtificial Intelligence, Vol. 36, 2022, pp. 11547–11555. 2, 22\\n[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\\nAccurate and efficient post-training quantization for large language\\nmodels, in: ICML, Vol. 202 of Proceedings of Machine Learning Re-\\nsearch, PMLR, 2023, pp. 38087–38099. 2, 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='search, PMLR, 2023, pp. 38087–38099. 2, 21\\n[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nCompression of generative pre-trained language models via quantiza-\\ntion, arXiv preprint arXiv:2203.10705 (2022). 2, 21\\n[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\\nGiraffe: Adventures in expanding context lengths in llms, arXiv preprint\\narXiv:2308.10882 (2023). 2, 17\\n[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\\nEfficient con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 35}, page_content='[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\\nEfficient con-\\ntext window extension of large language models, arXiv preprint\\narXiv:2309.00071 (2023). 2, 17\\n[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Longt5: Efficient text-to-text transformer for long sequences, arXiv\\npreprint arXiv:2112.07916 (2021). 2, 18\\n[49] S. Chen, S. Wong, L. Chen, Y. Tian, Extending context window\\nof large language models via positional interpolation, arXiv preprint\\narXiv:2306.15595 (2023). 2, 17\\n[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\\nJ. Zhang, Z. Dong, et al., A survey of large language models, arXiv\\npreprint arXiv:2303.18223 (2023). 2, 3, 7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='preprint arXiv:2303.18223 (2023). 2, 3, 7\\n[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\\nvey on word representation models: From classical to state-of-the-art\\nword representation language models, Transactions on Asian and Low-\\nResource Language Information Processing 20 (5) (2021) 1–35. 2, 3\\n[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\\nE. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='E. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\\ncessing via large pre-trained language models: A survey, arXiv preprint\\narXiv:2111.01243 (2021). 2, 3\\n[53] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He, et al., A comprehensive survey on pretrained foundation models:\\nA history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\\n2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='2, 3\\n[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\\narXiv:2301.00234 (2022). 2, 7, 18\\n[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\\nA survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\\n[56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\\nQ. Liu, Aligning large language models with human: A survey, arXiv\\npreprint arXiv:2307.12966 (2023). 2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='preprint arXiv:2307.12966 (2023). 2\\n[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, A survey on model compression\\nfor large language models, arXiv preprint arXiv:2308.07633 (2023). 2\\n[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\\nmodal large language models, arXiv preprint arXiv:2306.13549 (2023).\\n2, 22, 23\\n[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\\nING 1992 volume 4: The 14th international conference on computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='ING 1992 volume 4: The 14th international conference on computa-\\ntional linguistics, 1992. 4\\n[60] T. Kudo, Subword regularization: Improving neural network translation\\nmodels with multiple subword candidates, in: Proceedings of the 56th\\nAnnual Meeting of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), 2018, pp. 66–75. 4\\n[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\\nwords with subword units, in: Proceedings of the 54th Annual Meet-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='words with subword units, in: Proceedings of the 54th Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long\\nPapers), 2016, pp. 1715–1725. 4\\n[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\\nIEEE international conference on acoustics, speech and signal process-\\ning (ICASSP), IEEE, 2012, pp. 5149–5152. 4\\n[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\\nA. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='A. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-\\nacters: A brief history of open-vocabulary modeling and tokenization in\\nnlp, arXiv preprint arXiv:2112.10508 (2021). 4\\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\\ninformation processing systems 30 (2017). 4, 7\\n[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\\nlinear biases enables input length extrapolation, in: International Con-\\nference on Learning Representations, 2022.\\nURL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\\n[66] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: En-\\nhanced transformer with rotary position embedding, arXiv preprint\\narXiv:2104.09864 (2021). 4, 9, 17\\n[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\\nwith sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\\n23\\n[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness, Advances in Neural\\nInformation Processing Systems 35 (2022) 16344–16359. 4\\n[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\\nare universal approximators, Neural networks 2 (5) (1989) 359–366. 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='are universal approximators, Neural networks 2 (5) (1989) 359–366. 4\\n[70] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\\nmann machines, in: Proceedings of the 27th international conference on\\nmachine learning (ICML-10), 2010, pp. 807–814. 4\\n[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\\npreprint arXiv:1606.08415 (2016). 4\\n[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\\nDropout: a simple way to prevent neural networks from overfitting, The\\njournal of machine learning research 15 (1) (2014) 1929–1958. 4\\n[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\\nKe, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regular-\\nizing rnns by randomly preserving hidden activations, arXiv preprint\\narXiv:1606.01305 (2016). 4\\n[74] N.\\nShazeer,\\nGlu\\nvariants\\nimprove\\ntransformer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='arXiv:1606.01305 (2016). 4\\n[74] N.\\nShazeer,\\nGlu\\nvariants\\nimprove\\ntransformer,\\narXiv\\npreprint\\narXiv:2002.05202 (2020). 4\\n[75] Y. N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\\ngated convolutional networks, in: International conference on machine\\nlearning, PMLR, 2017, pp. 933–941. 4\\n[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\\narXiv:1607.06450 (2016). 4\\n[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\\nin Neural Information Processing Systems 32 (2019). 4\\n[78] A. Baevski, M. Auli, Adaptive input representations for neural language\\nmodeling, arXiv preprint arXiv:1809.10853 (2018). 4\\n[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\\ntransformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\\n[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\\nMegatron-lm: Training multi-billion parameter language models using\\nmodel parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\\n[81] \"bmtrain: Efficient training for big models.\".\\nURL https://github.com/OpenBMB/BMTrain 4, 5\\n[82] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\\ntac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='tac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\\nart natural language processing, in: Proceedings of the 2020 conference\\non empirical methods in natural language processing: system demon-\\nstrations, 2020, pp. 38–45. 5\\n[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\\nrin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\\nJax: composable transformations of python+ numpy programs (2018).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Jax: composable transformations of python+ numpy programs (2018).\\n5\\n[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You,\\nColossal-ai: A unified deep learning system for large-scale parallel train-\\ning, arXiv preprint arXiv:2110.14883 (2021). 5\\n[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe:\\nA\\nfast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\\n(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='(2021). 5\\n[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\\nwork, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\\n162. 5\\n[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\\native style, high-performance deep learning library, Advances in neural\\ninformation processing systems 32 (2019). 5\\n[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\\nS. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\\nscale machine learning., in: Osdi, Vol. 16, Savannah, GA, USA, 2016,\\npp. 265–283. 5\\n[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,\\nB. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and efficient machine\\nlearning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='learning library for heterogeneous distributed systems, arXiv preprint\\narXiv:1512.01274 (2015). 5\\n[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\\nlion parameter models with simple and efficient sparsity, The Journal of\\nMachine Learning Research 23 (1) (2022) 5232–5270. 5, 9\\n[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\\nY. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='Y. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\\nmodels with mixture-of-experts, in: International Conference on Ma-\\nchine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25\\n[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li,\\nX. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-P: Towards trillion\\nparameter language model with sparse heterogeneous computing, arXiv\\npreprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 36}, page_content='preprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\\n[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\\nJ. Launay, C. Raffel, What language model architecture and pretrain-\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='ing objective works best for zero-shot generalization?, in: International\\nConference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\\n[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou,\\nH.-W. Hon, Unified language model pre-training for natural language\\nunderstanding and generation, Advances in neural information process-\\ning systems 32 (2019). 6\\n[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\\nS. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\\nmodels, arXiv preprint arXiv:2001.08361 (2020). 6\\n[96] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\\net al., Training compute-optimal large language models, arXiv preprint\\narXiv:2203.15556 (2022). 6, 9, 25, 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='arXiv:2203.15556 (2022). 6, 9, 25, 29\\n[97] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\\nT. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\\nstruction meta learning through the lens of generalization, arXiv preprint\\narXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\\n[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan,\\nPrinciple-driven self-alignment of language models from scratch with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='Principle-driven self-alignment of language models from scratch with\\nminimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\\n7, 17\\n[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\\nN. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\\nas a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\\n7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='7\\n[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\\nP. Christiano, G. Irving, Fine-tuning language models from human pref-\\nerences, arXiv preprint arXiv:1909.08593 (2019). 7\\n[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\\ntion: Improving zero-shot and few-shot learning of language models via\\nchain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='chain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\\n7, 16\\n[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\\nining the power of symbolic tasks in instruction tuning, arXiv preprint\\narXiv:2304.07995 (2023). 7, 16\\n[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\\nD. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\\nlanguage models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='language models, Advances in Neural Information Processing Systems\\n35 (2022) 24824–24837. 7, 20, 23\\n[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\\nhery, D. Zhou, Self-consistency improves chain of thought reasoning in\\nlanguage models, arXiv preprint arXiv:2203.11171 (2022). 7, 20\\n[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. Narasimhan,\\nTree of thoughts: Deliberate problem solving with large language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='Tree of thoughts: Deliberate problem solving with large language mod-\\nels, arXiv preprint arXiv:2305.10601 (2023). 7, 20\\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\\ning for nlp, in: International Conference on Machine Learning, PMLR,\\n2019, pp. 2790–2799. 7, 20\\n[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\\nof large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\\n[108] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang,\\nK. Wang, X. Zhang, et al., Pangu-α : Large-scale autoregressive pre-\\ntrained chinese language models with auto-parallel computation, arXiv\\npreprint arXiv:2104.12369 (2021). 8, 23, 24, 25\\n[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\\nJ. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\\ntraining language models, AI Open 2 (2021) 65–68. 8, 30\\n[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY. Zhao, Y. Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\\npre-training for language understanding and generation, arXiv preprint\\narXiv:2107.02137 (2021). 8, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='arXiv:2107.02137 (2021). 8, 25\\n[111] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov,\\nTransformer-xl: Attentive language models beyond a fixed-length con-\\ntext, arXiv preprint arXiv:1901.02860 (2019). 8\\n[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\\nand evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\\n[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\\nficiencies of self-attention, Advances in Neural Information Processing\\nSystems 33 (2020) 22640–22651. 8, 11\\n[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\\nS. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\\nmodels bring?\\nintensive study on hyperclova: Billions-scale korean\\ngenerative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='generative pretrained transformers, arXiv preprint arXiv:2109.04650\\n(2021). 8, 25\\n[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\\nL. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\\nshot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\\n24, 25\\n[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='J. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\\nguage models: Methods, analysis & insights from training gopher, arXiv\\npreprint arXiv:2112.11446 (2021). 8, 9, 25, 28\\n[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al.,\\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a\\nlarge-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='large-scale generative language model, arXiv preprint arXiv:2201.11990\\n(2022). 8, 9, 24, 25\\n[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\\nH. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\\nsource autoregressive language model, arXiv preprint arXiv:2204.06745\\n(2022). 9, 23, 24, 25\\n[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\\nguage model (2021). 9\\n[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\\ncision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\\n[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\\nton, J. Dean, Outrageously large neural networks: The sparsely-gated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='ton, J. Dean, Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\\n[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\\natm 20b: Few-shot learning using a large-scale multilingual seq2seq\\nmodel, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25\\n[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\\narXiv preprint arXiv:2305.10403 (2023). 9, 25\\n[124] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia,\\nH. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\\nwith 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\\n24, 25\\n[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='24, 25\\n[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\\nChung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\\nguage learning paradigms, in: The Eleventh International Conference\\non Learning Representations, 2022. 9, 10, 24, 25\\n[126] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\\neral language model pretraining with autoregressive blank infilling, in:\\nProceedings of the 60th Annual Meeting of the Association for Compu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='Proceedings of the 60th Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. 10\\n[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\\nLlama: Open and efficient foundation language models, arXiv preprint\\narXiv:2302.13971 (2023). 10, 23, 25\\n[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\\npreprint arXiv:2112.05682 (2021). 10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='preprint arXiv:2112.05682 (2021). 10\\n[129] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\\nM. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\\ntransformer models, Proceedings of Machine Learning and Systems 5\\n(2023). 10\\n[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of\\nmodels, arXiv preprint arXiv:2407.21783 (2024). 10, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='models, arXiv preprint arXiv:2407.21783 (2024). 10, 25\\n[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25\\n[132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\\n25\\n[133] https://github.com/xai-org/grok-1. 10\\n[134] https://x.ai/blog/grok-1.5. 10\\n[135] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly\\ncapable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 37}, page_content='capable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\\n10\\n[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem-\\nini 1.5: Unlocking multimodal understanding across millions of tokens\\nof context, arXiv preprint arXiv:2403.05530 (2024). 10\\n[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun-\\ndyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b\\ntechnical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25\\n[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\\nQ. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\\nwith longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25\\n[139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,\\nC. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li,\\nF. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang,\\nH. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\\nJ. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao,\\nK. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li,\\nM. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang,\\nP. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge,\\nR. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye,\\nS. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\\nT. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao,\\nW. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen,\\nX. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical,\\nand efficient mixture-of-experts language model, CoRR abs/2405.04434\\n(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='(2024). 10, 25\\n[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,\\nC. Xiong, Codegen: An open large language model for code with multi-\\nturn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11,\\n23, 25, 28\\n[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\\nwards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\\n11, 25, 29, 31\\n[142] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\\ncode generation with alphacode, Science 378 (6624) (2022) 1092–1097.\\n11, 23, 25, 29\\n[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\\narXiv preprint arXiv:1911.02150 (2019). 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='arXiv preprint arXiv:1911.02150 (2019). 11\\n[144] R. Y. Pang, H. He, Text generation by learning from demonstrations,\\narXiv preprint arXiv:2009.07839 (2020). 11\\n[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine\\ntranslation models, arXiv preprint arXiv:2009.09372 (2020). 11\\n[146] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\\npre-trained encoder-decoder models for code understanding and genera-\\ntion, arXiv preprint arXiv:2109.00859 (2021). 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='tion, arXiv preprint arXiv:2109.00859 (2021). 11\\n[147] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\\nwith you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25\\n[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\\nA. Poulton, V. Kerkez, R. Stojnic, Galactica: A large language model for\\nscience, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='science, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\\n[149] FairScale authors, Fairscale: A general purpose modular pytorch library\\nfor high performance and large scale training, https://github.com/\\nfacebookresearch/fairscale (2021). 11\\n[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\\nCheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models\\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='for dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\\n[151] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\\nP. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\\nmodel for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33\\n[152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\\ncial chat model with hundreds of billions parameters, arXiv preprint\\narXiv:2305.12002 (2023). 11, 17, 25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='arXiv:2305.12002 (2023). 11, 17, 25\\n[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\\nformer language model with jax (2021). 12, 24\\n[154] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\\nT. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\\nCrosslingual generalization through multitask finetuning, arXiv preprint\\narXiv:2211.01786 (2022). 16, 25, 28, 31\\n[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\\nDynosaur: A dynamic growth paradigm for instruction-tuning data cu-\\nration, arXiv preprint arXiv:2305.14327 (2023). 16\\n[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\\nC. He, X. Yue, et al., Llama-adapter v2: Parameter-efficient visual in-\\nstruction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\\n[157] Openai. gpt-4 technical report (2023). 16, 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[157] Openai. gpt-4 technical report (2023). 16, 35\\n[158] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\\nT. B. Hashimoto, Stanford alpaca:\\nAn instruction-following llama\\nmodel,\\nhttps://github.com/tatsu-lab/stanford_alpaca\\n(2023). 16, 25, 28\\n[159] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\\nS. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\\n2023).\\nURL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\\n28\\n[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\\narXiv preprint arXiv:2304.03277 (2023). 16, 28\\n[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\\narithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\\n[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprint\\narXiv:2304.06975 (2023). 16\\n[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\\nWizardlm: Empowering large language models to follow complex in-\\nstructions, arXiv preprint arXiv:2304.12244 (2023). 16\\n[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\\nD. Jiang, Wizardcoder: Empowering code large language models with\\nevol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\\n[165] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick,\\nM. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\\ning language models to support answers with verified quotes, arXiv\\npreprint arXiv:2203.11147 (2022). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='preprint arXiv:2203.11147 (2022). 17\\n[166] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-\\nassisted question-answering with human feedback, arXiv preprint\\narXiv:2112.09332 (2021). 17, 19, 20, 25, 31\\n[167] A. Glaese, N. McAleese, M. Tr˛ebacz, J. Aslanides, V. Firoiu, T. Ewalds,\\nM. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\\nalignment of dialogue agents via targeted human judgements, arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='alignment of dialogue agents via targeted human judgements, arXiv\\npreprint arXiv:2209.14375 (2022). 17, 20, 25\\n[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\\nDirect preference optimization: Your language model is secretly a re-\\nward model, arXiv preprint arXiv:2305.18290 (2023). 17\\n[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\\nT. Zhang, Raft: Reward ranked finetuning for generative foundation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='T. Zhang, Raft: Reward ranked finetuning for generative foundation\\nmodel alignment, arXiv preprint arXiv:2304.06767 (2023). 17\\n[170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\\nresponses to align language models with human feedback without tears,\\narXiv preprint arXiv:2304.05302 (2023). 17\\n[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, H. Wang, Preference rank-\\ning optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='ing optimization for human alignment, arXiv preprint arXiv:2306.17492\\n(2023). 17\\n[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\\ntuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\\n17\\n[173] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\\nA. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\\nlessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='lessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\\n[174] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\\nP. Liang,\\nT. B. Hashimoto,\\nAlpacafarm:\\nA simulation frame-\\nwork for methods that learn from human feedback, arXiv preprint\\narXiv:2305.14387 (2023). 17\\n[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\\nPrompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 38}, page_content='Prompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\\n17\\n[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯ut˙e, A. Chen,\\nA. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\\nity for moral self-correction in large language models, arXiv preprint\\narXiv:2302.07459 (2023). 17\\n[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\\ntraining fail?, arXiv preprint arXiv:2307.02483 (2023). 17\\n39'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath,\\nB. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\\nguage models to reduce harms: Methods, scaling behaviors, and lessons\\nlearned, arXiv preprint arXiv:2209.07858 (2022). 17, 28\\n[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\\nlish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='lish, exploit: Red teaming language models from scratch, arXiv preprint\\narXiv:2306.09442 (2023). 17\\n[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\\nN. McAleese, G. Irving, Red teaming language models with language\\nmodels, arXiv preprint arXiv:2202.03286 (2022). 17\\n[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\\ncontinual learners, in: Proceedings of the 2022 Conference on Empirical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='continual learners, in: Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing, 2022, pp. 6107–6122. 17\\n[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\\ntuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\\n[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\\nC. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='C. Baral, Instruction tuned models are quick learners, arXiv preprint\\narXiv:2306.05539 (2023). 17\\n[184] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong,\\nJ. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\\nof low training data instruction tuning, arXiv preprint arXiv:2305.09246\\n(2023). 17\\n[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\\nP. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\\narXiv:2305.11206 (2023). 17, 25, 28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='arXiv:2305.11206 (2023). 17, 25, 28\\n[186] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm-infinite: Sim-\\nple on-the-fly length generalization for large language models, arXiv\\npreprint arXiv:2308.16137 (2023). 17, 18\\n[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan-\\nskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay, et al., Colt5: Faster\\nlong-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='long-range transformers with conditional computation, arXiv preprint\\narXiv:2303.09752 (2023). 18\\n[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\\nLongnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\\narXiv:2307.02486 (2023). 18\\n[189] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\\ncient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='cient fine-tuning of long-context large language models, arXiv preprint\\narXiv:2309.12307 (2023). 18\\n[190] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend,\\nE. Karpas, A. Shashua, K. Leyton-Brown, Y. Shoham, Parallel context\\nwindows for large language models, in: Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), 2023, pp. 6383–6402. 18\\n[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\\nAugmenting language models with long-term memory, arXiv preprint\\narXiv:2306.07174 (2023). 18\\n[192] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, S. Wang, Long\\ntime no see! open-domain conversation with long-term persona memory,\\narXiv preprint arXiv:2203.05797 (2022). 18\\n[193] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\\nImproving language models by retrieving from trillions of tokens, in:\\nInternational conference on machine learning, PMLR, 2022, pp. 2206–\\n2240. 18, 19, 34\\n[194] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank:\\nEnhanc-\\ning large language models with long-term memory, arXiv preprint\\narXiv:2305.10250 (2023). 18\\n[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\\nReflexion: Language agents with verbal reinforcement learning, arXiv\\npreprint arXiv:2303.11366 14 (2023). 18, 20\\n[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\\ning llms with databases as their symbolic memory, arXiv preprint\\narXiv:2306.03901 (2023). 18\\n[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, G. Neubig, Active retrieval augmented generation, arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='J. Callan, G. Neubig, Active retrieval augmented generation, arXiv\\npreprint arXiv:2305.06983 (2023). 18\\n[198] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, Y. Shoham, In-context retrieval-augmented language models,\\narXiv preprint arXiv:2302.00083 (2023). 18, 34\\n[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\\nimprove with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='improve with memory-of-thoughts, arXiv preprint arXiv:2305.05181\\n(2023). 18\\n[200] D. Schuurmans, Memory augmented large language models are compu-\\ntationally universal, arXiv preprint arXiv:2301.04589 (2023). 18\\n[201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\\ngeneral read-write memory for large language models, arXiv preprint\\narXiv:2305.14322 (2023). 18\\n[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\\nwork: Bm25 and beyond, Foundations and Trends® in Information Re-\\ntrieval 3 (4) (2009) 333–389. 18\\n[203] X. Wang,\\nJ. Wei,\\nD. Schuurmans,\\nQ. Le,\\nE. Chi,\\nD. Zhou,\\nRationale-augmented ensembles in language models, arXiv preprint\\narXiv:2207.00747 (2022). 18\\n[204] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-G. Lou, W. Chen,\\nRepocoder: Repository-level code completion through iterative retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='Repocoder: Repository-level code completion through iterative retrieval\\nand generation, arXiv preprint arXiv:2303.12570 (2023). 18\\n[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\\nO. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study, arXiv preprint\\narXiv:2304.06762 (2023). 19\\n[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\\nlarge language models, arXiv preprint arXiv:2307.07164 (2023). 19\\n[207] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What makes\\ngood in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\\n(2021). 19\\n[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\\ncontext learning, arXiv preprint arXiv:2112.08633 (2021). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='context learning, arXiv preprint arXiv:2112.08633 (2021). 19\\n[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\\nmodels, arXiv preprint arXiv:2301.12652 (2023). 19\\n[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\\narXiv preprint arXiv:2306.13421 (2023). 19\\n[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\\nlanguage model pre-training, in: International conference on machine\\nlearning, PMLR, 2020, pp. 3929–3938. 19\\n[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: Efficient and ef-\\nfective retrieval-augmented text generation, in: Proceedings of the 46th\\nInternational ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, 2023, pp. 1437–1447. 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='Information Retrieval, 2023, pp. 1437–1447. 19\\n[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\\nation, arXiv preprint arXiv:2107.07566 (2021). 19\\n[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\\naugmented language models through few-shot prompting for open-\\ndomain question answering, arXiv preprint arXiv:2203.05115 (2022).\\n19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='19\\n[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\\ngpt: A general multi-modal assistant that can plan, execute, inspect, and\\nlearn, arXiv preprint arXiv:2306.08640 (2023). 19\\n[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\\nJ. Gao, Chameleon: Plug-and-play compositional reasoning with large\\nlanguage models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23\\n[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\\nRibeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\\nguage models, arXiv preprint arXiv:2303.09014 (2023). 19\\n[218] C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y. Fujii, A. Ratner, C.-Y. Lee, R. Kr-\\nishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\\nlarge language models, arXiv preprint arXiv:2308.00675 (2023). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='large language models, arXiv preprint arXiv:2308.00675 (2023). 19\\n[219] Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt:\\nConnecting large language models with real-world applications via rest-\\nful apis, arXiv preprint arXiv:2306.06624 (2023). 19\\n[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\\nguage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='guage models with massive tools via tool embeddings, arXiv preprint\\narXiv:2305.11554 (2023). 19\\n[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\\nmodel connected with massive apis, arXiv preprint arXiv:2305.15334\\n(2023). 19\\n[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\\nlation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 39}, page_content='lation capability of open-source large language models, arXiv preprint\\narXiv:2305.16504 (2023). 19\\n[223] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,\\nB. Qian, et al., Toolllm: Facilitating large language models to master\\n16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\\n40'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='20\\n[224] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv-\\ning ai tasks with chatgpt and its friends in huggingface, arXiv preprint\\narXiv:2303.17580 (2023). 19, 20, 33\\n[225] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji,\\nS. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\\ndation models with millions of apis, arXiv preprint arXiv:2303.16434\\n(2023). 19\\n[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='(2023). 19\\n[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\\nexecution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\\n[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\\nuser assistance systems, Business & Information Systems Engineering\\n58 (2016) 367–370. 20\\n[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\\n134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='134 (1-2) (2002) 57–83. 20\\n[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\\nS. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\\nmulti-agent collaborative framework, arXiv preprint arXiv:2308.00352\\n(2023). 20\\n[230] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou, et al., The rise and potential of large language model\\nbased agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\\n[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\\nX. Chen, Y. Lin, et al., A survey on large language model based au-\\ntonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20\\n[232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\\nshot planners: Extracting actionable knowledge for embodied agents,\\nin: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='in: International Conference on Machine Learning, PMLR, 2022, pp.\\n9118–9147. 20\\n[233] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\\ning with language model is planning with world model, arXiv preprint\\narXiv:2305.14992 (2023). 20, 33\\n[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy,\\nZ. Chen, J. Zhang, D. Arpit, et al., Retroformer:\\nRetrospective\\nlarge language agents with policy gradient optimization, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='Retrospective\\nlarge language agents with policy gradient optimization, arXiv preprint\\narXiv:2308.02151 (2023). 20, 33\\n[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson,\\nN. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\\nlogue: Embodied reasoning through planning with language models, in:\\n6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='6th Annual Conference on Robot Learning, 2022.\\nURL https://openreview.net/forum?id=3R3Pz5i0tye 20\\n[236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\\nEmbodied finetuning for vision-language reasoning in robot manipula-\\ntion, arXiv preprint arXiv:2305.18898 (2023). 20, 33\\n[237] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\\nJ. Thomason, A. Garg, Progprompt: Generating situated robot task plans'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='J. Thomason, A. Garg, Progprompt: Generating situated robot task plans\\nusing large language models, in: 2023 IEEE International Conference on\\nRobotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20,\\n33\\n[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\\nChiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\\nfor robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='for robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\\n[239] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein,\\nMedagents: Large language models as collaborators for zero-shot med-\\nical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20\\n[240] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\\nJ. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\\nGrounding language in robotic affordances, in: Conference on Robot'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='Grounding language in robotic affordances, in: Conference on Robot\\nLearning, PMLR, 2023, pp. 287–318. 20, 33\\n[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\\nguided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\\n20\\n[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\\nnav: Grounding large language models for dynamic planning to navi-\\ngation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='gation in new environments, arXiv preprint arXiv:2309.04077 (2023).\\n20\\n[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su,\\nLlm-planner: Few-shot grounded planning for embodied agents with\\nlarge language models, arXiv preprint arXiv:2212.04088 (2022). 20\\n[244] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\\nyour\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\\npreprint arXiv:2303.03480 (2023). 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='preprint arXiv:2303.03480 (2023). 20\\n[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\\nrobot navigation, in: 2023 IEEE International Conference on Robotics\\nand Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\\n[246] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\\nwith large language models for object rearrangement, arXiv preprint\\narXiv:2303.06247 (2023). 20, 33\\n[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\\nstands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\\n[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-efficient tun-\\ning: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\\n20\\n[249] Y. Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\\nAdamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\\nguage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='guage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\\n[250] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\\nW. Chen, Lora: Low-rank adaptation of large language models, arXiv\\npreprint arXiv:2106.09685 (2021). 21, 22, 23\\n[251] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks, in: Pro-\\nceedings of the 60th Annual Meeting of the Association for Computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='ceedings of the 60th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 2: Short Papers), 2022, pp. 61–68. 21\\n[252] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\\nProgressive prompts: Continual learning for language models, arXiv\\npreprint arXiv:2301.12314 (2023). 21\\n[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\\nwards adaptive prefix tuning for parameter-efficient language model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='wards adaptive prefix tuning for parameter-efficient language model\\nfine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\\n[254] E. B. Zaken, S. Ravfogel, Y. Goldberg, Bitfit:\\nSimple parameter-\\nefficient fine-tuning for transformer-based masked language-models,\\narXiv preprint arXiv:2106.10199 (2021). 21\\n[255] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8 ():\\n8-bit matrix multiplication for transformers at scale, arXiv preprint\\narXiv:2208.07339 (2022). 21, 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='arXiv:2208.07339 (2022). 21, 22\\n[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq:\\nAccurate\\npost-training quantization for generative pre-trained transformers, arXiv\\npreprint arXiv:2210.17323 (2022). 21\\n[257] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\\npression+: Accurate quantization of large language models by equiva-\\nlent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='lent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\\n(2023). 21\\n[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\\naccurate post-training quantization and pruning, Advances in Neural In-\\nformation Processing Systems 35 (2022) 4475–4488. 21\\n[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, arXiv\\npreprint arXiv:2306.02272 (2023). 21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='preprint arXiv:2306.02272 (2023). 21\\n[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\\nW. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\\nefficient adaptation of large-scale pre-trained language models, arXiv\\npreprint arXiv:2210.03858 (2022). 21\\n[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient\\nfinetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='finetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\\n21, 22\\n[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr-\\nishnamoorthi, V. Chandra, Llm-qat: Data-free quantization aware train-\\ning for large language models, arXiv preprint arXiv:2305.17888 (2023).\\n21, 22\\n[263] Y. Guo, A. Yao, H. Zhao, Y. Chen, Network sketching: Exploiting bi-\\nnary structure in deep cnns, in: Proceedings of the IEEE Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='nary structure in deep cnns, in: Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\\n[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\\nMemory-efficient fine-tuning of compressed large language models via\\nsub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\\n22\\n[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and effective pruning\\napproach for large language models, arXiv preprint arXiv:2306.11695'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 40}, page_content='approach for large language models, arXiv preprint arXiv:2306.11695\\n(2023). 22\\n[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\\nmodels, arXiv preprint arXiv:1910.04732 (2019). 22\\n41'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[267] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,\\nY. Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\\nmissing secret sauce for pruning llms to high sparsity, arXiv preprint\\narXiv:2310.05175 (2023). 22\\n[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\\nStructured pruning for efficient generative pre-trained language models,\\nin: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='in: Findings of the Association for Computational Linguistics: ACL\\n2023, 2023, pp. 10880–10895. 22\\n[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,\\nA. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\\nguage model for few-shot learning, Advances in Neural Information Pro-\\ncessing Systems 35 (2022) 23716–23736. 22\\n[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models,\\narXiv preprint arXiv:2301.12597 (2023). 22\\n[271] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv preprint\\narXiv:2304.08485 (2023). 22\\n[272] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang,\\nY. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\\narXiv:2305.06355 (2023). 22\\n[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\\ntailed video understanding via large vision and language models, arXiv\\npreprint arXiv:2306.05424 (2023). 22\\n[274] H. Zhang,\\nX. Li,\\nL. Bing,\\nVideo-llama:\\nAn instruction-tuned\\naudio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='audio-visual language model for video understanding, arXiv preprint\\narXiv:2306.02858 (2023). 22\\n[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\\nY. Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\\ndio captioning dataset for audio-language multimodal research, arXiv\\npreprint arXiv:2303.17395 (2023). 22\\n[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\\nllm: Multi-modal language modeling with image, audio, video, and text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='llm: Multi-modal language modeling with image, audio, video, and text\\nintegration, arXiv preprint arXiv:2306.09093 (2023). 22\\n[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\\nvision-language understanding with advanced large language models,\\narXiv preprint arXiv:2304.10592 (2023). 22\\n[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\\nAn image is worth 16x16 words: Transformers for image recognition at\\nscale, arXiv preprint arXiv:2010.11929 (2020). 22\\n[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\\nS. Hoi, Instructblip: Towards general-purpose vision-language models\\nwith instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\\n[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\\nshot learning via instruction tuning, arXiv preprint arXiv:2212.10773\\n(2022). 22\\n[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\\nChatbridge: Bridging modalities with large language model as a lan-\\nguage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\\n[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,\\nX. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\\nlingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\\n[283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\\nH. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\\narXiv preprint arXiv:2305.14167 (2023). 22\\n[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\\nEfficient vision-language instruction tuning for large language models,\\narXiv preprint arXiv:2305.15023 (2023). 22\\n[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y. Qiao,\\nLlama-adapter: Efficient fine-tuning of language models with zero-init\\nattention, arXiv preprint arXiv:2303.16199 (2023). 22\\n[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\\nRobust speech recognition via large-scale weak supervision, in: Inter-\\nnational Conference on Machine Learning, PMLR, 2023, pp. 28492–\\n28518. 22\\n[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\\nmodal chain-of-thought reasoning in language models, arXiv preprint\\narXiv:2302.00923 (2023). 23\\n[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt\\ntuning in vision language models, arXiv preprint arXiv:2304.07919\\n(2023). 23\\n[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\\ning, drawing and editing with visual foundation models, arXiv preprint\\narXiv:2303.04671 (2023). 23\\n[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\\nM. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\\nsoning and action, arXiv preprint arXiv:2303.11381 (2023). 23\\n[291] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao,\\nS. Zhao, Y. Shan, et al., Caption anything: Interactive image descrip-\\ntion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\\n(2023). 23\\n[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\\nAdapting clip for powerful 3d open-world learning, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='Adapting clip for powerful 3d open-world learning, arXiv preprint\\narXiv:2211.11682 (2022). 23\\n[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\\nsoning without training, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\\n23\\n[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\\nfusion with intra-and inter-modality attention flow for visual question'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='fusion with intra-and inter-modality attention flow for visual question\\nanswering, in: Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition, 2019, pp. 6639–6648. 23\\n[295] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Deep modular co-attention net-\\nworks for visual question answering, in: Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition, 2019, pp. 6281–\\n6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='6290. 23\\n[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\\nW. Chang, S.-F. Chang, Idealgpt:\\nIteratively decomposing vision\\nand language reasoning via large language models, arXiv preprint\\narXiv:2305.14985 (2023). 23\\n[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li,\\nPrompt, generate, then cache: Cascade of foundation models makes\\nstrong few-shot learners, in: Proceedings of the IEEE/CVF Conference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='strong few-shot learners, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\\n23\\n[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\\nnormalization of self-attention, CoRR abs/1910.05895 (2019). 24\\n[299] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-\\ntraining approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='training approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\\n[300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nD. Song, Koala: A dialogue model for academic research, Blog post\\n(April 2023).\\nURL\\nhttps://bair.berkeley.edu/blog/2023/04/03/koala/\\n25\\n[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\\nJ. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile:\\nAn\\n800gb dataset of diverse text for language modeling, arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='An\\n800gb dataset of diverse text for language modeling, arXiv preprint\\narXiv:2101.00027 (2020). 28, 30\\n[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\\nT. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen,\\net al., The bigscience roots corpus: A 1.6 tb composite multilingual\\ndataset, Advances in Neural Information Processing Systems 35 (2022)\\n31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='31809–31826. 28\\n[303] Wikipedia.\\nURL https://en.wikipedia.org/wiki/Main_Page 28\\n[304] Together Computer, Redpajama: An open source recipe to reproduce\\nllama training dataset (Apr. 2023).\\nURL\\nhttps://github.com/togethercomputer/\\nRedPajama-Data 28\\n[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\\nTuning language models with (almost) no human labor, arXiv preprint\\narXiv:2212.09689 (2022). 28\\n[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='arXiv:2212.09689 (2022). 28\\n[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\\nD. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\\nharmless assistant with reinforcement learning from human feedback,\\narXiv preprint arXiv:2204.05862 (2022). 28\\n[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\\nJ. Steinhardt, Measuring massive multitask language understanding,\\narXiv preprint arXiv:2009.03300 (2020). 26, 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 41}, page_content='arXiv preprint arXiv:2009.03300 (2020). 26, 29\\n[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\\n42'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='the imitation game: Quantifying and extrapolating the capabilities of\\nlanguage models, arXiv preprint arXiv:2206.04615 (2022). 26, 29\\n[309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\\nA multi-task benchmark and analysis platform for natural language un-\\nderstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29\\n[310] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\\nJ. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='J. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\\neration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\\n29\\n[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu,\\nC. Yu, et al., Clue: A chinese language understanding evaluation bench-\\nmark, arXiv preprint arXiv:2004.05986 (2020). 29\\n[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\\nX. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='X. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\\nbenchmark, arXiv preprint arXiv:2107.07498 (2021). 29\\n[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y.-L. Boureau, Can\\nyou put it all together: Evaluating conversational agents’ ability to blend\\nskills, arXiv preprint arXiv:2004.08449 (2020). 29\\n[314] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\\nY. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of\\nlanguage models, arXiv preprint arXiv:2211.09110 (2022). 29\\n[315] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\\nY. Song, T. Oh, et al., Klue: Korean language understanding evaluation,\\narXiv preprint arXiv:2105.09680 (2021). 29\\n[316] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\\nanswering challenge, Transactions of the Association for Computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='answering challenge, Transactions of the Association for Computational\\nLinguistics 7 (2019) 249–266. 27, 29\\n[317] M.\\nT.\\nPilehvar,\\nJ.\\nCamacho-Collados,\\nWic:\\n10,000\\nexample\\npairs for evaluating context-sensitive representations, arXiv preprint\\narXiv:1808.09121 6 (2018). 27, 29\\n[318] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\\nmodels, arXiv preprint arXiv:1609.07843 (2016). 28, 29\\n[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\\nsive transformers for long-range sequence modelling, arXiv preprint\\narXiv:1911.05507 (2019). 28, 29\\n[320] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\\nlarge-scale chinese question matching corpus, in: Proceedings of the\\n27th international conference on computational linguistics, 2018, pp.\\n1952–1962. 28, 29\\n[321] S.\\nIyer,\\nN.\\nDandekar,\\nK.\\nCsernai,\\nFirst\\nquora\\ndataset\\nre-\\nlease:\\nQuestion\\npairs,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[321] S.\\nIyer,\\nN.\\nDandekar,\\nK.\\nCsernai,\\nFirst\\nquora\\ndataset\\nre-\\nlease:\\nQuestion\\npairs,\\nhttps://quoradata.quora.com/\\nFirst-Quora-Dataset-Release-Question-Pairs. 29\\n[322] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\\ncoreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\\n[323] M.-C. De Marneffe, M. Simons, J. Tonhauser, The commitmentbank: In-\\nvestigating projection in naturally occurring discourse, in: proceedings'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='vestigating projection in naturally occurring discourse, in: proceedings\\nof Sinn und Bedeutung, Vol. 23, 2019, pp. 107–124. 29\\n[324] Z. Li, N. Ding, Z. Liu, H. Zheng, Y. Shen, Chinese relation extraction\\nwith multi-grained information and external linguistic knowledge, in:\\nProceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics, 2019, pp. 4377–4386. 29\\n[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\\nand relation extraction dataset for chinese literature text, arXiv preprint\\narXiv:1711.07010 (2017). 29\\n[326] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\\nlarge-scale domain-specific chinese corpus for sentence semantic equiv-\\nalence identification, in: Proceedings of the 2018 conference on empiri-\\ncal methods in natural language processing, 2018, pp. 4946–4951. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='cal methods in natural language processing, 2018, pp. 4946–4951. 29\\n[327] B. Liu, D. Niu, H. Wei, J. Lin, Y. He, K. Lai, Y. Xu, Matching arti-\\ncle pairs with graphical decomposition and convolutions, arXiv preprint\\narXiv:1802.07459 (2018). 29\\n[328] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Dataset and neu-\\nral recurrent sequence labeling model for open-domain factoid question\\nanswering, arXiv preprint arXiv:1607.06275 (2016). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='answering, arXiv preprint arXiv:1607.06275 (2016). 29\\n[329] N. Peng, M. Dredze, Named entity recognition for chinese social media\\nwith jointly trained embeddings, in: Proceedings of the 2015 conference\\non empirical methods in natural language processing, 2015, pp. 548–\\n554. 29\\n[330] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\\nnale generation: Learning to solve and explain algebraic word problems,\\narXiv preprint arXiv:1705.04146 (2017). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='arXiv preprint arXiv:1705.04146 (2017). 29\\n[331] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\\ncus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\\nlease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\\ntium (2011). 29\\n[332] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\\ncomplex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\\n[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\\nin social media: A case study of african-american english, arXiv preprint\\narXiv:1608.08868 (2016). 29\\n[334] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\\nderwende, P. Kohli, J. Allen, A corpus and evaluation framework\\nfor deeper understanding of commonsense stories, arXiv preprint\\narXiv:1604.01696 (2016). 28, 29\\n[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\\nS. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\\nWord prediction requiring a broad discourse context, arXiv preprint\\narXiv:1606.06031 (2016). 28, 29\\n[336] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\\nrization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\\n[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\\nation with planning-based hierarchical variational model, arXiv preprint\\narXiv:1908.06605 (2019). 29\\n[338] J. Novikova, O. Dušek, V. Rieser, The e2e dataset: New challenges for\\nend-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\\n[339] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\\nfor cloze test, arXiv preprint arXiv:1906.01265 (2019). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='for cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\\n[340] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about phys-\\nical commonsense in natural language, in: Proceedings of the AAAI\\nconference on artificial intelligence, Vol. 34, 2020, pp. 7432–7439. 28,\\n29\\n[341] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\\ndistantly supervised challenge dataset for reading comprehension, arXiv\\npreprint arXiv:1705.03551 (2017). 28, 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='preprint arXiv:1705.03551 (2017). 28, 29, 31\\n[342] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nO. Tafjord, Think you have solved question answering? try arc, the ai2\\nreasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 28, 29,\\n31\\n[343] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost:\\nPhys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='Phys-\\nical reasoning of objects through space and time, arXiv preprint\\narXiv:2106.03634 (2021). 29\\n[344] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\\nduct electricity? a new dataset for open book question answering, arXiv\\npreprint arXiv:1809.02789 (2018). 29\\n[345] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\\nD. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\\nwebnlg+ shared task overview and evaluation results (webnlg+ 2020),'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='webnlg+ shared task overview and evaluation results (webnlg+ 2020),\\nin: Proceedings of the 3rd International Workshop on Natural Language\\nGeneration from the Semantic Web (WebNLG+), 2020. 29\\n[346] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\\nA chinese dataset for cant understanding with common sense and world\\nknowledge, arXiv preprint arXiv:2104.02704 (2021). 29\\n[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\\nLarge-scale'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\\nLarge-scale\\nreading comprehension dataset from examinations, arXiv preprint\\narXiv:1704.04683 (2017). 29\\n[348] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\\nL. Zettlemoyer, Quac: Question answering in context, arXiv preprint\\narXiv:1808.07036 (2018). 29\\n[349] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\\ntle use a laptop? a question answering benchmark with implicit reason-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='tle use a laptop? a question answering benchmark with implicit reason-\\ning strategies, Transactions of the Association for Computational Lin-\\nguistics 9 (2021) 346–361. 29, 31\\n[350] J. Boyd-Graber, B. Satinoff, H. He, H. Daumé III, Besting the quiz mas-\\nter: Crowdsourcing incremental classification games, in: Proceedings of\\nthe 2012 joint conference on empirical methods in natural language pro-\\ncessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 42}, page_content='cessing and computational natural language learning, 2012, pp. 1290–\\n1301. 29\\n[351] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\\nquestion answer matching using end-to-end character-level multi-scale\\ncnns, Applied Sciences 7 (8) (2017) 767. 29\\n[352] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\\nteraction networks for chinese medical question answer selection, IEEE\\n43'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Access 6 (2018) 74061–74071. 29\\n[353] C. Xu, J. Pei, H. Wu, Y. Liu, C. Li, Matinf: A jointly labeled large-scale\\ndataset for classification, question answering and summarization, arXiv\\npreprint arXiv:2004.12302 (2020). 29\\n[354] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y. Choi, Winogrande: An\\nadversarial winograd schema challenge at scale, Communications of the\\nACM 64 (9) (2021) 99–106. 27, 29\\n[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a\\nmachine really finish your sentence?, arXiv preprint arXiv:1905.07830\\n(2019). 29\\n[356] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\\nnatives: An evaluation of commonsense causal reasoning., in: AAAI\\nspring symposium: logical formalizations of commonsense reasoning,\\n2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='2011, pp. 90–95. 29\\n[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\\nlenge, in: Thirteenth international conference on the principles of knowl-\\nedge representation and reasoning, 2012. 27, 29\\n[358] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\\nanswering challenge targeting commonsense knowledge, arXiv preprint\\narXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='arXiv:1811.00937 (2018). 29, 31\\n[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:\\nCommonsense reasoning about social interactions, arXiv preprint\\narXiv:1904.09728 (2019). 29\\n[360] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\\nlenging chinese machine reading comprehension, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 141–155. 29\\n[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\\ning the gap between human and machine commonsense reading compre-\\nhension, arXiv preprint arXiv:1810.12885 (2018). 29\\n[362] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\\nfor machine comprehension of text, arXiv preprint arXiv:1606.05250\\n(2016). 29, 31\\n[363] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\\nK. Toutanova, Boolq: Exploring the surprising difficulty of natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='K. Toutanova, Boolq: Exploring the surprising difficulty of natural\\nyes/no questions, arXiv preprint arXiv:1905.10044 (2019). 29, 31\\n[364] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\\nable questions for squad, arXiv preprint arXiv:1806.03822 (2018). 29,\\n31\\n[365] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\\nA reading comprehension benchmark requiring discrete reasoning over\\nparagraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='paragraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\\n[366] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\\ntailment challenge, in: Machine learning challenges workshop, Springer,\\n2005, pp. 177–190. 29, 31\\n[367] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y. Bisk, Webqa: Mul-\\ntihop and multimodal qa, in: Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='on Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\\n29, 31\\n[368] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\\nevaluation on chinese machine reading comprehension, arXiv preprint\\narXiv:1709.08299 (2017). 29\\n[369] Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\\nA span-extraction dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:1810.07366 (2018). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='arXiv preprint arXiv:1810.07366 (2018). 29, 31\\n[370] Y. Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\\nA sentence cloze dataset for chinese machine reading comprehension,\\narXiv preprint arXiv:2004.03116 (2020). 29\\n[371] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, Y. Wang, Character-based bilstm-crf\\nincorporating pos and dictionaries for chinese opinion target extraction,\\nin: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='in: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\\n29\\n[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\\ning beyond the surface: A challenge set for reading comprehension\\nover multiple sentences, in: Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies, Volume 1 (Long Papers), 2018,\\npp. 252–262. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='tics: Human Language Technologies, Volume 1 (Long Papers), 2018,\\npp. 252–262. 29\\n[373] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\\nberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\\ntions: a benchmark for question answering research, Transactions of the\\nAssociation for Computational Linguistics 7 (2019) 453–466. 29\\n[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-\\nchine reading comprehension dataset, arXiv preprint arXiv:1806.00920\\n(2018). 29\\n[375] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu,\\nQ. She, et al., Dureader: a chinese machine reading comprehension\\ndataset from real-world applications, arXiv preprint arXiv:1711.05073\\n(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='(2017). 29\\n[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A\\nchinese dataset towards evaluating the robustness of machine reading\\ncomprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\\n[377] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\\nquestions, arXiv preprint arXiv:1707.06209 (2017). 29\\n[378] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\\nranking with kernel pooling, in: Proceedings of the 40th International'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='ranking with kernel pooling, in: Proceedings of the 40th International\\nACM SIGIR conference on research and development in information\\nretrieval, 2017, pp. 55–64. 29\\n[379] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, R. Morante,\\nQa4mre 2011-2013: Overview of question answering for machine read-\\ning evaluation, in: Information Access Evaluation. Multilinguality, Mul-\\ntimodality, and Visualization: 4th International Conference of the CLEF'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='timodality, and Visualization: 4th International Conference of the CLEF\\nInitiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\\nceedings 4, Springer, 2013, pp. 303–320. 29\\n[380] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\\nreading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\\n[381] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y. Feng, X. Han,\\nZ. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Z. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\\nment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\\n[382] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\\ncompetence with apps, arXiv preprint arXiv:2105.09938 (2021). 29, 31\\n[383] Y. Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\\nin: Proceedings of the 2017 conference on empirical methods in natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='in: Proceedings of the 2017 conference on empirical methods in natural\\nlanguage processing, 2017, pp. 845–854. 29, 31\\n[384] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\\nto solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\\n29, 31\\n[385] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='E. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with\\nlarge language models, CoRR abs/2108.07732 (2021). 29\\n[386] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W.\\nChung, Y. Tay, S. Ruder, D. Zhou, et al., Language models are mul-\\ntilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\\n(2022). 29\\n[387] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\\npreprint arXiv:1608.01413 (2016). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='preprint arXiv:1608.01413 (2016). 29\\n[388] S.-Y. Miao, C.-C. Liang, K.-Y. Su, A diverse corpus for evaluating\\nand developing english math word problem solvers, arXiv preprint\\narXiv:2106.15772 (2021). 29\\n[389] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\\nMawps: A math word problem repository, in: Proceedings of the 2016\\nconference of the north american chapter of the association for computa-\\ntional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='tional linguistics: human language technologies, 2016, pp. 1152–1157.\\n29\\n[390] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\\nsimple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\\n29\\n[391] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\\nD. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\\ndata science code generation, in: International Conference on Machine\\nLearning, PMLR, 2023, pp. 18319–18345. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='Learning, PMLR, 2023, pp. 18319–18345. 29\\n[392] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\\nlanguage models, arXiv preprint arXiv:2108.07732 (2021). 29\\n[393] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\\nsarial nli: A new benchmark for natural language understanding, arXiv\\npreprint arXiv:1910.14599 (2019). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 43}, page_content='preprint arXiv:1910.14599 (2019). 29, 31\\n[394] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\\ncorpus for sentence understanding through inference, arXiv preprint\\narXiv:1704.05426 (2017). 29\\n[395] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\\n44'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='nosing syntactic heuristics in natural language inference, arXiv preprint\\narXiv:1902.01007 (2019). 29\\n[396] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, Y. Zhang, Logiqa: A chal-\\nlenge dataset for machine reading comprehension with logical reason-\\ning, arXiv preprint arXiv:2007.08124 (2020). 29\\n[397] P. Lewis, B. O˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\\nuating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='uating cross-lingual extractive question answering, arXiv preprint\\narXiv:1910.07475 (2019). 29\\n[398] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\\nH. Schwenk, V. Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\\nresentations, arXiv preprint arXiv:1809.05053 (2018). 29, 31\\n[399] Y. Yang,\\nY. Zhang,\\nC. Tar,\\nJ. Baldridge,\\nPaws-x:\\nA cross-\\nlingual adversarial dataset for paraphrase identification, arXiv preprint\\narXiv:1908.11828 (2019). 29, 31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='arXiv:1908.11828 (2019). 29, 31\\n[400] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\\nsummary!, Topic-Aware Convolutional Neural Networks for Extreme\\nSummarization. ArXiv, abs (1808). 29\\n[401] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli´c, A. Korhonen,\\nXcopa: A multilingual dataset for causal commonsense reasoning, arXiv\\npreprint arXiv:2005.00333 (2020). 29\\n[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\\nas a baseline for cross-lingual transfer in commonsense reasoning, arXiv\\npreprint arXiv:2106.12066 (2021). 29\\n[403] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Niko-\\nlaev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\\ntion answering in typologically diverse languages, Transactions of the\\nAssociation for Computational Linguistics 8 (2020) 454–470. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='Association for Computational Linguistics 8 (2020) 454–470. 29\\n[404] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\\nMlsum:\\nThe multilingual summarization corpus,\\narXiv preprint\\narXiv:2004.14900 (2020). 29\\n[405] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\\nhuman falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29, 32\\n[406] I. Augenstein,\\nC. Lioma,\\nD. Wang,\\nL. C. Lima,\\nC. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc:\\nA real-world multi-domain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='D. Wang,\\nL. C. Lima,\\nC. Hansen,\\nC. Hansen, J. G. Simonsen, Multifc:\\nA real-world multi-domain\\ndataset for evidence-based fact checking of claims, arXiv preprint\\narXiv:1909.03242 (2019). 29\\n[407] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\\nlarge-scale dataset for fact extraction and verification, arXiv preprint\\narXiv:1803.05355 (2018). 29\\n[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\\nhate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\\n29, 32\\n[409] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\\nbias in pretrained language models, arXiv preprint arXiv:2004.09456\\n(2020). 29, 32\\n[410] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thomp-\\nson, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='son, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\\nquestion answering, arXiv preprint arXiv:2110.08193 (2021). 29\\n[411] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, K.-W. Chang, Gender bias\\nin coreference resolution: Evaluation and debiasing methods, arXiv\\npreprint arXiv:1804.06876 (2018). 29\\n[412] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\\nlenge dataset for measuring social biases in masked language models,\\narXiv preprint arXiv:2010.00133 (2020). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='arXiv preprint arXiv:2010.00133 (2020). 29\\n[413] S. Gehman, S. Gururangan, M. Sap, Y. Choi, N. A. Smith, Realtoxic-\\nityprompts: Evaluating neural toxic degeneration in language models,\\narXiv preprint arXiv:2009.11462 (2020). 29\\n[414] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\\nmetrics for measuring unintended bias with real data for text classifica-\\ntion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='tion, in: Companion proceedings of the 2019 world wide web confer-\\nence, 2019, pp. 491–500. 29\\n[415] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\\nM. Huck, A. J. Yepes, P. Koehn, V. Logacheva, C. Monz, et al., Find-\\nings of the 2016 conference on machine translation, in: Proceedings of\\nthe First Conference on Machine Translation: Volume 2, Shared Task\\nPapers, 2016, pp. 131–198. 29\\n[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-\\nman, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\\n2020 conference on machine translation (wmt20), in: Proceedings of\\nthe Fifth Conference on Machine Translation, Association for Compu-\\ntational Linguistics„ 2020, pp. 1–55. 29\\n[417] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\\nmatching dataset, arXiv preprint arXiv:2106.01979 (2021). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='matching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\\n[418] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\\nwikipedia: Knowledge-powered conversational agents, arXiv preprint\\narXiv:1811.01241 (2018). 29\\n[419] H. Rashkin, E. M. Smith, M. Li, Y.-L. Boureau, Towards empathetic\\nopen-domain conversation models: A new benchmark and dataset, arXiv\\npreprint arXiv:1811.00207 (2018). 29\\n[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,\\nD. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\\ntional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\\ntion: From Machine Learning to Intelligent Conversations, Springer,\\n2020, pp. 187–208. 29\\n[421] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\\nmulti-domain dialogue dataset towards multi-turn knowledge-driven\\nconversation, arXiv preprint arXiv:2004.04100 (2020). 29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='conversation, arXiv preprint arXiv:2004.04100 (2020). 29\\n[422] L. CO, Iflytek: a multiple categories chinese text classifier. competition\\nofficial website (2019). 29\\n[423] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\\npushshift reddit dataset, in: Proceedings of the international AAAI con-\\nference on web and social media, Vol. 14, 2020, pp. 830–839. 30\\n[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\\nform question answering, arXiv preprint arXiv:1907.09190 (2019). 31\\n[425] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\\nA. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\\nBenchmarking generalization via in-context instructions on 1,600+ lan-\\nguage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\\n[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\\nM. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\\ntasking structured knowledge grounding with text-to-text language mod-\\nels, arXiv preprint arXiv:2201.05966 (2022). 31\\n[427] Q. Ye, B. Y. Lin, X. Ren, Crossfit: A few-shot learning challenge\\nfor cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\\n(2021). 31\\n[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='(2021). 31\\n[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,\\nH. Zhuang, V. Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\\nmulti-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\\n(2021). 31\\n[429] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\\npus for sentence understanding through inference, in: Proceedings of\\nthe 2018 Conference of the North American Chapter of the Associ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='the 2018 Conference of the North American Chapter of the Associ-\\nation for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), Association for Computational Linguistics,\\nNew Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\\nN18-1101.\\nURL https://aclanthology.org/N18-1101 31\\n[430] Y. Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\\nscrambling, in: Proceedings of the 2019 Conference of the North Amer-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='scrambling, in: Proceedings of the 2019 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers), Associa-\\ntion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\\n1298–1308. doi:10.18653/v1/N19-1131.\\nURL https://aclanthology.org/N19-1131 32\\n[431] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\\nGPT a general-purpose natural language processing task solver?, in: The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='GPT a general-purpose natural language processing task solver?, in: The\\n2023 Conference on Empirical Methods in Natural Language Process-\\ning, 2023.\\nURL https://openreview.net/forum?id=u03xn1COsO 32\\n[432] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\\nN. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\\nprehensive survey of its applications, challenges, limitations, and future\\nprospects, TechRxiv (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='prospects, TechRxiv (2023). 32\\n[433] X. L. Dong, S. Moon, Y. E. Xu, K. Malik, Z. Yu, Towards next-\\ngeneration intelligent assistants leveraging llm techniques, in: Proceed-\\nings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, 2023, pp. 5792–5793. 32\\n[434] K. Pandya, M. Holia, Automating customer service using langchain:\\nBuilding custom open-source gpt chatbot for organizations, arXiv\\npreprint arXiv:2310.05421 (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 44}, page_content='preprint arXiv:2310.05421 (2023). 32\\n[435] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\\nR. Geng, et al., Can llm already serve as a database interface?\\na\\n45'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='big bench for large-scale database grounded text-to-sqls, arXiv preprint\\narXiv:2305.03111 (2023). 32\\n[436] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\\nchatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\\n2023–02. 32\\n[437] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\\nsir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\\nlanguage models for decision support in personalized oncology, JAMA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='language models for decision support in personalized oncology, JAMA\\nNetwork Open 6 (11) (2023) e2343689–e2343689. 32\\n[438] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\\nmaroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\\nploring the potential of chat-gpt as a supportive tool for sialendoscopy\\nclinical decision making and patient information support, European\\nArchives of Oto-Rhino-Laryngology (2023) 1–6. 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Archives of Oto-Rhino-Laryngology (2023) 1–6. 32\\n[439] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\\nData decentralisation of llm-based chatbot systems in chronic disease\\nself-management, in: Proceedings of the 2023 ACM Conference on In-\\nformation Technology for Social Good, 2023, pp. 205–212. 32\\n[440] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\\nhuman feedback for a therapy chatbot application (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='human feedback for a therapy chatbot application (2023). 32\\n[441] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\\nagents: A personalized llm-powered agent framework, arXiv preprint\\narXiv:2310.02374 (2023). 32\\n[442] K. V. Lemley, Does chatgpt help us understand the medical literature?,\\nJournal of the American Society of Nephrology (2023) 10–1681. 32\\n[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\\nnext-generation large language model (llm) or chatgpt is required for\\nbiomedical engineering and research, Annals of Biomedical Engineering\\n(2023) 1–4. 32\\n[444] Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\\ncalla dataset: Probing llms’ interactive knowledge acquisition from chi-\\nnese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='nese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\\n[445] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\\nS. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\\nlanguage models in medical education: Opportunities, challenges, and\\nfuture directions, JMIR Medical Education 9 (1) (2023) e48291. 32\\n[446] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\\nChatgpt passing usmle shines a spotlight on the flaws of medical educa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Chatgpt passing usmle shines a spotlight on the flaws of medical educa-\\ntion (2023). 32\\n[447] S. Ahn, The impending impacts of large language models on medical\\neducation, Korean Journal of Medical Education 35 (1) (2023) 103. 32\\n[448] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\\n(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\\n(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='(2023) 1–3. 32\\n[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\\nArtificial intelligence and public health: Evaluating chatgpt responses to\\nvaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 32\\n[450] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\\nTozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\\nai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='ai-driven infodemic threat in public health, Frontiers in Public Health 11\\n(2023) 1166120. 32\\n[451] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\\nformance of chatgpt and other large language models (llm) for scientific\\nand research advancements: a double-edged sword, International Re-\\nsearch Journal of Modernization in Engineering Technology and Science\\n5 (10) (2023) 875–899. 32\\n[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large\\nlanguage models provide feedback to students? a case study on chatgpt,\\nin: 2023 IEEE International Conference on Advanced Learning Tech-\\nnologies (ICALT), IEEE, 2023, pp. 323–325. 32\\n[453] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\\nF. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\\nChatgpt for good? on opportunities and challenges of large language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Chatgpt for good? on opportunities and challenges of large language\\nmodels for education, Learning and individual differences 103 (2023)\\n102274. 32\\n[454] N. Rane, Enhancing the quality of teaching and learning through chat-\\ngpt and similar large language models: Challenges, future prospects,\\nand ethical considerations in education, Future Prospects, and Ethical\\nConsiderations in Education (September 15, 2023) (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Considerations in Education (September 15, 2023) (2023). 32\\n[455] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\\ngenerating chatbot’s dialogue for english as a foreign language learning,\\nInternational Journal of Advanced Computer Science and Applications\\n14 (6) (2023). 32\\n[456] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\\nthe impacts of chatgpt on future scientific work, SocArXiv (2023). 32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='the impacts of chatgpt on future scientific work, SocArXiv (2023). 32\\n[457] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\\nscholarly writing: Is the integrity of the scientific discourse in jeopardy?,\\narXiv preprint arXiv:2311.06981 (2023). 32\\n[458] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\\nLarge language models for scientific synthesis, inference and explana-\\ntion, arXiv preprint arXiv:2310.07984 (2023). 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='tion, arXiv preprint arXiv:2310.07984 (2023). 33\\n[459] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\\nin scientific writing, PsyArXiv (2023). 33\\n[460] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\\nentific writing: a friend or a foe?, Reproductive BioMedicine Online\\n(2023). 33\\n[461] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\\nusing large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='using large language models, arXiv preprint arXiv:2303.05398 (2023).\\n33\\n[462] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\\non learning mathematical reasoning with large language models, arXiv\\npreprint arXiv:2308.01825 (2023). 33\\n[463] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\\nR. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\\naugmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='augmented language models, arXiv preprint arXiv:2306.15626 (2023).\\n33\\n[464] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\\nT. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\\nlanguage models for mathematics through interactions, arXiv preprint\\narXiv:2306.01694 (2023). 33\\n[465] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He,\\nZ. Liu, et al., Summary of chatgpt-related research and perspective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='Z. Liu, et al., Summary of chatgpt-related research and perspective\\ntowards the future of large language models, Meta-Radiology (2023)\\n100017. 33\\n[466] J. Drápal, H. Westermann, J. Savelka, Using large language models\\nto support thematic analysis in empirical legal studies, arXiv preprint\\narXiv:2310.18729 (2023). 33\\n[467] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\\ning legal concepts with augmented large language models (gpt-4), arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='ing legal concepts with augmented large language models (gpt-4), arXiv\\npreprint arXiv:2306.09525 (2023). 33\\n[468] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\\nA. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\\nbench: A collaboratively built benchmark for measuring legal reasoning\\nin large language models, arXiv preprint arXiv:2308.11462 (2023). 33\\n[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases, arXiv\\npreprint arXiv:2306.16092 (2023). 33\\n[470] H. Yang, X.-Y. Liu, C. D. Wang, Fingpt: Open-source financial large\\nlanguage models, arXiv preprint arXiv:2306.06031 (2023). 33\\n[471] Y. Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\\nsurvey, in: Proceedings of the Fourth ACM International Conference on\\nAI in Finance, 2023, pp. 374–382. 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='AI in Finance, 2023, pp. 374–382. 33\\n[472] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\\nrobot behaviour tree based on large language model, arXiv preprint\\narXiv:2305.19352 (2023). 33\\n[473] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\\naction, in: ACM/IEEE International Conference on Human-Robot Inter-\\naction, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='action, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\\n2023, pp. 905–906. 33\\n[474] Y. Ye, H. You, J. Du, Improved trust in human-robot collaboration with\\nchatgpt, IEEE Access (2023). 33\\n[475] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\\nknowledge from large language models for task and motion planning,\\nin: RSS 2023 Workshop on Learning for Task and Motion Planning,\\n2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 45}, page_content='2023. 33\\n[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\\nS. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\\nwith large language models, arXiv preprint arXiv:2305.05658 (2023).\\n33\\n[477] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\\nfor deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 34\\n[478] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\\n46'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='gers of stochastic parrots: Can language models be too big?, in: Pro-\\nceedings of the 2021 ACM conference on fairness, accountability, and\\ntransparency, 2021, pp. 610–623. 34\\n[479] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\\ndeep learning (still) requires rethinking generalization, Communications\\nof the ACM 64 (3) (2021) 107–115. 34\\n[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\\ntrained language models, arXiv preprint arXiv:2105.00828 (2021). 34\\n[481] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\\nNow (2019) 1–33. 34\\n[482] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\\nguage models still can’t plan (a benchmark for llms on planning and\\nreasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='reasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\\n[483] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen, et al., Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models, arXiv preprint arXiv:2309.01219\\n(2023). 34\\n[484] A. Webson, E. Pavlick, Do prompt-based models really understand the\\nmeaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='meaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\\n[485] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\\nthought, let’s not think step by step! bias and toxicity in zero-shot rea-\\nsoning, arXiv preprint arXiv:2212.08061 (2022). 34\\n[486] B. C. Das, M. H. Amini, Y. Wu, Security and privacy challenges of large\\nlanguage models: A survey, arXiv preprint arXiv:2402.00888 (2024). 34\\n[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-\\nial training for large neural language models, ArXiv (April 2020).\\nURL\\nhttps://www.microsoft.com/en-us/research/\\npublication/adversarial-training-for-large-neural-language-models/\\n34\\n[488] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu-\\nGhazaleh, Survey of vulnerabilities in large language models revealed\\nby adversarial attacks (2023). arXiv:2310.10844. 34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='by adversarial attacks (2023). arXiv:2310.10844. 34\\n[489] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\\nllm can fool itself: A prompt-based adversarial attack (2023). arXiv:\\n2310.13345. 34\\n[490] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\\nM. Du, Explainability for large language models: A survey (2023).\\narXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='arXiv:2309.01029. 35\\n[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large\\nlanguage models explain themselves? a study of llm-generated self-\\nexplanations (2023). arXiv:2310.11207. 35\\n[492] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\\nmean for a language model to preserve privacy?, in: Proceedings of the\\n2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='2022 ACM Conference on Fairness, Accountability, and Transparency,\\n2022, pp. 2280–2292. 35\\n[493] R. Plant, V. Giuffrida, D. Gkatzia, You are what you write:\\nPre-\\nserving privacy in the era of large language models, arXiv preprint\\narXiv:2204.09391 (2022). 35\\n[494] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\\nB. Ren, Y. Wang, Real-time execution of large-scale language models\\non mobile (2020). arXiv:2009.06823. 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='on mobile (2020). arXiv:2009.06823. 35\\n[495] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo,\\nY. Zhu, Olive:\\nAccelerating large language models via hardware-\\nfriendly outlier-victim pair quantization, in: Proceedings of the 50th\\nAnnual International Symposium on Computer Architecture, 2023, pp.\\n1–15. 35\\n[496] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\\nlanguage models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-18T00:29:28+00:00', 'source': '..\\\\data\\\\pdf\\\\llm.pdf', 'file_path': '..\\\\data\\\\pdf\\\\llm.pdf', 'total_pages': 47, 'format': 'PDF 1.5', 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;', 'subject': '', 'keywords': '', 'moddate': '2024-10-18T00:29:28+00:00', 'trapped': '', 'modDate': 'D:20241018002928Z', 'creationDate': 'D:20241018002928Z', 'page': 46}, page_content='language models (or generative ai) in healthcare, npj Digital Medicine\\n6 (1) (2023) 120. 35\\n[497] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\\nand policy implications for large language models: Guiding responsible\\ndevelopment and deployment, arXiv preprint arXiv:2308.02678 (2023).\\n35\\n[498] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\\nmodels: a three-layered approach, AI and Ethics (2023) 1–31. 35\\n47')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "002fe6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 703 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 22/22 [00:27<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (703, 384)\n",
      "Adding 703 documents to vector store...\n",
      "Sucessfully added 703documents to vectore store\n",
      "Total doc in collection: 4\n",
      "Collection count: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert text to embedding\n",
    "texts=[doc.page_content for doc in chunked_documents]\n",
    "texts\n",
    "\n",
    "#generate the embeddings\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "#store into vector database\n",
    "vectorestore.add_documents(chunked_documents,embeddings)\n",
    "\n",
    "print(\"Collection count:\", vectorestore.collection_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8e41b34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "A Comprehensive Overview of Large Language Models\n",
      "Humza Naveeda, Asad Ullah Khanb,∗, Shi Qiuc,∗, Muhammad Saqibd,e,∗, Saeed Anwarf,g, Muhammad Usmanf,g, Naveed Akhtarh,j,\n",
      "Nick Barnesi, Ajmal Mianj\n",
      "aThe University of Sydney, Sydney, Australia\n",
      "bUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
      "cThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
      "dUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "----\n",
      "dUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "eCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "fKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
      "gSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
      "hThe University of Melbourne (UoM), Melbourne, Australia\n",
      "iAustralian National University (ANU), Canberra, Australia\n",
      "jThe University of Western Australia (UWA), Perth, Australia\n",
      "----\n",
      "jThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "----\n",
      "robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
      "LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\n",
      "the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\n",
      "----\n",
      "yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad\n",
      "range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts\n",
      "along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a\n",
      "----\n",
      "systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive,\n",
      "informative summaries of the existing works to advance the LLM research.\n",
      "Keywords:\n",
      "Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
      "1. Introduction\n",
      "Language plays a fundamental role in facilitating commu-\n",
      "nication and self-expression for humans and their interaction\n",
      "----\n",
      "nication and self-expression for humans and their interaction\n",
      "with machines. The need for generalized models stems from\n",
      "the growing demand for machines to handle complex language\n",
      "tasks, including translation, summarization, information re-\n",
      "trieval, conversational interactions, etc. Recently, significant\n",
      "breakthroughs have been witnessed in language models, pri-\n",
      "marily attributed to transformers [1], increased computational\n",
      "capabilities, and the availability of large-scale training data.\n",
      "----\n",
      "capabilities, and the availability of large-scale training data.\n",
      "These developments have brought about a revolutionary trans-\n",
      "formation by enabling the creation of LLMs that can approxi-\n",
      "mate human-level performance on various tasks [2, 3]. Large\n",
      "∗Equal contribution\n",
      "Email addresses: humza_naveed@yahoo.com (Humza Naveed),\n",
      "aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "----\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
      "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
      "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
      "(Ajmal Mian)\n",
      "Figure 1: The trend of papers released over the years containing keywords\n",
      "\"Large Language Model\", \"Large Language Model + Fine-Tuning\", and \"Large\n",
      "Language Model + Alignment\".\n",
      "Preprint submitted to Elsevier\n",
      "October 18, 2024\n",
      "----\n",
      "Language Model + Alignment\".\n",
      "Preprint submitted to Elsevier\n",
      "October 18, 2024\n",
      "arXiv:2307.06435v10  [cs.CL]  17 Oct 2024\n",
      "----\n",
      "2019\n",
      "T5 (Oct)\n",
      "GPT-3 (May)\n",
      "WebGPT (Dec)\n",
      "OPT-IML\n",
      "TK-Instruct (May)\n",
      "mT0 (Dec)\n",
      "Wizard-LM\n",
      "Vicuna\n",
      "Alpaca (Mar)\n",
      "HuaTuo (Apr)\n",
      "Koala (May)\n",
      "Wizard-Coder (Jun)\n",
      "Goat\n",
      "PanGu-α (Apr)\n",
      "CPM-2 (Jun)\n",
      "GPT-NeoX-20B (Apr)\n",
      "CodeGen (Mar) \n",
      "Galactica (Nov)\n",
      "GLM (Oct)\n",
      "OPT\n",
      "UL2 (May)\n",
      "LLaMA (Feb)\n",
      "LLaMA 2 (Jul)\n",
      "MPT (Jun)\n",
      "CodeT5+\n",
      "Code Llama (Aug)\n",
      "StarCoder\n",
      "Xuan Yuan 2.0 (May)\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "mT5 (Oct)\n",
      "HyperCLOVA (Sep)\n",
      "ERNIE 3.0\n",
      "Codex (Jul)\n",
      "Jurassic-1 (Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0 (Oct)\n",
      "----\n",
      "Codex (Jul)\n",
      "Jurassic-1 (Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0 (Oct)\n",
      "ChatGPT (Nov)\n",
      "Sparrow (Sep)\n",
      "FLAN-U-PaLM (Oct)\n",
      "Bard (Oct)\n",
      "MT-NLG (Jan)\n",
      "AlphaCode (Feb)\n",
      "Chinchilla (Mar)\n",
      "PaLM (Apr)\n",
      "U-PALM (Oct)\n",
      "BLOOM (Nov)\n",
      "AlexaTM (Aug)\n",
      "PaLM2 (May)\n",
      "GPT-4\n",
      "PanGu-Σ (Mar)\n",
      "BloombergGPT\n",
      "Claude\n",
      "Gemini (Dec)\n",
      "DeepSeek (Jan)\n",
      "LLaMA 3 \n",
      "Grok-1 (Mar)\n",
      "Snowflake Arctic (Apr)\n",
      "DeepSeek-V2 (May)\n",
      "Mixtral 8x22B\n",
      "Nemotron (Feb)\n",
      "GPT-4o (May)\n",
      "OpenAI o1 (Sep)\n",
      "Gemini-1.5 (Feb)\n",
      "Grok-1.5 (Apr)\n",
      "----\n",
      "Mixtral 8x22B\n",
      "Nemotron (Feb)\n",
      "GPT-4o (May)\n",
      "OpenAI o1 (Sep)\n",
      "Gemini-1.5 (Feb)\n",
      "Grok-1.5 (Apr)\n",
      "Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\n",
      "on the upper half signify open-source availability, whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned\n",
      "----\n",
      "and open-source models, highlighting the evolving landscape and trends in natural language processing research.\n",
      "Language Models (LLMs) have emerged as cutting-edge arti-\n",
      "ficial intelligence systems that can process and generate text\n",
      "with coherent communication [4] and generalize to multiple\n",
      "tasks [5, 6].\n",
      "The historical progress in natural language processing (NLP)\n",
      "evolved from statistical to neural language modeling and then\n",
      "from pre-trained language models (PLMs) to LLMs.\n",
      "While\n",
      "----\n",
      "from pre-trained language models (PLMs) to LLMs.\n",
      "While\n",
      "conventional language modeling (LM) trains task-specific mod-\n",
      "els in supervised settings, PLMs are trained in a self-supervised\n",
      "setting on a large corpus of text [7, 8, 9] with the aim of learning\n",
      "a generic representation that is shareable among various NLP\n",
      "tasks. After fine-tuning for downstream tasks, PLMs surpass\n",
      "the performance gains of traditional language modeling (LM).\n",
      "The larger PLMs bring more performance gains, which has led\n",
      "----\n",
      "The larger PLMs bring more performance gains, which has led\n",
      "to the transitioning of PLMs to LLMs by significantly increas-\n",
      "ing model parameters (tens to hundreds of billions) [10] and\n",
      "training dataset (many GBs and TBs) [10, 11]. Following this\n",
      "development, numerous LLMs have been proposed in the lit-\n",
      "erature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\n",
      "number of released LLMs and names of a few significant LLMs\n",
      "proposed over the years are shown in Fig 1 and Fig 2, respec-\n",
      "tively.\n",
      "----\n",
      "proposed over the years are shown in Fig 1 and Fig 2, respec-\n",
      "tively.\n",
      "The early work on LLMs, such as T5 [10] and mT5 [11] em-\n",
      "ployed transfer learning until GPT-3 [6] showed LLMs are\n",
      "zero-shot transferable to downstream tasks without fine-tuning.\n",
      "LLMs accurately respond to task queries when prompted with\n",
      "task descriptions and examples. However, pre-trained LLMs\n",
      "fail to follow user intent and perform worse in zero-shot set-\n",
      "tings than in few-shot.\n",
      "Fine-tuning them with task instruc-\n",
      "----\n",
      "tings than in few-shot.\n",
      "Fine-tuning them with task instruc-\n",
      "tions data [16, 17, 18, 19] and aligning with human prefer-\n",
      "ences [20, 21] enhances generalization to unseen tasks, im-\n",
      "proving zero-shot performance significantly and reducing mis-\n",
      "aligned behavior.\n",
      "In addition to better generalization and domain adaptation,\n",
      "LLMs appear to have emergent abilities, such as reasoning,\n",
      "planning, decision-making, in-context learning, answering in\n",
      "zero-shot settings, etc.\n",
      "----\n",
      "planning, decision-making, in-context learning, answering in\n",
      "zero-shot settings, etc.\n",
      "These abilities are known to be ac-\n",
      "quired by them due to their gigantic scale even when the pre-\n",
      "trained LLMs are not trained specifically to possess these at-\n",
      "tributes [22, 23, 24]. Such abilities have led LLMs to be widely\n",
      "adopted in diverse settings, including multi-modal, robotics,\n",
      "tool manipulation, question answering, autonomous agents, etc.\n",
      "Various improvements have also been suggested in these areas\n",
      "----\n",
      "Various improvements have also been suggested in these areas\n",
      "either by task-specific training [25, 26, 27, 28, 29, 30, 31] or\n",
      "better prompting [32].\n",
      "The LLMs abilities to solve diverse tasks with human-level\n",
      "performance come at the cost of slow training and inference,\n",
      "extensive hardware requirements, and higher running costs.\n",
      "Such requirements have limited their adoption and opened up\n",
      "opportunities to devise better architectures [15, 33, 34, 35]\n",
      "----\n",
      "opportunities to devise better architectures [15, 33, 34, 35]\n",
      "and training strategies [36, 37, 21, 38, 39, 40, 41].\n",
      "Param-\n",
      "eter efficient tuning [38, 41, 40], pruning [42, 43], quantiza-\n",
      "tion [44, 45], knowledge distillation, and context length inter-\n",
      "polation [46, 47, 48, 49] among others are some of the methods\n",
      "widely studied for efficient LLM utilization.\n",
      "Due to the success of LLMs on a wide variety of tasks, the\n",
      "research literature has recently experienced a large influx of\n",
      "----\n",
      "research literature has recently experienced a large influx of\n",
      "LLM-related contributions.\n",
      "Researchers have organized the\n",
      "LLMs literature in surveys [50, 51, 52, 53], and topic-specific\n",
      "surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\n",
      "contribution focuses on providing a comprehensive yet concise\n",
      "overview of the general direction of LLM research. This arti-\n",
      "cle summarizes architectural and training details of pre-trained\n",
      "----\n",
      "cle summarizes architectural and training details of pre-trained\n",
      "LLMs and delves deeper into the details of concepts like fine-\n",
      "tuning, multi-modal LLMs, augmented LLMs, datasets, eval-\n",
      "uation, applications, challenges, and others to provide a self-\n",
      "contained comprehensive overview. Our key contributions are\n",
      "summarized as follows.\n",
      "• We present a survey on the developments in LLM research,\n",
      "providing a concise, comprehensive overview of the direc-\n",
      "tion.\n",
      "----\n",
      "providing a concise, comprehensive overview of the direc-\n",
      "tion.\n",
      "• We present extensive summaries of pre-trained models that\n",
      "include fine-grained details of architecture and training de-\n",
      "tails.\n",
      "• We summarize major findings of the popular contributions\n",
      "and provide a detailed discussion on the key design and\n",
      "development aspects of LLMs to help practitioners effec-\n",
      "tively leverage this technology.\n",
      "• In this self-contained article, we cover a range of con-\n",
      "----\n",
      "tively leverage this technology.\n",
      "• In this self-contained article, we cover a range of con-\n",
      "cepts to present the general direction of LLMs compre-\n",
      "hensively, including background, pre-training, fine-tuning,\n",
      "2\n",
      "----\n",
      "Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. Efficient 4. Inference 5. Evaluation 6. Applications\n",
      "7. Challenges\n",
      "multi-modal LLMs, augmented LLMs, LLMs-powered\n",
      "agents, datasets, evaluation, etc.\n",
      "We loosely follow the existing terminology to ensure a stan-\n",
      "dardized outlook of this research direction. For instance, fol-\n",
      "lowing [50], our survey discusses pre-trained LLMs with 10B\n",
      "----\n",
      "lowing [50], our survey discusses pre-trained LLMs with 10B\n",
      "parameters or more. We refer the readers interested in smaller\n",
      "pre-trained models to [51, 52, 53].\n",
      "The organization of this paper is as follows. Section 2 discusses\n",
      "the background of LLMs. Section 3 focuses on LLMs overview,\n",
      "architectures, training pipelines and strategies, fine-tuning, and\n",
      "utilization in different domains. Section 4 highlights the config-\n",
      "uration and parameters that play a crucial role in the function-\n",
      "----\n",
      "uration and parameters that play a crucial role in the function-\n",
      "ing of these models. Summary and discussions are presented\n",
      "in section 3.8. The LLM training and evaluation, datasets, and\n",
      "benchmarks are discussed in section 5, followed by challenges\n",
      "and future directions, and conclusion in sections 7 and 8, re-\n",
      "spectively.\n",
      "3\n",
      "----\n",
      "2. Background\n",
      "We provide the relevant background to understand the fun-\n",
      "damentals related to LLMs in this section. We briefly discuss\n",
      "necessary components in LLMs and refer the readers interested\n",
      "in details to the original works.\n",
      "2.1. Tokenization\n",
      "Tokenization [59] is an essential pre-processing step in\n",
      "LLM training that parses the text into non-decomposing units\n",
      "called tokens. Tokens can be characters, subwords [60], sym-\n",
      "bols [61], or words, depending on the tokenization process.\n",
      "----\n",
      "bols [61], or words, depending on the tokenization process.\n",
      "Some of the commonly used tokenization schemes in LLMs\n",
      "include wordpiece [62], byte pair encoding (BPE) [61], and un-\n",
      "igramLM [60]. Readers are encouraged to refer to [63] for a\n",
      "detailed survey.\n",
      "2.2. Encoding Positions\n",
      "The transformer processes input sequences in parallel and\n",
      "independently of each other.\n",
      "Moreover, the attention mod-\n",
      "ule in the transformer does not capture positional information.\n",
      "----\n",
      "Moreover, the attention mod-\n",
      "ule in the transformer does not capture positional information.\n",
      "As a result, positional encodings were introduced in trans-\n",
      "former [64], where a positional embedding vector is added to\n",
      "the token embedding. Variants of positional embedding include\n",
      "absolute, relative, or learned positional encodings. Within rel-\n",
      "ative encoding, Alibi and RoPE are two widely used positional\n",
      "embeddings in LLMs.\n",
      "Alibi [65]: It subtracts a scalar bias from the attention score\n",
      "----\n",
      "embeddings in LLMs.\n",
      "Alibi [65]: It subtracts a scalar bias from the attention score\n",
      "that increases with the distance between token positions. This\n",
      "favors using recent tokens for attention.\n",
      "RoPE [66]: It rotates query and key representations at an an-\n",
      "gle proportional to the token absolute position in the input\n",
      "sequence, resulting in a relative positional encoding scheme\n",
      "which decays with the distance between the tokens.\n",
      "2.3. Attention in LLMs\n",
      "----\n",
      "which decays with the distance between the tokens.\n",
      "2.3. Attention in LLMs\n",
      "Attention assigns weights to input tokens based on impor-\n",
      "tance so that the model gives more emphasis to relevant tokens.\n",
      "Attention in transformers [64] calculates query, key, and value\n",
      "mappings for input sequences, where the attention score is\n",
      "obtained by multiplying the query and key, and later used to\n",
      "weight values. We discuss different attention strategies used in\n",
      "LLMs below.\n",
      "----\n",
      "weight values. We discuss different attention strategies used in\n",
      "LLMs below.\n",
      "Self-Attention [64]: Calculates attention using queries, keys,\n",
      "and values from the same block (encoder or decoder).\n",
      "Cross Attention: It is used in encoder-decoder architectures,\n",
      "where encoder outputs are the queries, and key-value pairs\n",
      "come from the decoder.\n",
      "Sparse Attention [67]: Self-attention has O(n2) time complex-\n",
      "ity which becomes infeasible for large sequences. To speed\n",
      "----\n",
      "ity which becomes infeasible for large sequences. To speed\n",
      "up the computation, sparse attention [67] iteratively calculates\n",
      "attention in sliding windows for speed gains.\n",
      "Flash Attention [68]: Memory access is the major bottleneck\n",
      "in calculating attention using GPUs.\n",
      "To speed up, flash\n",
      "attention employs input tiling to minimize the memory reads\n",
      "and writes between the GPU high bandwidth memory (HBM)\n",
      "and the on-chip SRAM.\n",
      "2.4. Activation Functions\n",
      "----\n",
      "and the on-chip SRAM.\n",
      "2.4. Activation Functions\n",
      "The activation functions serve a crucial role in the curve-\n",
      "fitting abilities of neural networks [69]. We discuss activation\n",
      "functions used in LLMs in this section.\n",
      "ReLU [70]: The Rectified linear unit (ReLU) is defined as:\n",
      "ReLU(x) = max(0, x)\n",
      "(1)\n",
      "GeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\n",
      "combination of ReLU, dropout [72] and zoneout [73].\n",
      "GLU variants [74]: The Gated Linear Unit [75] is a neural\n",
      "----\n",
      "GLU variants [74]: The Gated Linear Unit [75] is a neural\n",
      "network layer that is an element-wise product (⊗) of a linear\n",
      "transformation and a sigmoid transformed (σ) linear projection\n",
      "of the input given as:\n",
      "GLU(x, W, V, b, c) = (xW + b) ⊗σ(xV + c),\n",
      "(2)\n",
      "where X is the input of layer and l, W, b, V and c are learned\n",
      "parameters. Other GLU variants [74] used in LLMs are:\n",
      "ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\n",
      "GEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\n",
      "----\n",
      "ReGLU(x, W, V, b, c) = max(0, xW + b)⊗,\n",
      "GEGLU(x, W, V, b, c) = GELU(xW + b) ⊗(xV + c),\n",
      "S wiGLU(x, W, V, b, c, β) = S wishβ(xW + b) ⊗(xV + c).\n",
      "2.5. Layer Normalization\n",
      "Layer normalization leads to faster convergence and is an in-\n",
      "tegrated component of transformers [64]. In addition to Layer-\n",
      "Norm [76] and RMSNorm [77], LLMs use pre-layer normal-\n",
      "ization [78], applying it before multi-head attention (MHA).\n",
      "Pre-norm is shown to provide training stability in LLMs. An-\n",
      "----\n",
      "Pre-norm is shown to provide training stability in LLMs. An-\n",
      "other normalization variant, DeepNorm [79] fixes the issue with\n",
      "larger gradients in pre-norm.\n",
      "2.6. Distributed LLM Training\n",
      "This section describes distributed LLM training approaches\n",
      "briefly. More details are available in [13, 37, 80, 81].\n",
      "Data Parallelism: Data parallelism replicates the model on\n",
      "multiple devices where data in a batch gets divided across de-\n",
      "vices. At the end of each training iteration weights are synchro-\n",
      "----\n",
      "vices. At the end of each training iteration weights are synchro-\n",
      "nized across all devices.\n",
      "Tensor Parallelism: Tensor parallelism shards a tensor compu-\n",
      "tation across devices. It is also known as horizontal parallelism\n",
      "or intra-layer model parallelism.\n",
      "Pipeline Parallelism: Pipeline parallelism shards model layers\n",
      "across different devices. This is also known as vertical paral-\n",
      "lelism.\n",
      "Model Parallelism: A combination of tensor and pipeline par-\n",
      "allelism is known as model parallelism.\n",
      "----\n",
      "allelism is known as model parallelism.\n",
      "3D Parallelism: A combination of data, tensor, and model par-\n",
      "allelism is known as 3D parallelism.\n",
      "Optimizer Parallelism: Optimizer parallelism also known as\n",
      "zero redundancy optimizer [37] implements optimizer state\n",
      "partitioning, gradient partitioning, and parameter partitioning\n",
      "across devices to reduce memory consumption while keeping\n",
      "the communication costs as low as possible.\n",
      "4\n",
      "----\n",
      "2.7. Libraries\n",
      "Some commonly used libraries for LLMs training are:\n",
      "Transformers [82]: The library provides access to various pre-\n",
      "trained transformer models with APIs to train, fine-tune, infer,\n",
      "and develop custom models.\n",
      "DeepSpeed [36]: A library for scalable distributed training and\n",
      "inference of deep learning models.\n",
      "Megatron-LM [80]: It provides GPU-optimized techniques for\n",
      "large-scale training of LLMs.\n",
      "JAX [83]: A Python library for high-performance numerical\n",
      "----\n",
      "large-scale training of LLMs.\n",
      "JAX [83]: A Python library for high-performance numerical\n",
      "computing and scaleable machine learning. It can differenti-\n",
      "ate native Python and NumPy functions and execute them on\n",
      "GPUs.\n",
      "Colossal-AI [84]: A collection of components to write dis-\n",
      "tributed deep learning models.\n",
      "BMTrain [81]: A library to write efficient stand-alone LLMs\n",
      "training code.\n",
      "FastMoE [85]:\n",
      "Provides API to build mixture-of-experts\n",
      "(MoE) model in PyTorch.\n",
      "----\n",
      "training code.\n",
      "FastMoE [85]:\n",
      "Provides API to build mixture-of-experts\n",
      "(MoE) model in PyTorch.\n",
      "MindSpore [86]: A deep learning training and inference frame-\n",
      "work extendable to mobile, edge, and cloud computing.\n",
      "PyTorch [87]: A framework developed by Facebook AI Re-\n",
      "search lab (FAIR) to build deep learning models. The main\n",
      "features of PyTorch include a dynamic computation graph and\n",
      "a pythonic coding style.\n",
      "Tensorflow [88]:\n",
      "A deep learning framework written by\n",
      "----\n",
      "a pythonic coding style.\n",
      "Tensorflow [88]:\n",
      "A deep learning framework written by\n",
      "Google. The key features of TensorFlow are graph-based com-\n",
      "putation, eager execution, scalability, etc.\n",
      "MXNet [89]: Apache MXNet is a deep learning framework\n",
      "with support to write programs in multiple languages, includ-\n",
      "ing, Python, C++, Scala, R, etc. It also provides support for\n",
      "dynamic and static computation graphs.\n",
      "2.8. Data PreProcessing\n",
      "This section briefly summarizes data preprocessing tech-\n",
      "----\n",
      "2.8. Data PreProcessing\n",
      "This section briefly summarizes data preprocessing tech-\n",
      "niques used in LLMs training.\n",
      "Quality Filtering: For better results, training data quality is\n",
      "essential. Some approaches to filtering data are: 1) classifier-\n",
      "based and 2) heuristics-based.\n",
      "Classifier-based approaches\n",
      "train a classifier on high-quality data and predict the quality of\n",
      "text for filtering, whereas heuristics-based employ some rules\n",
      "for filtering like language, metrics, statistics, and keywords.\n",
      "----\n",
      "for filtering like language, metrics, statistics, and keywords.\n",
      "Data Deduplication: Duplicated data can affect model per-\n",
      "formance and increase data memorization; therefore, to train\n",
      "LLMs, data deduplication is one of the preprocessing steps.\n",
      "This can be performed at multiple levels, like sentences,\n",
      "documents, and datasets.\n",
      "Privacy Reduction: Most of the training data for LLMs is\n",
      "collected through web sources.\n",
      "This data contains private\n",
      "information; therefore, many LLMs employ heuristics-based\n",
      "----\n",
      "This data contains private\n",
      "information; therefore, many LLMs employ heuristics-based\n",
      "methods to filter information such as names, addresses, and\n",
      "phone numbers to avoid learning personal information.\n",
      "2.9. Architectures\n",
      "Here we discuss the variants of the transformer architectures\n",
      "used in LLMs. The difference arises due to the application of\n",
      "Figure 4: An example of attention patterns in language models, image is taken\n",
      "from [93].\n",
      "----\n",
      "Figure 4: An example of attention patterns in language models, image is taken\n",
      "from [93].\n",
      "Figure 5: An example of language model training objectives, image from [93].\n",
      "the attention and the connection of transformer blocks. An il-\n",
      "lustration of attention patterns of these architectures is shown\n",
      "in Figure 4.\n",
      "Encoder Decoder: This architecture processes inputs through\n",
      "the encoder and passes the intermediate representation to the\n",
      "decoder to generate the output.\n",
      "Here, the encoder sees the\n",
      "----\n",
      "decoder to generate the output.\n",
      "Here, the encoder sees the\n",
      "complete sequence utilizing self-attention whereas the decoder\n",
      "processes the sequence one after the other with implementing\n",
      "cross-attention.\n",
      "Causal Decoder: A type of architecture that does not have an\n",
      "encoder and processes and generates output using a decoder,\n",
      "where the predicted token depends only on the previous time\n",
      "steps.\n",
      "Prefix Decoder: It is also known as a non-causal decoder,\n",
      "----\n",
      "steps.\n",
      "Prefix Decoder: It is also known as a non-causal decoder,\n",
      "where the attention calculation is not strictly dependent on the\n",
      "past information and the attention is bidirectional. An example\n",
      "of a non-causal attention mask is shown in Figure 4.\n",
      "Mixture-of-Experts: It is a variant of transformer architecture\n",
      "with parallel independent experts and a router to route tokens\n",
      "to experts. These experts are feed-forward layers after the at-\n",
      "tention block [90]. Mixture-of-Experts (MoE) is an efficient\n",
      "----\n",
      "tention block [90]. Mixture-of-Experts (MoE) is an efficient\n",
      "sparse architecture that offers comparable performance to dense\n",
      "models and allows increasing the model size without increas-\n",
      "ing the computational cost by activating only a few experts at a\n",
      "time [91, 92].\n",
      "2.10. Pre-Training Objectives\n",
      "This section describes LLMs pre-training objectives.\n",
      "For\n",
      "more details see the paper [93].\n",
      "Full Language Modeling: An autoregressive language model-\n",
      "----\n",
      "For\n",
      "more details see the paper [93].\n",
      "Full Language Modeling: An autoregressive language model-\n",
      "ing objective where the model is asked to predict future tokens\n",
      "given the previous tokens, an example is shown in Figure 5.\n",
      "Prefix Language Modeling: A non-causal training objective,\n",
      "where a prefix is chosen randomly and only remaining target\n",
      "tokens are used to calculate the loss. An example is shown in\n",
      "Figure 5.\n",
      "5\n",
      "----\n",
      "Figure 6: A basic flow diagram depicting various stages of LLMs from pre-training to prompting/utilization. Prompting LLMs to generate responses is possible at\n",
      "different training stages like pre-training, instruction-tuning, or alignment tuning. “RL” stands for reinforcement learning, “RM” represents reward-modeling, and\n",
      "“RLHF” represents reinforcement learning with human feedback.\n",
      "Masked Language Modeling: In this training objective, tokens\n",
      "----\n",
      "Masked Language Modeling: In this training objective, tokens\n",
      "or spans (a sequence of tokens) are masked randomly and the\n",
      "model is asked to predict masked tokens given the past and\n",
      "future context. An example is shown in Figure 5.\n",
      "Unified Language Modeling: Unified language modeling [94]\n",
      "is a combination of causal, non-causal, and masked language\n",
      "training objectives. Here in masked language modeling, the\n",
      "attention is not bidirectional but unidirectional, attending either\n",
      "----\n",
      "attention is not bidirectional but unidirectional, attending either\n",
      "left-to-right or right-to-left context.\n",
      "2.11. LLMs Scaling Laws\n",
      "Scaling laws study the optimal combination of model param-\n",
      "eters, dataset size, and computational resources that predict the\n",
      "improvement in the model performance. It has been shown\n",
      "that the loss scales according to the power-law with model size,\n",
      "dataset size, and compute resources [95]. This study suggests\n",
      "----\n",
      "dataset size, and compute resources [95]. This study suggests\n",
      "larger models are more important than big data for better perfor-\n",
      "mance. Another variant of scaling law [96] suggests the model\n",
      "size and the number of training tokens should be scaled equally.\n",
      "2.12. LLMs Adaptation Stages\n",
      "This section discusses the fundamentals of LLMs adaptation\n",
      "stages, from pre-training to fine-tuning for downstream tasks\n",
      "and utilization. An example of different training stages and in-\n",
      "----\n",
      "and utilization. An example of different training stages and in-\n",
      "ference in LLMs is shown in Figure 6. In this paper, we refer\n",
      "to alignment-tuning as aligning with human preferences, while\n",
      "occasionally the literature uses the term alignment for different\n",
      "purposes.\n",
      "2.12.1. Pre-Training\n",
      "In the very first stage, the model is trained in a self-\n",
      "supervised manner on a large corpus to predict the next to-\n",
      "kens given the input. The design choices of LLMs vary from\n",
      "----\n",
      "kens given the input. The design choices of LLMs vary from\n",
      "encoder-decoder to decoder-only architectures with different\n",
      "building blocks and loss functions in sections 2.5, 2.4, 2.10.\n",
      "2.12.2. Fine-Tuning\n",
      "There are different styles to fine-tune an LLM. This section\n",
      "briefly discusses fine-tuning approaches.\n",
      "Transfer Learning: The pre-trained LLMs perform well for\n",
      "various tasks [6, 15]. However, to improve the performance for\n",
      "6\n",
      "----\n",
      "a downstream task, pre-trained models are fine-tuned with the\n",
      "task-specific data [10, 11], known as transfer learning.\n",
      "Instruction-tuning: To enable a model to respond to user\n",
      "queries effectively, the pre-trained model is fine-tuned on in-\n",
      "struction formatted data i.e., instruction and an input-output\n",
      "pair. Instructions generally comprise multi-task data in plain\n",
      "natural language, guiding the model to respond according to the\n",
      "prompt and the input. This type of fine-tuning improves zero-\n",
      "----\n",
      "prompt and the input. This type of fine-tuning improves zero-\n",
      "shot generalization and downstream task performance. Details\n",
      "on formatting instruction data and its various styles are avail-\n",
      "able in [16, 50, 97].\n",
      "Alignment-tuning: LLMs are prone to generating false, biased,\n",
      "and harmful text. To make them helpful, honest, and harmless,\n",
      "models are aligned using human feedback. Alignment involves\n",
      "asking LLMs to generate unexpected responses and then updat-\n",
      "----\n",
      "asking LLMs to generate unexpected responses and then updat-\n",
      "ing their parameters to avoid such responses [20, 21, 98].\n",
      "It ensures LLMs operate according to human intentions and\n",
      "values. A model is defined to be an “aligned” model if the\n",
      "model fulfills three criteria of helpful, honest, and harmless or\n",
      "“HHH” [99].\n",
      "Researchers employ reinforcement learning with human feed-\n",
      "back (RLHF) [100] for model alignment. In RLHF, a fine-tuned\n",
      "model on demonstrations is further trained with reward model-\n",
      "----\n",
      "model on demonstrations is further trained with reward model-\n",
      "ing (RM) and reinforcement learning (RL), shown in Figure 6.\n",
      "Below we briefly discuss RM and RL pipelines in RLHF.\n",
      "Reward modeling: trains a model to rank generated responses\n",
      "according to human preferences using a classification objec-\n",
      "tive. To train the classifier humans annotate LLMs generated\n",
      "responses based on the HHH criteria.\n",
      "Reinforcement learning: in combination with the reward model\n",
      "----\n",
      "responses based on the HHH criteria.\n",
      "Reinforcement learning: in combination with the reward model\n",
      "is used for alignment in the next stage. The previously trained\n",
      "reward model ranks LLM-generated responses into preferred\n",
      "vs. non-preferred, which is used to align the model with proxi-\n",
      "mal policy optimization (PPO). This process repeats iteratively\n",
      "until convergence.\n",
      "2.12.3. Prompting/Utilization\n",
      "Prompting is a method to query trained LLMs for generating\n",
      "----\n",
      "2.12.3. Prompting/Utilization\n",
      "Prompting is a method to query trained LLMs for generating\n",
      "responses, as illustrated in Figure 6. LLMs can be prompted in\n",
      "various prompt setups, where they can be adapted to the instruc-\n",
      "tions without fine-tuning and in other cases with fine-tuning on\n",
      "data containing different prompt styles [16, 101, 102]. A good\n",
      "guide on prompt engineering is available at [32]. Below, we\n",
      "will discuss various widely used prompt setups.\n",
      "----\n",
      "will discuss various widely used prompt setups.\n",
      "Zero-Shot Prompting: LLMs are zero-shot learners and ca-\n",
      "pable of answering queries never seen before. This style of\n",
      "prompting requires LLMs to answer user questions without see-\n",
      "ing any examples in the prompt.\n",
      "In-context Learning: Also known as few-shot learning, here,\n",
      "multiple input-output demonstration pairs are shown to the\n",
      "model to generate the desired response. This adaptation style\n",
      "----\n",
      "model to generate the desired response. This adaptation style\n",
      "is also called few-shot learning. A discussion on formatting in-\n",
      "context learning (ICL) templates is available in [54, 50, 18, 16].\n",
      "Reasoning in LLMs: LLMs are zero-shot reasoners and can\n",
      "be provoked to generate answers to logical problems, task\n",
      "planning, critical thinking, etc. with reasoning.\n",
      "Generating\n",
      "reasons is possible only by using different prompting styles,\n",
      "whereas to improve LLMs further on reasoning tasks many\n",
      "----\n",
      "whereas to improve LLMs further on reasoning tasks many\n",
      "methods [16, 97] train them on reasoning datasets. We discuss\n",
      "various prompting techniques for reasoning below.\n",
      "Chain-of-Thought (CoT): A special case of prompting where\n",
      "demonstrations contain reasoning information aggregated with\n",
      "inputs and outputs so that the model generates outcomes with\n",
      "step-by-step reasoning. More details on CoT prompts are avail-\n",
      "able in [55, 103, 101].\n",
      "Self-Consistency:\n",
      "Improves CoT performance by generat-\n",
      "----\n",
      "able in [55, 103, 101].\n",
      "Self-Consistency:\n",
      "Improves CoT performance by generat-\n",
      "ing multiple responses and selecting the most frequent an-\n",
      "swer [104].\n",
      "Tree-of-Thought (ToT): Explores multiple reasoning paths\n",
      "with possibilities to look ahead and backtrack for problem-\n",
      "solving [105].\n",
      "Single-Turn Instructions: In this prompting setup, LLMs are\n",
      "queried only once with all the relevant information in the\n",
      "prompt. LLMs generate responses by understanding the con-\n",
      "----\n",
      "prompt. LLMs generate responses by understanding the con-\n",
      "text either in a zero-shot or few-shot setting.\n",
      "Multi-Turn Instructions: Solving a complex task requires mul-\n",
      "tiple interactions with LLMs, where feedback and responses\n",
      "from the other tools are given as input to the LLM for the next\n",
      "rounds. This style of using LLMs in the loop is common in\n",
      "autonomous agents.\n",
      "3. Large Language Models\n",
      "This section reviews LLMs, briefly describing their architec-\n",
      "----\n",
      "3. Large Language Models\n",
      "This section reviews LLMs, briefly describing their architec-\n",
      "tures, training objectives, pipelines, datasets, and fine-tuning\n",
      "details.\n",
      "3.1. Pre-Trained LLMs\n",
      "Here, we provide summaries of various well-known pre-\n",
      "trained LLMs with significant discoveries, changing the course\n",
      "of research and development in NLP. These LLMs have consid-\n",
      "erably improved the performance in NLU and NLG domains,\n",
      "and are widely fine-tuned for downstream tasks. Moreover, We\n",
      "----\n",
      "and are widely fine-tuned for downstream tasks. Moreover, We\n",
      "also identify key findings and insights of pre-trained LLMs in\n",
      "Table 1 and 2 that improve their performance.\n",
      "3.1.1. General Purpose\n",
      "T5 [10]: An encoder-decoder model employing a unified text-\n",
      "to-text training for all NLP problems is shown in Figure 7. T5\n",
      "places layer normalization outside the residual path in a conven-\n",
      "tional transformer model [64]. It uses masked language mod-\n",
      "----\n",
      "tional transformer model [64]. It uses masked language mod-\n",
      "eling as a pre-training objective where spans (consecutive to-\n",
      "kens) are replaced with a single mask instead of separate masks\n",
      "for each token. This type of masking speeds up the training as\n",
      "it produces shorter sequences. After pre-training, the model is\n",
      "fine-tuned using adapter layers [106] for downstream tasks.\n",
      "GPT-3 [6]: The GPT-3 architecture is the same as the GPT-\n",
      "2 [5] but with dense and sparse attention in transformer layers\n",
      "----\n",
      "2 [5] but with dense and sparse attention in transformer layers\n",
      "similar to the Sparse Transformer [67]. It shows that large mod-\n",
      "els can train on larger batch sizes with a lower learning rate to\n",
      "decide the batch size during training, GPT-3 uses the gradient\n",
      "noise scale as in [107]. Overall, GPT-3 increases model param-\n",
      "eters to 175B showing that the performance of large language\n",
      "7\n",
      "----\n",
      "Figure 7: Unified text-to-text training example, source image from [10].\n",
      "Figure 8: The image is the article of [108], showing an example of PanGu-α\n",
      "architecture.\n",
      "models improves with the scale and is competitive with the fine-\n",
      "tuned models.\n",
      "mT5 [11]: A multilingual T5 model [10] trained on the mC4\n",
      "dataset with 101 languages. The dataset is extracted from the\n",
      "public common crawl scrape. The model uses a larger vocab-\n",
      "ulary size of 250,000 to cover multiple languages. To avoid\n",
      "----\n",
      "ulary size of 250,000 to cover multiple languages. To avoid\n",
      "over-fitting or under-fitting for a language, mT5 employs a data\n",
      "sampling procedure to select samples from all languages. The\n",
      "paper suggests using a small amount of pre-training datasets,\n",
      "including all languages when fine-tuning for a task using En-\n",
      "glish language data. This allows the model to generate correct\n",
      "non-English outputs.\n",
      "PanGu-α [108]: An autoregressive model that has a query\n",
      "----\n",
      "non-English outputs.\n",
      "PanGu-α [108]: An autoregressive model that has a query\n",
      "layer at the end of standard transformer layers, example shown\n",
      "in Figure 8, to predict the next token. Its structure is similar to\n",
      "the transformer layer but with an additional embedding for the\n",
      "next position in the attention mechanism, given in Eq. 3.\n",
      "a = pnWq\n",
      "hWk\n",
      "hTHT\n",
      "L\n",
      "(3)\n",
      "CPM-2 [12]: Cost-efficient Pre-trained language Models\n",
      "(CPM-2) pre-trains bilingual (English and Chinese) 11B and\n",
      "----\n",
      "(CPM-2) pre-trains bilingual (English and Chinese) 11B and\n",
      "198B mixture-of-experts (MoE) models on the WuDaoCor-\n",
      "pus [109] dataset. The tokenization process removes “_” white\n",
      "space tokens in the sentencepiece tokenizer. The models are\n",
      "trained with knowledge inheritance, starting with only the Chi-\n",
      "nese language in the first stage and then adding English and\n",
      "Chinese data. This trained model gets duplicated multiple times\n",
      "to initialize the 198B MoE model. Moreover, to use the model\n",
      "----\n",
      "to initialize the 198B MoE model. Moreover, to use the model\n",
      "for downstream tasks, CPM-2 experimented with both com-\n",
      "plete fine-tuning and prompt fine-tuning as in [40] where only\n",
      "prompt-related parameters are updated by inserting prompts at\n",
      "various positions, front, middle, and back. CPM-2 also pro-\n",
      "poses the INFMOE, a memory-efficient framework with a strat-\n",
      "egy to dynamically offload parameters to the CPU for inference\n",
      "at a 100B scale. It overlaps data movement with inference com-\n",
      "----\n",
      "at a 100B scale. It overlaps data movement with inference com-\n",
      "putation for lower inference time.\n",
      "ERNIE 3.0 [110]: ERNIE 3.0 takes inspiration from multi-\n",
      "task learning to build a modular architecture using Transformer-\n",
      "XL [111] as the backbone. The universal representation mod-\n",
      "ule is shared by all the tasks, which serve as the basic block\n",
      "for task-specific representation modules, which are all trained\n",
      "jointly for natural language understanding, natural language\n",
      "----\n",
      "jointly for natural language understanding, natural language\n",
      "generation, and knowledge extraction. This LLM is primar-\n",
      "ily focused on the Chinese language. It claims to train on the\n",
      "largest Chinese text corpora for LLM training, and achieved\n",
      "state-of-the-art in 54 Chinese NLP tasks.\n",
      "Jurassic-1 [112]: A pair of auto-regressive language mod-\n",
      "els, including a 7B-parameter J1-Large model and a 178B-\n",
      "parameter J1-Jumbo model.\n",
      "The training vocabulary of\n",
      "----\n",
      "parameter J1-Jumbo model.\n",
      "The training vocabulary of\n",
      "Jurassic-1 comprise word pieces, complete words, and multi-\n",
      "word expressions without any word boundaries, where possible\n",
      "out-of-vocabulary instances are interpreted as Unicode bytes.\n",
      "Compared to the GPT-3 counterparts, the Jurassic-1 models\n",
      "apply a more balanced depth-to-width self-attention architec-\n",
      "ture [113] and an improved tokenizer for a faster prediction\n",
      "based on broader resources, achieving a comparable perfor-\n",
      "----\n",
      "based on broader resources, achieving a comparable perfor-\n",
      "mance in zero-shot learning tasks and a superior performance in\n",
      "few-shot learning tasks given the ability to feed more examples\n",
      "as a prompt.\n",
      "HyperCLOVA [114]: A Korean language model with GPT-3\n",
      "architecture.\n",
      "Yuan 1.0 [115]: Trained on a Chinese corpus with 5TB of\n",
      "high-quality text collected from the Internet. A Massive Data\n",
      "Filtering System (MDFS) built on Spark is developed to pro-\n",
      "----\n",
      "Filtering System (MDFS) built on Spark is developed to pro-\n",
      "cess the raw data via coarse and fine filtering techniques. To\n",
      "speed up the training of Yuan 1.0 to save energy expenses and\n",
      "carbon emissions, various factors that improve the performance\n",
      "of distributed training are incorporated in architecture and train-\n",
      "ing: like increasing the hidden state size improves pipeline and\n",
      "tensor parallelism performance, larger micro batches improve\n",
      "----\n",
      "tensor parallelism performance, larger micro batches improve\n",
      "pipeline parallelism performance, and larger global batch size\n",
      "improve data parallelism performance. In practice, the Yuan 1.0\n",
      "model performs well on text classification, Winograd Schema,\n",
      "natural language inference, and reading comprehension tasks.\n",
      "Gopher [116]: The Gopher family of models ranges from\n",
      "44M to 280B parameters in size to study the effect of scale\n",
      "on the LLMs performance. The 280B model beats GPT-3 [6],\n",
      "----\n",
      "on the LLMs performance. The 280B model beats GPT-3 [6],\n",
      "Jurrasic-1 [112], MT-NLG [117], and others on 81% of the\n",
      "evaluated tasks.\n",
      "ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan extends ERNIE 3.0\n",
      "by training a larger model with 26x the number of parameters\n",
      "of the latter. This bigger model outperformed other state-of-the-\n",
      "art models in 68 NLP tasks. LLMs produce text with incorrect\n",
      "facts. In order to have control of the generated text with fac-\n",
      "----\n",
      "facts. In order to have control of the generated text with fac-\n",
      "tual consistency, ERNIE 3.0 Titan adds another task, Credible\n",
      "and Controllable Generations, to its multi-task learning setup.\n",
      "8\n",
      "----\n",
      "It introduces additional self-supervised adversarial and control-\n",
      "lable language modeling losses to the pre-training step, which\n",
      "enables ERNIE 3.0 Titan to beat other LLMs in their manually\n",
      "selected Factual QA task set evaluations.\n",
      "GPT-NeoX-20B [118]: An auto-regressive model that largely\n",
      "follows GPT-3 with a few deviations in architecture design,\n",
      "trained on the Pile dataset without any data deduplication. GPT-\n",
      "NeoX has parallel attention and feed-forward layers in a trans-\n",
      "----\n",
      "NeoX has parallel attention and feed-forward layers in a trans-\n",
      "former block, given in Eq. 4, that increases throughput by 15%.\n",
      "It uses rotary positional embedding [66], applying it to only\n",
      "25% of embedding vector dimension as in [119]. This reduces\n",
      "the computation without performance degradation. As opposed\n",
      "to GPT-3, which uses dense and sparse layers, GPT-NeoX-20B\n",
      "uses only dense layers. The hyperparameter tuning at this scale\n",
      "is difficult; therefore, the model chooses hyperparameters from\n",
      "----\n",
      "is difficult; therefore, the model chooses hyperparameters from\n",
      "the method [6] and interpolates values between 13B and 175B\n",
      "models for the 20B model. The model training is distributed\n",
      "among GPUs using both tensor and pipeline parallelism.\n",
      "x + Attn(LN1(x)) + FF(LN2(x))\n",
      "(4)\n",
      "OPT [14]: It is a clone of GPT-3, developed to open-source\n",
      "a model that replicates GPT-3 performance. Training of OPT\n",
      "employs dynamic loss scaling [120] and restarts from an earlier\n",
      "----\n",
      "employs dynamic loss scaling [120] and restarts from an earlier\n",
      "checkpoint with a lower learning rate whenever loss divergence\n",
      "is observed. Overall, the performance of OPT-175B models is\n",
      "comparable to the GPT3-175B model.\n",
      "BLOOM [13]: A causal decoder model trained on the ROOTS\n",
      "corpus to open-source an LLM. The architecture of BLOOM is\n",
      "shown in Figure 9, with differences like ALiBi positional em-\n",
      "bedding, an additional normalization layer after the embedding\n",
      "----\n",
      "bedding, an additional normalization layer after the embedding\n",
      "layer as suggested by the bitsandbytes1 library. These changes\n",
      "stabilize training with improved downstream performance.\n",
      "GLaM [91]: Generalist Language Model (GLaM) represents a\n",
      "family of language models using a sparsely activated decoder-\n",
      "only mixture-of-experts (MoE) structure [121, 90].\n",
      "To gain\n",
      "more model capacity while reducing computation, the experts\n",
      "are sparsely activated where only the best two experts are used\n",
      "----\n",
      "are sparsely activated where only the best two experts are used\n",
      "to process each input token. The largest GLaM model, GLaM\n",
      "(64B/64E), is about 7× larger than GPT-3 [6], while only part of\n",
      "the parameters are activated per input token. The largest GLaM\n",
      "(64B/64E) model achieves better overall results as compared\n",
      "to GPT-3 while consuming only one-third of GPT-3’s training\n",
      "energy.\n",
      "MT-NLG [117]: A 530B causal decoder based on the GPT-\n",
      "2 architecture that has roughly 3× GPT-3 model parameters.\n",
      "----\n",
      "2 architecture that has roughly 3× GPT-3 model parameters.\n",
      "MT-NLG is trained on filtered high-quality data collected from\n",
      "various public datasets and blends various types of datasets in a\n",
      "single batch, which beats GPT-3 on several evaluations.\n",
      "Chinchilla [96]: A causal decoder trained on the same dataset\n",
      "as the Gopher [116] but with a little different data sampling\n",
      "distribution (sampled from MassiveText). The model architec-\n",
      "ture is similar to the one used for Gopher, with the exception of\n",
      "----\n",
      "ture is similar to the one used for Gopher, with the exception of\n",
      "AdamW optimizer instead of Adam. Chinchilla identifies the\n",
      "1https://github.com/TimDettmers/bitsandbytes\n",
      "Figure 9: The BLOOM architecture example sourced from [13].\n",
      "relationship that model size should be doubled for every dou-\n",
      "bling of training tokens. Over 400 language models ranging\n",
      "from 70 million to over 16 billion parameters on 5 to 500 bil-\n",
      "lion tokens are trained to get the estimates for compute-optimal\n",
      "----\n",
      "lion tokens are trained to get the estimates for compute-optimal\n",
      "training under a given budget. The authors train a 70B model\n",
      "with the same compute budget as Gopher (280B) but with 4\n",
      "times more data. It outperforms Gopher [116], GPT-3 [6], and\n",
      "others on various downstream tasks, after fine-tuning.\n",
      "AlexaTM [122]: An encoder-decoder model, where encoder\n",
      "weights and decoder embeddings are initialized with a pre-\n",
      "trained encoder to speed up training. The encoder stays frozen\n",
      "----\n",
      "trained encoder to speed up training. The encoder stays frozen\n",
      "for the initial 100k steps and is later unfrozen for end-to-end\n",
      "training. The model is trained on a combination of denoising\n",
      "and causal language modeling (CLM) objectives, concatenat-\n",
      "ing a [CLM] token at the beginning for mode switching. Dur-\n",
      "ing training, the CLM task is applied for 20% of the time, which\n",
      "improves the in-context learning performance.\n",
      "PaLM [15]: A causal decoder with parallel attention and\n",
      "----\n",
      "PaLM [15]: A causal decoder with parallel attention and\n",
      "feed-forward layers similar to Eq. 4, speeding up training by\n",
      "a factor of 15. Additional changes to the conventional trans-\n",
      "former model include SwiGLU activation, RoPE embeddings,\n",
      "multi-query attention that saves computation cost during decod-\n",
      "ing, and shared input-output embeddings. During training, loss\n",
      "spiking was observed, and to fix it, model training was restarted\n",
      "from a 100-step earlier checkpoint by skipping 200-500 batches\n",
      "----\n",
      "from a 100-step earlier checkpoint by skipping 200-500 batches\n",
      "around the spike. Moreover, the model was found to memo-\n",
      "rize around 2.4% of the training data at the 540B model scale,\n",
      "whereas this number was lower for smaller models.\n",
      "PaLM-2 [123]: A smaller multi-lingual variant of PaLM,\n",
      "trained for larger iterations on a better quality dataset. PaLM-\n",
      "2 shows significant improvements over PaLM, while reducing\n",
      "training and inference costs due to its smaller size. To lessen\n",
      "----\n",
      "training and inference costs due to its smaller size. To lessen\n",
      "toxicity and memorization, it appends special tokens with a\n",
      "fraction of pre-training data, which shows a reduction in gener-\n",
      "ating harmful responses.\n",
      "U-PaLM [124]: This method trains PaLM for 0.1% addi-\n",
      "tional compute with the UL2 (also named as UL2Restore) ob-\n",
      "jective [125], using the same dataset it outperforms the baseline\n",
      "significantly on various NLP tasks, including zero-shot, few-\n",
      "----\n",
      "significantly on various NLP tasks, including zero-shot, few-\n",
      "shot, commonsense reasoning, CoT, etc. Training with UL2R\n",
      "involves converting a causal decoder PaLM to a non-causal de-\n",
      "coder PaLM and employing 50% sequential denoising, 25%\n",
      "regular denoising, and 25% extreme denoising loss functions.\n",
      "9\n",
      "----\n",
      "UL2 [125]: An encoder-decoder architecture trained using a\n",
      "mixture of denoisers (MoD) objective. Denoisers include 1)\n",
      "R-Denoiser: a regular span masking, 2) S-Denoiser: which cor-\n",
      "rupts consecutive tokens of a large sequence and 3) X-Denoiser:\n",
      "which corrupts a large number of tokens randomly. During pre-\n",
      "training, UL2 includes a denoiser token from R, S, X to rep-\n",
      "resent a denoising setup. It helps improve fine-tuning perfor-\n",
      "mance for downstream tasks that bind the task to one of the up-\n",
      "----\n",
      "mance for downstream tasks that bind the task to one of the up-\n",
      "stream training modes. This MoD style of training outperforms\n",
      "the T5 model on many benchmarks.\n",
      "GLM-130B [33]: GLM-130B is a bilingual (English and Chi-\n",
      "nese) model trained using an auto-regressive mask infilling pre-\n",
      "training objective similar to the GLM [126]. This training style\n",
      "makes the model bidirectional as compared to GPT-3, which is\n",
      "unidirectional. As opposed to GLM, the training of GLM-130B\n",
      "----\n",
      "unidirectional. As opposed to GLM, the training of GLM-130B\n",
      "includes a small amount of multi-task instruction pre-training\n",
      "data (5% of the total data) along with self-supervised mask in-\n",
      "filling. To stabilize the training, it applies embedding layer gra-\n",
      "dient shrink.\n",
      "LLaMA [127, 21]: A set of decoder-only language models\n",
      "varying from 7B to 70B parameters. LLaMA models series is\n",
      "the most famous among the community for parameter efficiency\n",
      "and instruction tuning.\n",
      "----\n",
      "the most famous among the community for parameter efficiency\n",
      "and instruction tuning.\n",
      "LLaMA-1 [127]: Implements efficient causal attention [128]\n",
      "by not storing and computing masked attention weights and\n",
      "key/query scores. Another optimization is reducing the number\n",
      "of activations recomputed in the backward pass, as in [129].\n",
      "LLaMA-2 [21]: This work is more focused on fine-tuning a\n",
      "safer and better LLaMA-2-Chat model for dialogue generation.\n",
      "----\n",
      "safer and better LLaMA-2-Chat model for dialogue generation.\n",
      "The pre-trained model has 40% more training data with a larger\n",
      "context length and grouped-query attention.\n",
      "LLaMA-3/3.1 [130]: A collection of models trained on a\n",
      "seven times larger dataset as compared to LLaMA-2 with dou-\n",
      "ble the context length, outperforming its previous variants and\n",
      "other models.\n",
      "PanGu-Σ [92]: An autoregressive model with parameters\n",
      "copied from PanGu-α and extended to a trillion scale with Ran-\n",
      "----\n",
      "copied from PanGu-α and extended to a trillion scale with Ran-\n",
      "dom Routed Experts (RRE), the architectural diagram is shown\n",
      "in Figure 10. RRE is similar to the MoE architecture, with\n",
      "distinctions at the second level, where tokens are randomly\n",
      "routed to experts in a domain instead of using a learnable gat-\n",
      "ing method. The model has bottom layers densely activated and\n",
      "shared across all domains, whereas top layers are sparsely ac-\n",
      "tivated according to the domain. This training style allows for\n",
      "----\n",
      "tivated according to the domain. This training style allows for\n",
      "extracting task-specific models and reduces catastrophic forget-\n",
      "ting effects in the case of continual learning.\n",
      "Mixtral8x22b [131]: A mixture-of-experts (MoE) model with\n",
      "eight distinct experts routes each token to two experts at each\n",
      "layer and combines the outputs additively.\n",
      "Snowflake Arctic [132]: Arctic LLM is a hybrid of dense and\n",
      "mixture-of-experts (MoE) architecture. The MoE (128×3.66B\n",
      "----\n",
      "mixture-of-experts (MoE) architecture. The MoE (128×3.66B\n",
      "MLP experts) is parallel to the dense transformer (10B) with\n",
      "only two experts activated. The model has many experts, com-\n",
      "pared to other MoE LLMs [131, 133], to increase the model\n",
      "capacity and provide an opportunity to choose among many ex-\n",
      "perts for a diverse configuration. The model has 480B param-\n",
      "eters, and only 17B are active during a forward pass, reducing\n",
      "the computation significantly.\n",
      "----\n",
      "eters, and only 17B are active during a forward pass, reducing\n",
      "the computation significantly.\n",
      "Grok [133, 134]: Grok is a family of LLMs including Grok-1\n",
      "and Grok-1.5, released by XAI.\n",
      "Grok-1 [133]: Grok-1 is a 314B parameters language MoE\n",
      "model (eight experts), where two experts are activated per to-\n",
      "ken.\n",
      "Grok-1.5 [134]: Grok-1.5 is a multi-modal LLM with a larger\n",
      "context length and improved performance.\n",
      "Gemini [135, 136]: Gemini replaces Bard (based on PaLM)\n",
      "----\n",
      "context length and improved performance.\n",
      "Gemini [135, 136]: Gemini replaces Bard (based on PaLM)\n",
      "with multi-modal capabilities and significant language model-\n",
      "ing performance improvements.\n",
      "Gemini-1 [135]: The first-ever auto-regressive model to\n",
      "achieve human-level capabilities on the MMLU benchmark.\n",
      "Gemini-1.5 [136]: A multi-modal LLM with MoE architec-\n",
      "ture builds on the findings of Gemini-1. The model has a 2M\n",
      "context window and can reason over information up to 10M\n",
      "----\n",
      "context window and can reason over information up to 10M\n",
      "tokens. Such large context windows were never achieved pre-\n",
      "viously and shown to have a huge impact on performance gain.\n",
      "Nemotron-4 340B [137]: A decoder-only model that has been\n",
      "aligned on 98% synthetic data and only 2% manually annotated\n",
      "data. Utilizing synthetic data at a large proportion improves the\n",
      "model performance significantly. The paper suggested intro-\n",
      "ducing alignment data with a smaller subset of previously seen\n",
      "----\n",
      "ducing alignment data with a smaller subset of previously seen\n",
      "data during the late stage of the model pre-training, enabling the\n",
      "smooth transition from the pre-trained stage to the final train-\n",
      "ing stage. To train better instruction-following models, weaker\n",
      "models are trained into stronger models iteratively. The syn-\n",
      "thetic data generated by the weaker instruction-tuned model is\n",
      "used to train a base model which is later supervised fine-tuned\n",
      "outperforming the weaker model.\n",
      "----\n",
      "used to train a base model which is later supervised fine-tuned\n",
      "outperforming the weaker model.\n",
      "DeepSeek [138]: DeepSeek studies the LLMs scaling laws\n",
      "in detail to determine the optimal non-embedding model size\n",
      "and training data. The experiments were performed for 8 bud-\n",
      "gets ranging from 1e17 to 3e20 training FLOPs. Each compute\n",
      "budget was tested against ten different models/data scales. The\n",
      "batch size and learning rates were also fitted for the given com-\n",
      "----\n",
      "batch size and learning rates were also fitted for the given com-\n",
      "pute budget finding that the batch size should increase with\n",
      "the increased compute budget while decreasing the learning\n",
      "rate. Following are the equations for the optimal batch-size (B),\n",
      "learning rate (η), model size (M), and data (D):\n",
      "Bopt = 0.2920.C0.3271\n",
      "ηopt = 0.3118.C−0.1250\n",
      "Mopt = Mbase.Ca\n",
      "Dopt = Dbase.Cb\n",
      "Mbase = 0.1715, Dbase = 5.8316, a = 0.5243, b = 0.4757\n",
      "(5)\n",
      "DeepSeek-v2 [139]: An MoE model that introduces multi-\n",
      "----\n",
      "(5)\n",
      "DeepSeek-v2 [139]: An MoE model that introduces multi-\n",
      "head latent attention (MLA) to reduce inference costs, by com-\n",
      "pressing Key-Value (KV) cache into a latent vector.\n",
      "MLA\n",
      "achieves better performance than multi-head attention (MHA),\n",
      "and other efficient attention mechanisms such as grouped query\n",
      "attention (GQA), multi-query attention (MQA), etc. Because\n",
      "of MLA, DeepSeek-v2 achieves 5.76 times faster inference\n",
      "throughput as compared to DeepSeek [138].\n",
      "10\n",
      "----\n",
      "3.1.2. Coding\n",
      "CodeGen [140]:\n",
      "CodeGen has a similar architecture to\n",
      "PaLM [15], i.e., parallel attention, MLP layers, and RoPE em-\n",
      "beddings. The model is trained on both natural language and\n",
      "programming language data sequentially (trained on the first\n",
      "dataset, then the second, and so on) on the following datasets\n",
      "1) PILE, 2) BIGQUERY, and 3) BIGPYTHON. CodeGen pro-\n",
      "posed a multi-step approach to synthesizing code. The purpose\n",
      "is to simplify the generation of long sequences where the previ-\n",
      "----\n",
      "is to simplify the generation of long sequences where the previ-\n",
      "ous prompt and generated code are given as input with the next\n",
      "prompt to generate the next code sequence. CodeGen open-\n",
      "source a Multi-Turn Programming Benchmark (MTPB) to eval-\n",
      "uate multi-step program synthesis.\n",
      "Codex [141]: This LLM is trained on a subset of public Python\n",
      "Github repositories to generate code from docstrings. Com-\n",
      "puter programming is an iterative process where the programs\n",
      "----\n",
      "puter programming is an iterative process where the programs\n",
      "are often debugged and updated before fulfilling the require-\n",
      "ments. Similarly, Codex generates 100 versions of a program\n",
      "by repetitive sampling for a given description, which produces\n",
      "a working solution for 77.5% of the problems passing unit tests.\n",
      "Its powerful version powers Github Copilot2.\n",
      "AlphaCode [142]: A set of large language models, ranging\n",
      "from 300M to 41B parameters, designed for competition-level\n",
      "----\n",
      "from 300M to 41B parameters, designed for competition-level\n",
      "code generation tasks. It uses the multi-query attention [143] to\n",
      "reduce memory and cache costs. Since competitive program-\n",
      "ming problems highly require deep reasoning and an under-\n",
      "standing of complex natural language algorithms, the Alpha-\n",
      "Code models are pre-trained on filtered GitHub code in popular\n",
      "languages and then fine-tuned on a new competitive program-\n",
      "ming dataset named CodeContests. The CodeContests dataset\n",
      "----\n",
      "ming dataset named CodeContests. The CodeContests dataset\n",
      "mainly contains problems, solutions, and test cases collected\n",
      "from the Codeforces platform3. The pre-training employs stan-\n",
      "dard language modeling objectives, while GOLD [144] with\n",
      "tempering [145] serves as the training objective for the fine-\n",
      "tuning on CodeContests data. To evaluate the performance of\n",
      "AlphaCode, simulated programming competitions are hosted\n",
      "on the Codeforces platform: overall, AlphaCode ranks at the\n",
      "----\n",
      "on the Codeforces platform: overall, AlphaCode ranks at the\n",
      "top 54.3% among over 5000 competitors, where its Codeforces\n",
      "rating is within the top 28% of recently participated users.\n",
      "CodeT5+ [34]: CodeT5+ is based on CodeT5 [146], with\n",
      "shallow encoder and deep decoder, trained in multiple stages\n",
      "initially unimodal data (code) and later bimodal data (text-code\n",
      "pairs). Each training stage has different training objectives and\n",
      "activates different model blocks encoder, decoder, or both ac-\n",
      "----\n",
      "activates different model blocks encoder, decoder, or both ac-\n",
      "cording to the task. The unimodal pre-training includes span\n",
      "denoising and CLM objectives, whereas bimodal pre-training\n",
      "objectives contain contrastive learning, matching, and CLM for\n",
      "text-code pairs. CodeT5+ adds special tokens with the text to\n",
      "enable task modes, for example, [CLS ] for contrastive loss,\n",
      "[Match] for text-code matching, etc.\n",
      "StarCoder [147]: A decoder-only model with the SantaCoder\n",
      "----\n",
      "[Match] for text-code matching, etc.\n",
      "StarCoder [147]: A decoder-only model with the SantaCoder\n",
      "architecture, employing Flash attention to scale up the context\n",
      "length to 8k. The StarCoder trains an encoder to filter names,\n",
      "2https://github.com/features/copilot\n",
      "3https://codeforces.com/\n",
      "emails, and other personal data from the training data. Its fine-\n",
      "tuned variant outperforms PaLM, LLaMA, and LAMDA on\n",
      "HumanEval and MBPP benchmarks.\n",
      "3.1.3. Scientific Knowledge\n",
      "----\n",
      "HumanEval and MBPP benchmarks.\n",
      "3.1.3. Scientific Knowledge\n",
      "Galactica [148]: A large curated corpus of human scientific\n",
      "knowledge with 48 million papers, textbooks, lecture notes,\n",
      "millions of compounds and proteins, scientific websites, en-\n",
      "cyclopedias, and more are trained using the metaseq library3,\n",
      "which is built on PyTorch and fairscale [149]. The model wraps\n",
      "reasoning datasets with the < work > token to provide step-by-\n",
      "step reasoning context to the model, which has been shown to\n",
      "----\n",
      "step reasoning context to the model, which has been shown to\n",
      "improve the performance on reasoning tasks.\n",
      "3.1.4. Dialog\n",
      "LaMDA [150]: A decoder-only model pre-trained on pub-\n",
      "lic dialog data, public dialog utterances, and public web doc-\n",
      "uments, where more than 90% of the pre-training data is in\n",
      "English. LaMDA is trained with the objective of producing re-\n",
      "sponses that exhibit high levels of quality, safety, and grounded-\n",
      "ness. To achieve this, discriminative and generative fine-tuning\n",
      "----\n",
      "ness. To achieve this, discriminative and generative fine-tuning\n",
      "techniques are incorporated to enhance the model’s safety and\n",
      "quality aspects. As a result, the LaMDA models can be utilized\n",
      "as a general language model performing various tasks.\n",
      "3.1.5. Finance\n",
      "BloombergGPT [151]: A non-causal decoder model trained\n",
      "using both financial (“FINPILE” from the Bloomberg archive)\n",
      "and general-purpose datasets. The model’s architecture is sim-\n",
      "ilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\n",
      "----\n",
      "ilar to the BLOOM [13] and OPT [14]. It allocates 50B param-\n",
      "eters to different blocks of the model using the approach [113].\n",
      "For effective training, BloombergGPT packs documents to-\n",
      "gether with < |endoftext| > to use the maximum sequence\n",
      "length, uses warmup batch size starting from 1024 to 2048, and\n",
      "manually reduces the learning rate multiple times during the\n",
      "training.\n",
      "Xuan Yuan 2.0 [152]: A Chinese financial chat model with\n",
      "BLOOM’s [13] architecture trained on a combination of general\n",
      "----\n",
      "BLOOM’s [13] architecture trained on a combination of general\n",
      "purpose, financial, general purpose instructions, and financial\n",
      "institutions datasets. Xuan Yuan 2.0 combined the pre-training\n",
      "and fine-tuning stages to avoid catastrophic forgetting.\n",
      "3.2. Fine-Tuned LLMs\n",
      "Pre-trained LLMs have excellent generalization abilities to\n",
      "unseen tasks. However, because they are generally trained with\n",
      "the objective of next token prediction, LLMs have limited ca-\n",
      "----\n",
      "the objective of next token prediction, LLMs have limited ca-\n",
      "pacity to follow user intent and are prone to generate unethical,\n",
      "toxic or inaccurate responses [20]. For their effective utiliza-\n",
      "tion, LLMs are fine-tuned to follow instructions [16, 17, 97] and\n",
      "generate safe responses [20], which also results in increasing\n",
      "zero-shot, few-shot, and cross-task generalization [97, 16, 18],\n",
      "with minimal compute increment, e.g., 0.2% of the total pre-\n",
      "training for PaLM 540B [16].\n",
      "----\n",
      "with minimal compute increment, e.g., 0.2% of the total pre-\n",
      "training for PaLM 540B [16].\n",
      "We review various fine-tuned LLMs and strategies for effective\n",
      "fine-tuning in this section.\n",
      "11\n",
      "----\n",
      "Table 1: Noteworthy findings and insights of pre-trained Large Language Models.\n",
      "Models\n",
      "Findings & Insights\n",
      "T5\n",
      "• Encoder and decoder with shared parameters perform equivalently when parameters are not shared\n",
      "• Fine-tuning model layers (adapter layers) work better than the conventional way of training on only\n",
      "classification layers\n",
      "GPT-3\n",
      "• Few-shot performance of LLMs is better than the zero-shot, suggesting that LLMs are meta-\n",
      "learners\n",
      "mT5\n",
      "----\n",
      "learners\n",
      "mT5\n",
      "• Large multi-lingual models perform equivalently to single language models on downstream tasks.\n",
      "However, smaller multi-lingual models perform worse\n",
      "PanGu-α\n",
      "• LLMs have good few shot capabilities\n",
      "CPM-2\n",
      "• Prompt fine-tuning requires updating very few parameters while achieving performance compara-\n",
      "ble to full model fine-tuning\n",
      "• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\n",
      "----\n",
      "• Prompt fine-tuning takes more time to converge as compared to full model fine-tuning\n",
      "• Inserting prompt tokens in-between sentences can allow the model to understand relations between\n",
      "sentences and long sequences\n",
      "• In an analysis, CPM-2 finds that prompts work as a provider (additional context) and aggregator\n",
      "(aggregate information with the input text) for the model\n",
      "ERNIE 3.0\n",
      "• A modular LLM architecture with a universal representation module and task-specific representa-\n",
      "----\n",
      "• A modular LLM architecture with a universal representation module and task-specific representa-\n",
      "tion module helps in the finetuning phase\n",
      "• Optimizing the parameters of a task-specific representation network during the fine-tuning phase is\n",
      "an efficient way to take advantage of the powerful pre-trained model\n",
      "Jurassic-1\n",
      "• The performance of LLM is highly related to the network size\n",
      "• To improve runtime performance, more operations can be performed in parallel (width) rather than\n",
      "----\n",
      "• To improve runtime performance, more operations can be performed in parallel (width) rather than\n",
      "sequential (depth)\n",
      "• To efficiently represent and fit more text in the same context length, the model uses a larger vo-\n",
      "cabulary to train a SentencePiece tokenizer without restricting it to word boundaries. This further\n",
      "benefits in few-shot learning tasks\n",
      "HyperCLOVA\n",
      "• By employing prompt-based tuning, the performances of models can be improved, often surpassing\n",
      "----\n",
      "• By employing prompt-based tuning, the performances of models can be improved, often surpassing\n",
      "those of state-of-the-art models when the backward gradients of inputs are accessible\n",
      "Yuan 1.0\n",
      "• The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting\n",
      "behavior in zero-shot and few-shot learning\n",
      "Gopher\n",
      "• Relative encodings enable the model to evaluate for longer sequences than training.\n",
      "ERNIE 3.0 Titan\n",
      "----\n",
      "ERNIE 3.0 Titan\n",
      "• Additional self-supervised adversarial loss to distinguish between real and generated text improves\n",
      "the model performance as compared to ERNIE 3.0\n",
      "GPT-NeoX-20B\n",
      "• Parallel attention + FF layers speed-up training 15% with the same performance as with cascaded\n",
      "layers\n",
      "• Initializing feed-forward output layers before residuals with scheme in [153] avoids activations\n",
      "from growing with increasing depth and width\n",
      "• Training on Pile outperforms GPT-3 on five-shot\n",
      "----\n",
      "from growing with increasing depth and width\n",
      "• Training on Pile outperforms GPT-3 on five-shot\n",
      "Table Continued on Next Page\n",
      "12\n",
      "----\n",
      "Models\n",
      "Findings & Insights\n",
      "OPT\n",
      "• Restart training from an earlier checkpoint with a lower learning rate if loss diverges\n",
      "• Model is prone to generate repetitive text and stuck in a loop\n",
      "Galactica\n",
      "• Galactica’s performance has continued to improve across validation set, in-domain, and out-of-\n",
      "domain benchmarks, even with multiple repetitions of the corpus, which is superior to existing\n",
      "research on LLMs\n",
      "• A working memory token approach can achieve strong performance over existing methods on\n",
      "----\n",
      "• A working memory token approach can achieve strong performance over existing methods on\n",
      "mathematical MMLU and MATH benchmarks. It sets a new state-of-the-art on several downstream\n",
      "tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\n",
      "GLaM\n",
      "• The model capacity can be maintained at reduced computation by replacing the feed-forward layer\n",
      "in each transformer layer with a mixture-of-experts (MoE)\n",
      "• The model trained on filtered data shows consistently better performances on both NLG and NLU\n",
      "----\n",
      "• The model trained on filtered data shows consistently better performances on both NLG and NLU\n",
      "tasks, where the effect of filtering is more significant on the former tasks\n",
      "• Filtered pretraining corpora play a crucial role in the generation capability of LLMs, especially for\n",
      "the downstream tasks\n",
      "• The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in\n",
      "the MoE layer. Given a fixed budget of computation, more experts contribute to a better perfor-\n",
      "mance\n",
      "----\n",
      "mance\n",
      "LaMDA\n",
      "• The model can be fine-tuned to learn to call different external information resources and tools\n",
      "AlphaCode\n",
      "• For higher effectiveness and efficiency, a transformer model can be asymmetrically constructed\n",
      "with a shallower encoder and a deeper decoder\n",
      "• To achieve better performances, it is necessary to employ strategies such as massively scaling\n",
      "upsampling, followed by the filtering and clustering of samples into a compact set\n",
      "----\n",
      "upsampling, followed by the filtering and clustering of samples into a compact set\n",
      "• The utilization of novel sampling-efficient transformer architectures designed to facilitate large-\n",
      "scale sampling is crucial\n",
      "• Simplifying problem descriptions can effectively improve the model’s performance\n",
      "Chinchilla\n",
      "• The model size and the number of training tokens should be scaled proportionately: for each dou-\n",
      "bling of the model size, the number of training tokens should be doubled as well\n",
      "PaLM\n",
      "----\n",
      "bling of the model size, the number of training tokens should be doubled as well\n",
      "PaLM\n",
      "• English-centric models produce better translations when translating to English as compared to non-\n",
      "English\n",
      "• Generalized models can have equivalent performance for language translation to specialized small\n",
      "models\n",
      "• Larger models have a higher percentage of training data memorization\n",
      "• Performance has not yet saturated even at 540B scale, which means larger models are likely to\n",
      "perform better\n",
      "AlexaTM\n",
      "----\n",
      "perform better\n",
      "AlexaTM\n",
      "• Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the\n",
      "context than decoder-only\n",
      "• Causal Language Modeling (CLM) task can be added to benefit the model with efficient in-context\n",
      "learning\n",
      "• Placing layer norm at the beginning of each transformer layer improves the training stability\n",
      "Table Continued on Next Page\n",
      "13\n",
      "----\n",
      "Models\n",
      "Findings & Insights\n",
      "U-PaLM\n",
      "• Training with a mixture of denoisers outperforms PaLM when trained further for a few more FLOPs\n",
      "• Training with a mixture of denoisers improves the infilling ability and open-ended text generation\n",
      "diversity\n",
      "UL2\n",
      "• Mode switching training enables better performance on downstream tasks\n",
      "• CoT prompting outperforms standard prompting for UL2\n",
      "GLM-130B\n",
      "• Pre-training data with a small proportion of multi-task instruction data improves the overall model\n",
      "performance\n",
      "----\n",
      "performance\n",
      "CodeGen\n",
      "• Multi-step prompting for code synthesis leads to a better user intent understanding and code gen-\n",
      "eration\n",
      "LLaMA\n",
      "• A constant performance improvement is observed when scaling the model\n",
      "• Smaller models can achieve good performances with more training data and computing time\n",
      "PanGu-Σ\n",
      "• Sparse models provide the benefits of large models at a lower computation cost\n",
      "• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for\n",
      "----\n",
      "• Randomly Routed Experts reduces catastrophic forgetting effects which in turn is essential for\n",
      "continual learning\n",
      "• Randomly Routed Experts allow extracting a domain-specific sub-model in deployment which is\n",
      "cost-efficient while maintaining a performance similar to the original\n",
      "BloombergGPT\n",
      "• Pre-training with general-purpose and task-specific data improves task performance without hurt-\n",
      "ing other model capabilities\n",
      "XuanYuan 2.0\n",
      "----\n",
      "ing other model capabilities\n",
      "XuanYuan 2.0\n",
      "• Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting\n",
      "CodeT5+\n",
      "• Causal LM is crucial for a model’s generation capability in encoder-decoder architectures\n",
      "• Multiple training objectives like span corruption, Causal LM, matching, etc complement each other\n",
      "for better performance\n",
      "StarCoder\n",
      "• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\n",
      "LLaMA-2\n",
      "----\n",
      "• HHH prompt by Anthropic allows the model to follow instructions without fine-tuning\n",
      "LLaMA-2\n",
      "• Model trained on unfiltered data is more toxic but may perform better on downstream tasks after\n",
      "fine-tuning\n",
      "• Model trained on unfiltered data requires fewer samples for safety alignment\n",
      "PaLM-2\n",
      "• Data quality is important to train better models\n",
      "• Model and data size should be scaled with 1:1 proportions\n",
      "• Smaller models trained for larger iterations outperform larger models\n",
      "LLaMA-3/3.1\n",
      "----\n",
      "• Smaller models trained for larger iterations outperform larger models\n",
      "LLaMA-3/3.1\n",
      "• Increasing batch size gradually stabilizes the training without loss spikes\n",
      "• High-quality data at the final stages of training improves the model performance\n",
      "• Increasing model context length windows step-wise allows it to better adapt to various sequence\n",
      "lengths\n",
      "Nemotron-40B\n",
      "• Model aligned iteratively on synthetic data with data generated from the previously aligned model\n",
      "achieves competitive performance\n",
      "----\n",
      "achieves competitive performance\n",
      "DeepSeek\n",
      "• Batch size should increase with the increase in compute budget while decreasing the learning rate\n",
      "DeepSeek-v2\n",
      "• Mult-head latent attention (MLA) performs better than multi-head attention (MHA) while requiring\n",
      "a significantly smaller KV cache, therefore achieving faster data generation\n",
      "14\n",
      "----\n",
      "Table 2: Key insights and findings from the study of instruction-tuned Large Language Models.\n",
      "Models\n",
      "Findings & Insights\n",
      "T0\n",
      "• Multi-task prompting enables zero-shot generalization and outperforms baselines\n",
      "• Even a single prompt per dataset task is enough to improve performance\n",
      "WebGPT\n",
      "• To aid the model in effectively filtering and utilizing relevant information, human labelers play a\n",
      "crucial role in answering questions regarding the usefulness of the retrieved documents\n",
      "----\n",
      "crucial role in answering questions regarding the usefulness of the retrieved documents\n",
      "• Interacting a fine-tuned language model with a text-based web-browsing environment can improve\n",
      "end-to-end retrieval and synthesis via imitation learning and reinforcement learning\n",
      "• Generating answers with references can make labelers easily judge the factual accuracy of answers\n",
      "Tk-INSTRUCT\n",
      "• Instruction tuning leads to a stronger generalization of unseen tasks\n",
      "----\n",
      "Tk-INSTRUCT\n",
      "• Instruction tuning leads to a stronger generalization of unseen tasks\n",
      "• More tasks improve generalization whereas only increasing task instances does not help\n",
      "• Supervised trained models are better than generalized models\n",
      "• Models pre-trained with instructions and examples perform well for different types of inputs\n",
      "mT0 and BLOOMZ\n",
      "• Instruction tuning enables zero-shot generalization to tasks never seen before\n",
      "----\n",
      "mT0 and BLOOMZ\n",
      "• Instruction tuning enables zero-shot generalization to tasks never seen before\n",
      "• Multi-lingual training leads to even better zero-shot generalization for both English and non-\n",
      "English\n",
      "• Training on machine-translated prompts improves performance for held-out tasks with non-English\n",
      "prompts\n",
      "• English only fine-tuning on multilingual pre-trained language model is enough to generalize to\n",
      "other pre-trained language tasks\n",
      "OPT-IML\n",
      "----\n",
      "other pre-trained language tasks\n",
      "OPT-IML\n",
      "• Creating a batch with multiple task examples is important for better performance\n",
      "• Only example proportional sampling is not enough, training datasets should also be proportional\n",
      "for better generalization/performance\n",
      "• Fully held-out and partially supervised tasks performance improves by scaling tasks or categories\n",
      "whereas fully supervised tasks have no effect\n",
      "• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\n",
      "----\n",
      "• Including small amounts i.e. 5% of pretraining data during fine-tuning is effective\n",
      "• Only 1% reasoning data improves the performance, adding more deteriorates performance\n",
      "• Adding dialogue data makes the performance worse\n",
      "Sparrow\n",
      "• Labelers’ judgment and well-defined alignment rules help the model generate better responses\n",
      "• Good dialogue goals can be broken down into detailed natural language rules for the agent and the\n",
      "raters\n",
      "----\n",
      "raters\n",
      "• The combination of reinforcement learning (RL) with reranking yields optimal performance in\n",
      "terms of preference win rates and resilience against adversarial probing\n",
      "Flan\n",
      "• Finetuning with CoT improves performance on held-out tasks\n",
      "• Fine-tuning along with CoT data improves reasoning abilities\n",
      "• CoT tuning improves zero-shot reasoning\n",
      "• Performance improves with more tasks\n",
      "• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\n",
      "----\n",
      "• Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models\n",
      "• Improving the model’s performance with instruction tuning is compute-efficient\n",
      "• Multitask prompting enables zero-shot generalization abilities in LLM\n",
      "WizardCoder\n",
      "• Fine-tuning with re-written instruction-tuning data into a complex set improves performance\n",
      "LLaMA-2-Chat\n",
      "• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\n",
      "----\n",
      "• Model learns to write safe responses with fine-tuning on safe demonstrations, while additional\n",
      "RLHF step further improves model safety and make it less prone to jailbreak attacks\n",
      "LIMA\n",
      "• Less high quality data is enough for fine-tuned model generalization\n",
      "15\n",
      "----\n",
      "Figure 10: This example illustrates the PanGu-P architecture, as depicted in\n",
      "the image sourced from [92].\n",
      "3.2.1. Instruction-Tuning with Manually Created Datasets\n",
      "Numerous hand-crafted instruction-tuning datasets with\n",
      "different design choices are proposed in the literature to\n",
      "instruction-tune LLMs. The performance of fine-tuned LLMs\n",
      "depends on multiple factors, such as dataset, instruction diver-\n",
      "sity, prompting templates, model size, and training objectives.\n",
      "----\n",
      "sity, prompting templates, model size, and training objectives.\n",
      "Keeping this in view, diverse fine-tuned models have emerged\n",
      "in the literature using manually created datasets.\n",
      "The models T0 [17] and mT0 (multi-lingual) [154] employ\n",
      "templates to convert existing datasets into prompt datasets.\n",
      "They have shown improvements in generalization to zero-shot\n",
      "and held-out tasks. Tk-Instruct [18] fine-tuned the T5 model\n",
      "with in-context instructions to study generalization on unseen\n",
      "----\n",
      "with in-context instructions to study generalization on unseen\n",
      "tasks when given in-context instructions during test time. The\n",
      "model outperformed Instruct-GPT, despite being smaller in\n",
      "size, i.e., 11B parameters as compared to 175B of GPT-3.\n",
      "Increasing Tasks and Prompt Setups: Zero-shot and few-shot\n",
      "performance improves significantly by expanding task collec-\n",
      "tion and prompt styles. OPT-IML [97] and Flan [16] curated\n",
      "larger 2k and 1.8k task datasets, respectively. While increasing\n",
      "----\n",
      "larger 2k and 1.8k task datasets, respectively. While increasing\n",
      "task size alone is not enough, OPT-IML and Flan add more\n",
      "prompting setups in their datasets, zero-shot, few-shot, and\n",
      "CoT. In continuation, CoT Collection [101] fine-tunes Flan-T5\n",
      "further on 1.88M CoT samples. Another method [102] uses\n",
      "symbolic tasks with tasks in T0, Flan, etc.\n",
      "3.2.2. Instruction-Tuning with LLMs Generated Datasets\n",
      "Generating an instruction-tuning dataset requires carefully\n",
      "----\n",
      "Generating an instruction-tuning dataset requires carefully\n",
      "writing instructions and input-output pairs, which are often\n",
      "written by humans, smaller in size, and less diverse. To over-\n",
      "come this, self-instruct [19] proposed an approach to prompt\n",
      "available LLMs to generate instruction-tuning datasets. Self-\n",
      "instruct outperformed models trained on manually created\n",
      "dataset SUPER-NATURALINSTRUCTIONS (a dataset with\n",
      "1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\n",
      "----\n",
      "1600+ tasks) [18] by 33%. It starts with a seed of 175 tasks, 1\n",
      "instruction, and 1 sample per task and iteratively generates new\n",
      "instructions (52k) and instances (82k input-output pairs) using\n",
      "Figure 11: An example image shows an instance of the Flan training paradigm,\n",
      "taken from [16].\n",
      "GPT-3 [6]. Contrary to this, Dynosaur [155] uses the meta-data\n",
      "of datasets on Huggingface to prompt LLMs to generate multi-\n",
      "ple task instruction-tuning datasets.\n",
      "----\n",
      "of datasets on Huggingface to prompt LLMs to generate multi-\n",
      "ple task instruction-tuning datasets.\n",
      "LLaMA Tuned: Various models in the literature instruction-\n",
      "tune LLaMA [156] with GPT-3 [6] or GPT-4 [157] gener-\n",
      "ated datasets.\n",
      "Among these, Alpaca [158], Vicuna [159],\n",
      "and LLaMA-GPT-4 [160] are a few general-purpose fine-tuned\n",
      "models, where Alpaca is trained on 52k samples from text-\n",
      "davinci-003, Vicuna on 70k samples from ShareGPT.com, and\n",
      "LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\n",
      "----\n",
      "LLaMA-GPT-4 by re-creating Alpaca instructions from GPT-\n",
      "4. Goat [161] fine-tunes LLaMA for arithmetic tasks (1 million\n",
      "samples) by generating data from ChatGPT and outperforms\n",
      "GPT-4, PaLM, BLOOM, OPT, etc., attributing its success to the\n",
      "LLaMA’s consistent tokenization of numbers. HuaTuo [162] is\n",
      "a medical knowledge model, fine-tuned with a generated QA\n",
      "dataset of 8k instructions.\n",
      "Complex Instructions: Evol-Instruct [163, 164] prompts LLMs\n",
      "----\n",
      "dataset of 8k instructions.\n",
      "Complex Instructions: Evol-Instruct [163, 164] prompts LLMs\n",
      "to convert given instructions into a more complex set. The in-\n",
      "structions are iteratively evolved with re-writing instructions in\n",
      "complex wording and creating new instructions. With this style\n",
      "of automated instruction generation, WizardLM [163] (fine-\n",
      "tuned LLaMA on 250k instructions), outperforms Vicuna and\n",
      "Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\n",
      "Claude-Plus, Bard, and others.\n",
      "----\n",
      "Alpaca, and WizardCoder [164] (fine-tuned StarCoder) beats\n",
      "Claude-Plus, Bard, and others.\n",
      "3.2.3. Aligning with Human Preferences\n",
      "Incorporating human preferences into LLMs presents a\n",
      "significant advantage in mitigating undesirable behaviors and\n",
      "ensuring accurate outputs. The initial work on alignment, such\n",
      "as InstructGPT [20] aligns GPT-3 using a 3-step approach,\n",
      "instruction-tuning, reward modeling, and fine-tuning with\n",
      "reinforcement learning (RL). The supervised fine-tuned GPT-3\n",
      "----\n",
      "reinforcement learning (RL). The supervised fine-tuned GPT-3\n",
      "on demonstrations is queried to generate responses, which\n",
      "human labelers rank according to human values, and a reward\n",
      "model is trained on the ranked data. Lastly, the GPT-3 is trained\n",
      "with proximal policy optimization (PPO) using rewards on the\n",
      "generated data from the reward model. LLaMA 2-Chat [21]\n",
      "improves alignment by dividing reward modeling into help-\n",
      "fulness and safety rewards and using rejection sampling in\n",
      "----\n",
      "fulness and safety rewards and using rejection sampling in\n",
      "addition to PPO. The initial four versions of LLaMA 2-Chat\n",
      "are fine-tuned with rejection sampling and then with PPO on\n",
      "16\n",
      "----\n",
      "top of rejection sampling.\n",
      "Aligning with Supported Evidence: This style of alignment\n",
      "allows the model to generate responses with proofs and facts,\n",
      "reduces hallucination, and assists humans more effectively,\n",
      "which increases trust in the model’s output.\n",
      "Similar to\n",
      "the RLHF training style, a reward model is trained to rank\n",
      "generated responses containing web citations in answers\n",
      "to questions, which is later used to train the model, as in\n",
      "GopherCite [165], WebGPT [166], and Sparrow [167]. The\n",
      "----\n",
      "GopherCite [165], WebGPT [166], and Sparrow [167]. The\n",
      "ranking model in Sparrow [167] is divided into two branches,\n",
      "preference reward and rule reward, where human annotators\n",
      "adversarial probe the model to break a rule. These two rewards\n",
      "together rank a response to train with RL.\n",
      "Aligning Directly with SFT: The PPO in the RLHF pipeline\n",
      "is complex, memory-intensive, and unstable, requiring mul-\n",
      "tiple models, reward, value, policy, and reference models.\n",
      "----\n",
      "tiple models, reward, value, policy, and reference models.\n",
      "Avoiding this sophisticated alignment pipeline is possible by\n",
      "incorporating minimal changes in the supervised fine-tuning\n",
      "(SFT) pipeline as in [168, 169, 170], with better or compa-\n",
      "rable performance to PPO. Direct preference optimization\n",
      "(DPO) [168] trains a model directly on the human-preferred\n",
      "responses to maximize the likelihood of preferred against\n",
      "unpreferred responses, with per-sample importance weight.\n",
      "----\n",
      "unpreferred responses, with per-sample importance weight.\n",
      "Reward ranked fine-tuning RAFT [169] fine-tunes the model\n",
      "on ranked responses by the reward model. Preference ranking\n",
      "optimization (PRO) [171] and RRHF [170] penalize the model\n",
      "to rank responses with human preferences and supervised loss.\n",
      "On the other hand, chain-of-hindsight (CoH) [172] provides\n",
      "feedback to the model in language rather than reward, to learn\n",
      "good versus bad responses.\n",
      "Aligning with Synthetic Feedback:\n",
      "Aligning LLMs with\n",
      "----\n",
      "good versus bad responses.\n",
      "Aligning with Synthetic Feedback:\n",
      "Aligning LLMs with\n",
      "human feedback is slow and costly. The literature suggests a\n",
      "semi-automated process to align LLMs by prompting LLMs to\n",
      "generate helpful, honest, and ethical responses to the queries,\n",
      "and fine-tuning using the newly created dataset. Constitutional\n",
      "AI [173] replaces human feedback in RLHF with AI, calling\n",
      "it RL from AI feedback (RLAIF). AlpacaFarm [174] designs\n",
      "prompts to imitate human feedback using LLMs APIs. Oppo-\n",
      "----\n",
      "prompts to imitate human feedback using LLMs APIs. Oppo-\n",
      "site to constitutional AI, AlpacaFarm injects noise in feedback\n",
      "to replicate human mistakes.\n",
      "Self-Align [98] prompts the\n",
      "LLM with ICL examples, instructing the LLM about what the\n",
      "response should contain to be considered useful and ethical.\n",
      "The same LLM is later fine-tuned with the new dataset.\n",
      "Aligning with Prompts: LLMs can be steered with prompts to\n",
      "generate desirable responses without training [175, 176]. The\n",
      "----\n",
      "generate desirable responses without training [175, 176]. The\n",
      "self-correction prompting in [176] concatenates instructions\n",
      "and CoT with questions, guiding the model to answer its\n",
      "instruction following a strategy to ensure moral safety before\n",
      "the actual answer. This strategy is shown to reduce the harm in\n",
      "generated responses significantly.\n",
      "Red-Teaming/Jailbreaking/Adversarial\n",
      "Attacks:\n",
      "LLMs\n",
      "exhibit harmful behaviors, hallucinations, leaking personal in-\n",
      "----\n",
      "Attacks:\n",
      "LLMs\n",
      "exhibit harmful behaviors, hallucinations, leaking personal in-\n",
      "formation, and other shortcomings through adversarial probing.\n",
      "The models are susceptible to generating harmful responses\n",
      "even though they are aligned for safety [177, 178].\n",
      "Red-\n",
      "teaming is a common approach to address illicit outputs, where\n",
      "the LLMs are prompted to generate harmful outputs [178, 179].\n",
      "The dataset collected through red-teaming is used to fine-tune\n",
      "----\n",
      "The dataset collected through red-teaming is used to fine-tune\n",
      "models for safety. While red-teaming largely relies on human\n",
      "annotators, another work [180] red-team LLMs to find prompts\n",
      "that lead to harmful outputs for other LLMs.\n",
      "3.2.4. Continue Pre-Training\n",
      "Although fine-tuning boosts a model’s performance, it leads\n",
      "to catastrophic forgetting of previously learned information.\n",
      "Concatenating fine-tuning data with a few randomly selected\n",
      "----\n",
      "Concatenating fine-tuning data with a few randomly selected\n",
      "pre-training samples in every iteration avoids network forget-\n",
      "ting [181, 152]. This is also effective in adapting LLMs for\n",
      "cases where fine-tuning data is small and the original capac-\n",
      "ity is to be maintained. Prompt-based continued pre-training\n",
      "(PCP) [182] trains the model with text and instructions related\n",
      "to tasks and then finally instruction-tunes the model for down-\n",
      "stream tasks.\n",
      "3.2.5. Sample Efficiency\n",
      "----\n",
      "stream tasks.\n",
      "3.2.5. Sample Efficiency\n",
      "While fine-tuning data is generally many-fold smaller than\n",
      "the pre-training data, it still has to be large enough for accept-\n",
      "able performance [16, 97, 18] and requires proportional com-\n",
      "puting resources. Studying the effects on performance with less\n",
      "data, existing literature [183, 184] finds that models trained\n",
      "on less data can outperform models trained with more data.\n",
      "In [183], 25% of the total downstream data is found enough\n",
      "----\n",
      "In [183], 25% of the total downstream data is found enough\n",
      "for state-of-the-art performance. Selecting coreset-based 0.5%\n",
      "of the total instruction-tuning data improves the model perfor-\n",
      "mance by 2% in [184], as compared to the complete data tun-\n",
      "ing. Less is more for alignment (LIMA) [185] uses only 1000\n",
      "carefully created demonstrations to fine-tune the model and has\n",
      "achieved comparable performance to GPT-4.\n",
      "3.3. Increasing Context Window\n",
      "LLMs are trained with limited context windows due to ex-\n",
      "----\n",
      "3.3. Increasing Context Window\n",
      "LLMs are trained with limited context windows due to ex-\n",
      "pensive attention and high memory requirements.\n",
      "A model\n",
      "trained on limited sequence lengths fails to generalize to unseen\n",
      "lengths at inference time [186, 49]. Alternatively, LLMs with\n",
      "ALiBi [65] positional encodings can perform zero-shot length\n",
      "extrapolation. However, ALiBi has less expressive power [66]\n",
      "and inferior performance on multiple benchmarks [46], and\n",
      "----\n",
      "and inferior performance on multiple benchmarks [46], and\n",
      "many LLMs use RoPE positional embedding that is unable to\n",
      "perform zero-shot extrapolation. A larger context length has\n",
      "benefits such as a better understanding of longer documents,\n",
      "more samples in in-context learning, execution of bigger rea-\n",
      "soning processes, etc. Expanding context length during fine-\n",
      "tuning is slow, inefficient, and computationally expensive [49].\n",
      "Therefore, researchers employ various context window extrap-\n",
      "----\n",
      "Therefore, researchers employ various context window extrap-\n",
      "olation techniques discussed below.\n",
      "Position Interpolation: Rather than extrapolating, [49] shows\n",
      "that interpolating position encodings within the pre-trained con-\n",
      "text window are more effective. The work demonstrates that\n",
      "only 1000 steps of fine-tuning are enough to achieve better re-\n",
      "sults on larger windows without reducing performance com-\n",
      "pared to the original context size. Giraffe [46] uses power scal-\n",
      "----\n",
      "pared to the original context size. Giraffe [46] uses power scal-\n",
      "ing in RoPE, and YaRN [47] proposed NTK-aware interpola-\n",
      "tion.\n",
      "17\n",
      "----\n",
      "Efficient Attention Mechanism:\n",
      "Dense global attention is\n",
      "one of the major constraints in training larger context win-\n",
      "dow LLMs.\n",
      "Using efficient attention variants, such as lo-\n",
      "cal, sparse, and dilated attention, reduces the computation cost\n",
      "significantly.\n",
      "LongT5 [48] proposes transient global atten-\n",
      "tion (TGlobal), applying attention to local and global tokens\n",
      "(windowed token averaging).\n",
      "The model replaces attention\n",
      "in T5 [10] with TGlobal attention, pre-trains the model on\n",
      "----\n",
      "The model replaces attention\n",
      "in T5 [10] with TGlobal attention, pre-trains the model on\n",
      "4098 sequence length, fine-tunes on larger window sizes, as\n",
      "large as 16k, and improves task performance on longer inputs.\n",
      "This shows the extrapolation ability of TGlobal attention with\n",
      "only fine-tuning. COLT5 [187] uses two branches, one with\n",
      "lightweight and the other with heavyweight attention and feed-\n",
      "forward layers. All tokens are processed from the lightweight\n",
      "----\n",
      "forward layers. All tokens are processed from the lightweight\n",
      "branch, and only important tokens are routed to the heavy-\n",
      "weight branch. LongNet [188] replaces standard attention with\n",
      "dilated attention, expanding sequence length to 1 billion tokens.\n",
      "LongLoRA [189] proposes shift-short attention, used during\n",
      "fine-tuning to reduce dense attention costs. However, the model\n",
      "during inference uses dense attention and achieves similar per-\n",
      "formance as full attention fine-tuning.\n",
      "----\n",
      "formance as full attention fine-tuning.\n",
      "Extrapolation without Training: LM-Infinite [186] and par-\n",
      "allel context windows (PCW) [190] show length extrapolation\n",
      "is possible using pre-trained LLMs. LM-Infinite suggested Λ-\n",
      "shaped attention applied within the original context window\n",
      "limits. Likewise, PCW chunks larger inputs into the pre-trained\n",
      "context lengths and applies the same positional encodings to\n",
      "each chunk.\n",
      "3.4. Augmented LLMs\n",
      "LLMs are capable of learning from the examples concate-\n",
      "----\n",
      "each chunk.\n",
      "3.4. Augmented LLMs\n",
      "LLMs are capable of learning from the examples concate-\n",
      "nated with the input, known as context augmentation, in-\n",
      "context learning (ICL), or few-shot prompting. They show ex-\n",
      "cellent generalization to unseen tasks with few-shot prompt-\n",
      "ing, enabling LLMs to answer queries beyond the capacity ac-\n",
      "quired during training [6, 55]. These emergent abilities allow\n",
      "for adapting the model without fine-tuning—a costly process.\n",
      "----\n",
      "for adapting the model without fine-tuning—a costly process.\n",
      "Aside from this, hallucination, producing inaccurate, unsafe,\n",
      "or factually incorrect responses, is common for LLMs, which is\n",
      "avoided by augmenting contextual data. While the user can pro-\n",
      "vide in-context samples in the query [54, 32], here we specifi-\n",
      "cally refer to the methods that access external storage program-\n",
      "matically, calling them augmented LLMs.\n",
      "The literature suggests various external memory designs to aug-\n",
      "----\n",
      "The literature suggests various external memory designs to aug-\n",
      "ment LLMs, long-term [191, 192, 193, 194], short-term [195],\n",
      "symbolic [196], and non-symbolic [197, 198]. The memory\n",
      "can be maintained in different formats such as documents, vec-\n",
      "tors, or databases. A few systems maintain intermediate mem-\n",
      "ory representations to retain information across multiple iter-\n",
      "ations [194, 192], while others extract important information\n",
      "from the datasets and save it in memory for recall [199]. The\n",
      "----\n",
      "from the datasets and save it in memory for recall [199]. The\n",
      "memory read and write operations are performed either with\n",
      "or without LLMs cooperation [192, 200, 194, 201], acting as\n",
      "a feedback signal in [195]. We discuss different types of aug-\n",
      "mented LLMs below.\n",
      "Figure 12: A flow diagram of Retrieval Augmented LLMs. The retriever ex-\n",
      "tracts a similar context to the input and forwards it to the LLM either in simple\n",
      "language or encoded through Fusion-in-Decoder (FiD). Depending on the task,\n",
      "----\n",
      "language or encoded through Fusion-in-Decoder (FiD). Depending on the task,\n",
      "retrieval and generation may repeat multiple times.\n",
      "3.4.1. Retrieval Augmented LLMs\n",
      "LLMs may have limited memory and outdated information,\n",
      "leading to inaccurate responses. Retrieving relevant informa-\n",
      "tion from external up-to-date storage enables the LLMs to\n",
      "accurately answer with references and utilize more informa-\n",
      "tion. With retrieval augmentation, smaller models have been\n",
      "----\n",
      "tion. With retrieval augmentation, smaller models have been\n",
      "shown to perform at par with larger models. For instance, the\n",
      "11B model can become competitive to 540B PaLM in [25] and\n",
      "7.5B to 280B Gopher in [193]. Retrieval augmented language\n",
      "modeling (RALM) has two major components, shown in\n",
      "Figure 12, namely: 1) retriever and 2) language model.\n",
      "In\n",
      "RALM, the retriever plays a crucial role in driving LLM\n",
      "response, where incorrect information can steer LLMs to false\n",
      "----\n",
      "response, where incorrect information can steer LLMs to false\n",
      "behavior. This leads to the development of various methods to\n",
      "retrieve accurate information and fuse with the query for better\n",
      "performance.\n",
      "Zero-Shot Retrieval Augmentation: This kind of augmen-\n",
      "tation keeps the original LLM architecture and weights\n",
      "unchanged and uses BM25 [202], nearest neighbors, or frozen\n",
      "pre-trained models like Bert [7] as a retriever. The retrieved\n",
      "information is provided as input to the model for response\n",
      "----\n",
      "information is provided as input to the model for response\n",
      "generation, shown to improve performance over LLMs without\n",
      "retrieval [198, 203].\n",
      "In some scenarios, multiple retrieval\n",
      "iterations are required to complete the task.\n",
      "The output\n",
      "generated in the first iteration is forwarded to the retriever\n",
      "to fetch similar documents. Forward-looking active retrieval\n",
      "(FLARE) [197] initially generates the response and corrects\n",
      "the output by retrieving relevant documents if the response\n",
      "----\n",
      "the output by retrieving relevant documents if the response\n",
      "contains low-confidence tokens. Similarly, RepoCoder [204]\n",
      "fetches code snippets recursively for code completion.\n",
      "Training with Retrieval Augmentation: To reduce failures in\n",
      "retrieval augmentation generation (RAG), researchers train or\n",
      "fine-tune retrievers and LLMs with a retrieval augmentation\n",
      "pipeline. We discuss the literature below based on their focus\n",
      "on the respective training processes of the pipeline.\n",
      "----\n",
      "on the respective training processes of the pipeline.\n",
      "Training LLM: Retrieval-enhanced transformer (RETRO) [193]\n",
      "shows pre-training smaller LLMs with RAG pipeline outper-\n",
      "forms larger LLMs, such as GPT-3 trained without RAG.\n",
      "RETRO uses a 2-trillion token subset of MassiveText as\n",
      "18\n",
      "----\n",
      "a database.\n",
      "The retrieval pipeline divides the input query\n",
      "into subsets and retrieves relevant chunks from the database\n",
      "for each subset, encoded together with input intermediate\n",
      "representations for generating tokens. It uses cross-chunked\n",
      "attention to attend to previous chunks auto-regressively.\n",
      "A\n",
      "study on RETRO [205] shows models pre-trained without RAG\n",
      "but fine-tuned using RAG lack the performance gains obtained\n",
      "by pre-training with RAG.\n",
      "----\n",
      "but fine-tuned using RAG lack the performance gains obtained\n",
      "by pre-training with RAG.\n",
      "Training Retriever: Quality of responses generated by LLMs\n",
      "is highly dependent on the in-context examples.\n",
      "There-\n",
      "fore, [206, 207, 208, 209] train retrievers to retrieve accurate\n",
      "few-shot samples while keeping the LLM frozen for gener-\n",
      "ation.\n",
      "Retrieved samples are ranked to build ground-truth\n",
      "data to train retrievers with contrastive learning in [206, 208].\n",
      "----\n",
      "data to train retrievers with contrastive learning in [206, 208].\n",
      "RoBERTa is trained for downstream tasks in [207] for ICL\n",
      "samples retrieval.\n",
      "REPLUG [209] trains the retriever with\n",
      "supervised signals from the frozen LLM-generated outputs.\n",
      "Training Retriever and LLM: Further benefits are achieved by\n",
      "training both the retriever and the model in [25, 210, 211]. In\n",
      "this case, the error propagates back to the retriever, updating\n",
      "both the language model and the retriever.\n",
      "While masked\n",
      "----\n",
      "both the language model and the retriever.\n",
      "While masked\n",
      "language modeling (MLM) is a common pre-training objec-\n",
      "tive [25, 211], retrieval pre-trained transformer (RPT) [210]\n",
      "used document chunk prediction as a pre-training objective for\n",
      "long text modeling.\n",
      "Encoded Context Augmentation:\n",
      "Concatenating retrieved\n",
      "documents with the query becomes infeasible as the sequence\n",
      "length and sample size grow. Encoding the context and fusing\n",
      "it with the decoder (Fusion-in-Decoder) using cross-attention\n",
      "----\n",
      "it with the decoder (Fusion-in-Decoder) using cross-attention\n",
      "makes it possible to augment more samples without increasing\n",
      "computation costs significantly [212, 193, 210, 25].\n",
      "Web Augmented:\n",
      "Locally stored memory, but external to\n",
      "LLM, has limited information. However, a large amount of\n",
      "information is available on the internet, which gets updated\n",
      "regularly.\n",
      "Rather than storing information locally, various\n",
      "methods retrieve query-related context through a web search\n",
      "----\n",
      "methods retrieve query-related context through a web search\n",
      "and forward it to LLMs [213, 214, 166].\n",
      "3.4.2. Tool Augmented LLMs\n",
      "While RAG relies on the retriever to provide context to the\n",
      "LLM to answer queries, tool augmented LLMs capitalize on the\n",
      "reasoning abilities of LLMs to iteratively plan by dividing tasks\n",
      "into sub-tasks, selecting necessary tools, and taking actions to\n",
      "complete the task [215, 216, 217, 27]. A generic pipeline of\n",
      "tool-augmented LLMs is shown in Figure 13, where different\n",
      "----\n",
      "tool-augmented LLMs is shown in Figure 13, where different\n",
      "modules in Figure 13 are selected in a loop until the task com-\n",
      "pletion.\n",
      "Zero-Shot Tool Augmentation: LLMs in-context learning and\n",
      "reasoning abilities enable them to interact with tools with-\n",
      "out training. Automatic reasoning and tool-use (ART) [217]\n",
      "builds a task library with demonstrations of reasoning steps and\n",
      "calling external tools. It retrieves similar task examples and\n",
      "provides the context to the LLM for inference. Aside from\n",
      "----\n",
      "provides the context to the LLM for inference. Aside from\n",
      "this, [218] shows tool documentation is enough to teach LLMs\n",
      "to use tools without demonstrations. RestGPT [219] integrates\n",
      "LLMs with RESTful APIs by decomposing tasks into planning\n",
      "Figure 13: A basic flow diagram of tool augmented LLMs. Given an input and\n",
      "a set of available tools, the model generates a plan to complete the task. The\n",
      "tool augmented LLMs utilize different modules iteratively, such as retriever,\n",
      "----\n",
      "tool augmented LLMs utilize different modules iteratively, such as retriever,\n",
      "tool execution, read-write to memory, feedback, etc., depending on the task.\n",
      "and API selection steps. The API selector understands the API\n",
      "documentation to select a suitable API for the task and plan the\n",
      "execution. ToolkenGPT [220] uses tools as tokens by concate-\n",
      "nating tool embeddings with other token embeddings. During\n",
      "inference, the LLM generates the tool tokens representing the\n",
      "----\n",
      "inference, the LLM generates the tool tokens representing the\n",
      "tool call, stops text generation, and restarts using the tool exe-\n",
      "cution output.\n",
      "Training with Tool Augmentation: LLMs are trained to inter-\n",
      "act with diverse tools, enhancing planning abilities to overcome\n",
      "the limitations of zero-shot tool augmentation [221, 27, 222,\n",
      "223]. Gorilla [221] instruction-tunes LLaMA with information\n",
      "retrieval from API documentation. It uses the self-instruct [19]\n",
      "----\n",
      "retrieval from API documentation. It uses the self-instruct [19]\n",
      "data generation pipeline with GPT-4 by providing in-context\n",
      "examples retrieved from API documentation. Tool augmented\n",
      "language model (TALM) [27] fine-tunes T5 [10] for tool use\n",
      "with a self-play approach, where it iteratively completes tool\n",
      "manipulation tasks and includes them back in the training set.\n",
      "ToolLLM [223] collects 16k APIs from RapidAPI. It samples\n",
      "APIs from the list to generate an instruction-tuning dataset us-\n",
      "----\n",
      "APIs from the list to generate an instruction-tuning dataset us-\n",
      "ing ChatGPT in single-tool and multi-tool scenarios. For high-\n",
      "quality datasets, ToolLLM suggested a depth-first search-based\n",
      "decision tree (DFSDT) method to generate ground-truths with\n",
      "diverse reasoning and planning.\n",
      "Multimodal Tool Augmentation: The compositional reasoning\n",
      "capacity of LLMs allows them to manipulate tools in multi-\n",
      "modal settings [215, 216, 224]. Following the pipeline shown\n",
      "----\n",
      "modal settings [215, 216, 224]. Following the pipeline shown\n",
      "in Figure 13, the LLM outlines a plan, generally executing in a\n",
      "sequence: Plan →Tool selection →Execute →Inspect →\n",
      "Generate, to respond to the user query. Here, the database of\n",
      "tools is rich in modalities, including text, images, etc. Many of\n",
      "the multimodal tool augmentation systems employ multimodal\n",
      "LLMs [31, 225, 224, 216], while others utilize single modality\n",
      "19\n",
      "----\n",
      "LLMs and generate a plan on using different modality tools to\n",
      "solve multimodal queries [226].\n",
      "3.5. LLMs-Powered Agents\n",
      "AI agents are autonomous entities, capable of planning,\n",
      "decision-making, and performing actions to achieve complex\n",
      "goals.\n",
      "In the early days, AI agents were rule-based, de-\n",
      "signed for narrow tasks, and had limited capabilities, such\n",
      "as Clippy [227] and Deep Blue [228].\n",
      "In contrast to this,\n",
      "LLMs abilities to respond to dynamic scenarios have made it\n",
      "----\n",
      "In contrast to this,\n",
      "LLMs abilities to respond to dynamic scenarios have made it\n",
      "possible to incorporate them in diverse applications, includ-\n",
      "ing LLMs-powered agents [224, 216], where LLMs behave\n",
      "as the brain of agents. LLMs have been incorporated in web\n",
      "agents [166, 167], coding agents [229], tool agents [27, 223],\n",
      "embodied agents [26], and conversational agents [195], requir-\n",
      "ing minimal to no fine-tuning\". Below we summarize the re-\n",
      "----\n",
      "ing minimal to no fine-tuning\". Below we summarize the re-\n",
      "search in LLMs-based autonomous agents. For a more detailed\n",
      "discussion, please refer to [230, 231].\n",
      "LLMs Steering Autonomous Agents: LLMs are the cognitive\n",
      "controllers of the autonomous agents. They generate plans, rea-\n",
      "son about tasks, incorporate memory to complete tasks, and\n",
      "adapt the outline depending on the feedback from the environ-\n",
      "ment. Depending on the acquired capabilities of LLMs, many\n",
      "----\n",
      "ment. Depending on the acquired capabilities of LLMs, many\n",
      "methods fine-tune, propose a better prompting approach, or uti-\n",
      "lize different modules to enhance agents’ performance. Mod-\n",
      "ules and strategies employed in autonomous agents are briefly\n",
      "discussed below.\n",
      "Planning and Reasoning: Completing a complex task requires\n",
      "human-like logical thinking, planning necessary steps, and\n",
      "reasoning current and future directions. Prompting methods\n",
      "----\n",
      "reasoning current and future directions. Prompting methods\n",
      "like chain-of-thoughts [103], tree-of-thoughts [105], and self-\n",
      "consistency [104] are central to agents, eliciting LLMs to rea-\n",
      "son its actions and choose among different paths for task com-\n",
      "pletion. When LLMs are prompted with a task description and\n",
      "a sequence of actions, they can accurately generate plan ac-\n",
      "tions without any fine-tuning [232]. Reasoning via planning\n",
      "(RAP) [233] incorporates a re-purposed LLM as a world model\n",
      "----\n",
      "(RAP) [233] incorporates a re-purposed LLM as a world model\n",
      "to reason about future outcomes and explore alternative paths\n",
      "for task completion. Retroformer [234] uses a retrospective\n",
      "LLM to improve main LLM planning and reasoning capabil-\n",
      "ities by providing helpful task cues.\n",
      "Feedback: LLMs in open-loop systems generate plans and as-\n",
      "sume that the agent will complete them successfully. However,\n",
      "the actual scenario is different with failures and variable re-\n",
      "----\n",
      "the actual scenario is different with failures and variable re-\n",
      "sponses from the environment. To correctly complete tasks,\n",
      "many methods use LLMs in a closed-loop where the action re-\n",
      "sponse is provided as feedback to the LLMs to re-assess and\n",
      "update the plan as required [235, 236, 237, 195]. Another di-\n",
      "rection of research exploits LLMs as reward functions to train\n",
      "reinforcement learning (RL) policies instead of humans [238].\n",
      "Memory: LLMs can learn from the context provided in the\n",
      "----\n",
      "Memory: LLMs can learn from the context provided in the\n",
      "prompt. In addition to internal memory, various systems em-\n",
      "ploy external memory to save the response history.\n",
      "Reflex-\n",
      "ion [195] maintains an episodic memory to use previous re-\n",
      "sponses as feedback to improve future decision-making. Retro-\n",
      "former [234] improves its responses by employing short-term\n",
      "and long-term memory, where short-term memory contains re-\n",
      "cent responses and long-term memory keeps summarized failed\n",
      "----\n",
      "cent responses and long-term memory keeps summarized failed\n",
      "attempts to add in the prompt as reflection.\n",
      "Multi-Agents Systems: LLMs can play user-defined roles and\n",
      "behave like a specific domain expert. In multi-agent systems,\n",
      "each LLM is assigned a unique role, simulating human behav-\n",
      "ior and collaborating with other agents to complete a complex\n",
      "task [229, 239].\n",
      "LLMs in Physical Environment:\n",
      "LLMs are good at\n",
      "instruction-following, however, utilizing them for physically\n",
      "----\n",
      "LLMs are good at\n",
      "instruction-following, however, utilizing them for physically\n",
      "grounded tasks requires adaptation, as they lack real-world\n",
      "knowledge. This could lead to generating illogical responses\n",
      "for a particular physical situation [240, 26].\n",
      "SayCan [240]\n",
      "make LLMs aware of the available low-level task operations.\n",
      "LLM (Say) builds a high-level plan to complete the task and\n",
      "a learned affordance function (Can) explores the possibility of\n",
      "----\n",
      "a learned affordance function (Can) explores the possibility of\n",
      "executing the plan in the real world. SayCan uses RL to train\n",
      "the language-conditioned affordance function. PaLM-E enables\n",
      "the LLM to solve grounded tasks by training multi-modal LLM\n",
      "feeding inputs directly from the sensors.\n",
      "Manipulation: In the area of manipulation [236, 241], LLMs\n",
      "enhance a robot’s dexterity and adaptability, excelling in tasks\n",
      "like object recognition, grasping, and collaboration. They ana-\n",
      "----\n",
      "like object recognition, grasping, and collaboration. They ana-\n",
      "lyze visual and spatial information to determine the most effec-\n",
      "tive approach to interact with objects.\n",
      "Navigation: LLMs enhance a robot’s ability to navigate com-\n",
      "plex environments with precision and adaptability [242, 243,\n",
      "244, 245]. They generate feasible paths and trajectories for\n",
      "robots, accounting for intricate environmental details [246].\n",
      "This ability is valuable in scenarios requiring precise and\n",
      "----\n",
      "This ability is valuable in scenarios requiring precise and\n",
      "dynamically adaptable navigation in environments like ware-\n",
      "houses, transport, healthcare facilities, and residences.\n",
      "3.6. Efficient LLMs\n",
      "Deploying LLMs in production is expensive. Reducing their\n",
      "running costs while preserving performance is an appealing\n",
      "area of research. This section summarizes the approaches sug-\n",
      "gested to enhance LLMs’ efficiency.\n",
      "3.6.1. Parameter Efficient Fine-Tuning\n",
      "----\n",
      "gested to enhance LLMs’ efficiency.\n",
      "3.6.1. Parameter Efficient Fine-Tuning\n",
      "Fine-tuning LLMs with tens or hundreds of billions of pa-\n",
      "rameters, such as GPT-3 (175B), BLOOM (176B), MT-NLG\n",
      "(540B), etc., is computationally intensive and time-consuming.\n",
      "To avoid complete model fine-tuning, numerous parameter-\n",
      "efficient fine-tuning (PEFT) techniques [40, 247, 41, 38, 39] try\n",
      "to achieve acceptable model fine-tuning performance at reduced\n",
      "costs. As compared to full fine-tuning [248], PEFT performs\n",
      "----\n",
      "costs. As compared to full fine-tuning [248], PEFT performs\n",
      "better in low-resource setups, achieves comparable perfor-\n",
      "mance on medium-resource scenarios, and performs worse than\n",
      "full fine-tuning under high-resource availability. An overview\n",
      "of different PEFT approaches is shown in Figure 14.\n",
      "Adapter Tuning: Adds a few trainable parameters within the\n",
      "transformer block. The adapter layer is a sequence of feature\n",
      "downscaling, non-linearity, and upscaling [106]. Variants of\n",
      "----\n",
      "downscaling, non-linearity, and upscaling [106]. Variants of\n",
      "adapter tuning inject adapter layers sequentially [106] and in\n",
      "parallel [38], whereas the mixture of adapter (AdaMix) [249]\n",
      "20\n",
      "----\n",
      "Figure 14: Illustration of parameter-efficient fine-tuning paradigms, where x is input and h is hidden state, figure courtesy [38]. Parallel adapter and LoRA fall in\n",
      "the adapter tuning category.\n",
      "employs multiple adapter modules in a single layer. AdaMix\n",
      "routes input instances randomly to one of the multiple down-\n",
      "scale and upscale modules. The mixture of adapters is averaged\n",
      "out for inference to avoid additional latency. Low-Rank Adap-\n",
      "tation (LoRA) [250] learns low-rank decomposed matrices to\n",
      "----\n",
      "tation (LoRA) [250] learns low-rank decomposed matrices to\n",
      "freeze original weights. The learned weights are fused with the\n",
      "original weights for inference, avoiding latency.\n",
      "Prompt Tuning: Prompting is an effective way to adapt a\n",
      "pre-trained LLM for the downstream task. However, manual\n",
      "prompts bring uncertainty in the model’s prediction, where a\n",
      "change in a single word drops the performance [247]. Prompt\n",
      "tuning alleviates this problem by fine-tuning only 0.001%-3%\n",
      "----\n",
      "tuning alleviates this problem by fine-tuning only 0.001%-3%\n",
      "additional parameters [251]. It concatenates trainable prompt\n",
      "parameters with the model embeddings [247, 40, 251]. Task-\n",
      "specific fixed discrete prompts are concatenated with input em-\n",
      "beddings in [40]. As discrete prompts bring instability, prompts\n",
      "are encoded through a learnable mapping in P-Tuning [247],\n",
      "naming continuous prompts, which are appended with the dis-\n",
      "crete prompts.\n",
      "Only the prompt encoder is trainable in the\n",
      "----\n",
      "crete prompts.\n",
      "Only the prompt encoder is trainable in the\n",
      "model. In an extension of P-Tuning, continuous prompts are\n",
      "concatenated with each layer of the network in [251]. Progres-\n",
      "sive prompts [252] avoid catastrophic forgetting and transfer\n",
      "previously learned knowledge by sequentially adding trainable\n",
      "prompt embeddings to the previously frozen task embeddings.\n",
      "Prefix Tuning: A set of trainable task-specific prefix vectors\n",
      "are appended to the frozen transformer layers in prefix tun-\n",
      "----\n",
      "are appended to the frozen transformer layers in prefix tun-\n",
      "ing [41]. The prefix vectors are virtual tokens attended by the\n",
      "context tokens on the right. In addition, adaptive prefix tun-\n",
      "ing [253] applies a gating mechanism to control the information\n",
      "from the prefix and actual tokens.\n",
      "Bias Tuning: Fine-tuning only bias terms in small to medium\n",
      "training data has been found effective in BitFit [254].\n",
      "This\n",
      "method achieves full fine-tuning performance for tasks with less\n",
      "----\n",
      "This\n",
      "method achieves full fine-tuning performance for tasks with less\n",
      "training data and comparable performance with more training\n",
      "data.\n",
      "3.6.2. Quantization\n",
      "LLMs require extensive computing and memory for infer-\n",
      "ence.\n",
      "Deploying a 175B parameter GPT-3 model needs at\n",
      "least five 80GB A100 GPUs and 350GB of memory to store in\n",
      "FP16 format [44]. Such demanding requirements for deploying\n",
      "LLMs make it harder for smaller organizations to utilize them.\n",
      "----\n",
      "LLMs make it harder for smaller organizations to utilize them.\n",
      "Model compression is an effective solution but comes at the cost\n",
      "of degraded performance, especially at large scales greater than\n",
      "6B. These models exhibit very large magnitude outliers that do\n",
      "not exist in smaller models [255], making it challenging and re-\n",
      "quiring specialized methods for quantizing LLMs [44, 256].\n",
      "Post-Training Quantization: Minimal or no training is re-\n",
      "----\n",
      "Post-Training Quantization: Minimal or no training is re-\n",
      "quired in this type of quantization, without significantly com-\n",
      "promising the model performance. LLM-8-bit [255] uses full-\n",
      "precision matrix multiplication for weights associated with out-\n",
      "lier features and 8-bit for remaining features. The lower pre-\n",
      "cision multiplication outputs are converted to FP-16 and con-\n",
      "catenated with others. The quantized models have homogenous\n",
      "word embeddings, which may degrade their performance. To\n",
      "----\n",
      "word embeddings, which may degrade their performance. To\n",
      "fix this, token-level knowledge distillation is employed in [45]\n",
      "along with independent quantization scaling factors for each\n",
      "module due to varying weight distribution. Feature distribu-\n",
      "tions are asymmetric and appear in different channels; outlier\n",
      "suppression [257] shifts and scales per-channel activation dis-\n",
      "tributions for effective quantization. SmoothQuant [44] quan-\n",
      "tizes activations and weights to INT8 format by smoothing\n",
      "----\n",
      "tizes activations and weights to INT8 format by smoothing\n",
      "activations and migrating the quantization difficulty toward\n",
      "weights. It multiplies the inverse of the smoothing factor with\n",
      "weights, which introduces a few outliers in the weights but is\n",
      "easier to quantify than unsmoothed activations. OPTQ [256]\n",
      "uses the optimal brain compression (OBC) [258] algorithm to\n",
      "quantize the model layer-by-layer and update weights to com-\n",
      "pensate for quantization error.\n",
      "To improve speed and per-\n",
      "----\n",
      "pensate for quantization error.\n",
      "To improve speed and per-\n",
      "formance, OPTQ updates weights in arbitrary order, employs\n",
      "lazy updates, and uses better Cholesky kernels. Outlier-aware\n",
      "weight quantization (OWQ) [259] uses the OPTQ algorithm for\n",
      "quantization but assigns higher precision to vulnerable weights,\n",
      "causing outliers and lower precision for others.\n",
      "Quantization-Aware Training:\n",
      "To compensate for perfor-\n",
      "mance degradation,\n",
      "a quantized model is fine-tuned in\n",
      "----\n",
      "To compensate for perfor-\n",
      "mance degradation,\n",
      "a quantized model is fine-tuned in\n",
      "quantization-aware training (QAT) [260, 261, 262].\n",
      "Al-\n",
      "pha Tuning quantizes the model using binary coding quan-\n",
      "tization (BCQ) [263] and fine-tunes only quantization scal-\n",
      "ing factors.\n",
      "This approach improves performance over\n",
      "21\n",
      "----\n",
      "parameter-efficient fine-tuning of the pre-trained model. Sim-\n",
      "ilarly, parameter-efficient and quantization-aware adaptation\n",
      "(PEQA) [264] reduces the precision of fully-connected layers\n",
      "and fine-tunes only quantization scaling parameters.\n",
      "LLM-\n",
      "QAT [262] generates training data from the pre-trained network\n",
      "and trains a quantized student model with knowledge distilla-\n",
      "tion. QLoRA [261] fine-tunes 4-bit quantized pre-trained LLM\n",
      "with LoRA [250] using a 4-bit normal float, which shows better\n",
      "----\n",
      "with LoRA [250] using a 4-bit normal float, which shows better\n",
      "performance over a 4-bit integer and float.\n",
      "3.6.3. Pruning\n",
      "Pruning is an alternative approach to quantization to com-\n",
      "press model size, thereby reducing LLMs deployment costs\n",
      "significantly. Compared to task-agnostic pruning, task-specific\n",
      "pruning is easily achievable with good performance, where a\n",
      "model is fine-tuned on the downstream task and pruned for\n",
      "faster inference. It is possible to prune LLMs for individual\n",
      "----\n",
      "faster inference. It is possible to prune LLMs for individual\n",
      "tasks, but the cost of pruning and deploying task-specific mod-\n",
      "els is high. To overcome this, many structured and unstructured\n",
      "pruning methods for LLMs have been proposed to maintain rea-\n",
      "sonable performance across all tasks while shrinking the model\n",
      "size [265, 42, 266].\n",
      "Unstructured Pruning: This kind of pruning removes less im-\n",
      "portant weights without maintaining any structure.\n",
      "Existing\n",
      "----\n",
      "portant weights without maintaining any structure.\n",
      "Existing\n",
      "LLM pruning methods take advantage of the unique charac-\n",
      "teristics of LLMs, uncommon for smaller models, where a\n",
      "small subset of hidden states are activated with large magni-\n",
      "tude [255]. Pruning by weights and activations (Wanda) [265]\n",
      "prunes weights in every row based on importance, calculated\n",
      "by multiplying the weights with the norm of input. The pruned\n",
      "model does not require fine-tuning, thereby saving computa-\n",
      "----\n",
      "model does not require fine-tuning, thereby saving computa-\n",
      "tional costs. Outlier weighed layerwise sparsity (OWL) [267]\n",
      "extends Wanda with non-uniform layer pruning. It shows that\n",
      "the number of outliers varies for different layers; therefore, the\n",
      "model should have variable pruning ratios for better perfor-\n",
      "mance for every layer. Contrastive pruning (CAP) [43] itera-\n",
      "tively prunes the model by training the sparse model using con-\n",
      "trastive loss between pre-trained, fine-tuned, and snapshots of\n",
      "----\n",
      "trastive loss between pre-trained, fine-tuned, and snapshots of\n",
      "previous sparse models to learn task-specific and task-agnostic\n",
      "knowledge.\n",
      "Structured Pruning: Here, the parameters are removed in\n",
      "groups, rows, columns, or matrices, which speeds up the\n",
      "inference because of effective hardware tensor core utiliza-\n",
      "tion [265].\n",
      "LLM-Pruner [42] employs a 3-stage structured\n",
      "pruning strategy, identifying the groups of hidden states caus-\n",
      "ing each other to activate during the forward-pass, keeping im-\n",
      "----\n",
      "ing each other to activate during the forward-pass, keeping im-\n",
      "portant groups and removing less important ones, and fine-\n",
      "tuning the pruned model with LoRA. Sparsity-induced mask\n",
      "learning (SIMPLE) [268] prunes the network using learnable\n",
      "masks. Similarly, another method prunes LLMs by learning\n",
      "masks and removing unimportant rank-1 components of the\n",
      "factorized weight matrix [266].\n",
      "3.7. Multimodal LLMs\n",
      "Inspired by the success of LLMs in natural language process-\n",
      "----\n",
      "3.7. Multimodal LLMs\n",
      "Inspired by the success of LLMs in natural language process-\n",
      "ing applications, an increasing number of research works are\n",
      "now facilitating LLMs to perceive different modalities of infor-\n",
      "mation like image [269, 270, 271], video [272, 273, 274], au-\n",
      "dio [275, 274, 276], etc. Multimodal LLMs (MLLMs) present\n",
      "substantial benefits compared to standard LLMs that process\n",
      "only text. By incorporating information from various modal-\n",
      "----\n",
      "only text. By incorporating information from various modal-\n",
      "ities, MLLMs can achieve a deeper understanding of context,\n",
      "leading to more intelligent responses infused with a variety of\n",
      "expressions. Importantly, MLLMs align closely with human\n",
      "perceptual experiences, leveraging the synergistic nature of our\n",
      "multisensory inputs to form a comprehensive understanding of\n",
      "the world [276, 26]. Coupled with a user-friendly interface,\n",
      "MLLMs can offer intuitive, flexible, and adaptable interactions,\n",
      "----\n",
      "MLLMs can offer intuitive, flexible, and adaptable interactions,\n",
      "allowing users to engage with intelligent assistants through a\n",
      "spectrum of input methods. According to the ways of construct-\n",
      "ing models, current MLLMs can be generally divided into three\n",
      "streams: pre-training, fine-tuning, and prompting. In this sec-\n",
      "tion, we will discuss more details of these main streams, as well\n",
      "as the important application of MLLMs in visual reasoning.\n",
      "----\n",
      "as the important application of MLLMs in visual reasoning.\n",
      "Pre-training: This stream of MLLMs intends to support differ-\n",
      "ent modalities using unified end-to-end models. For instance,\n",
      "Flamingo [269] applies gated cross-attention to fuse vision and\n",
      "language modalities, which are collected from pre-trained and\n",
      "frozen visual encoder and LLM, respectively. Moreover, BLIP-\n",
      "2 [270] proposes a two-stage strategy to pre-train a Querying\n",
      "Transformer (Q-Former) for the alignment between vision and\n",
      "----\n",
      "Transformer (Q-Former) for the alignment between vision and\n",
      "language modalities: in the first stage, vision-language repre-\n",
      "sentation learning is bootstrapped from a frozen visual encoder;\n",
      "and in the second stage, a frozen LLM bootstraps vision-to-\n",
      "language generative learning for zero-shot image-to-text gen-\n",
      "eration. Similarly, MiniGPT-4 [277] deploys pre-trained and\n",
      "frozen ViT [278], Q-Former and Vicuna LLM [159], only train-\n",
      "ing the linear projection layer for vision and language modali-\n",
      "----\n",
      "ing the linear projection layer for vision and language modali-\n",
      "ties alignment.\n",
      "Fine-tuning: Derived from instruction tuning [16] for NLP\n",
      "tasks [20, 16, 97], researchers are fine-tune pre-trained LLMs\n",
      "using multimodal instructions. Following this method, LLMs\n",
      "can be easily and effectively extended as multimodal chat-\n",
      "bots [277, 271, 29] and multimodal task solvers [279, 30, 280].\n",
      "The key issue of this stream of MLLMs is to collect multi-\n",
      "----\n",
      "The key issue of this stream of MLLMs is to collect multi-\n",
      "modal instruction-following data for fine-tuning [58]. To ad-\n",
      "dress this issue, the solutions of benchmark adaptation [279,\n",
      "281, 282], self-instruction [19, 31, 283], and hybrid composi-\n",
      "tion [284, 280] are employed, respectively. To mitigate the gap\n",
      "between the original language modality and additional modal-\n",
      "ities, the learnable interface is introduced to connect differ-\n",
      "ent modalities from frozen pre-trained models.\n",
      "Particularly,\n",
      "----\n",
      "ent modalities from frozen pre-trained models.\n",
      "Particularly,\n",
      "the learnable interface is expected to work in a parameter-\n",
      "efficient tuning manner: e.g., LLaMA-Adapter [285] applies\n",
      "an efficient transformer-based adapter module for training,\n",
      "and LaVIN [284] dynamically learns the multimodal feature\n",
      "weights using a mixture-of-modality adapter. Different from\n",
      "the learnable interface, the expert models can directly convert\n",
      "multimodalities into language: e.g., VideoChat-Text [272] in-\n",
      "----\n",
      "multimodalities into language: e.g., VideoChat-Text [272] in-\n",
      "corporates Whisper [286], a speech recognition expert model,\n",
      "to generate the captions of given videos for the understanding\n",
      "of following LLMs.\n",
      "Prompting:\n",
      "Different from the fine-tuning technique that\n",
      "22\n",
      "----\n",
      "directly updates the model parameters given task-specific\n",
      "datasets, the prompting technique provides certain context, ex-\n",
      "amples, or instructions to the model, fulfilling specialized tasks\n",
      "without changing the model parameters. Since prompting can\n",
      "significantly reduce the need for large-scale multimodal data,\n",
      "this technique is widely used to construct MLLMs. Particularly,\n",
      "to solve multimodal Chain of Thought (CoT) problems [103],\n",
      "LLMs are prompted to generate both the reasoning process and\n",
      "----\n",
      "LLMs are prompted to generate both the reasoning process and\n",
      "the answer given multimodal inputs [287]. On this front, differ-\n",
      "ent learning paradigms are exploited in practice: for example,\n",
      "Multimodal-CoT [287] involves two stages of rationale genera-\n",
      "tion and answer inference, where the input of the second stage\n",
      "is a combination of the original input and the output of the first\n",
      "stage; and CoT-PT [288] applies both prompt tuning and spe-\n",
      "----\n",
      "stage; and CoT-PT [288] applies both prompt tuning and spe-\n",
      "cific visual bias to generate a chain of reasoning implicitly. In\n",
      "addition to CoT problems, LLMs can also be prompted with\n",
      "multimodal descriptions and tools, effectively dividing complex\n",
      "tasks into sub-tasks [289, 290].\n",
      "Visual Reasoning Application: Recent visual reasoning sys-\n",
      "tems [291, 292, 216, 293] tend to apply LLMs for better visual\n",
      "information analysis and visual-language integration. Differ-\n",
      "----\n",
      "information analysis and visual-language integration. Differ-\n",
      "ent from previous works [294, 295] that rely on limited VQA\n",
      "datasets and small-scale neural networks, current LLM-aided\n",
      "methods offer benefits of stronger generalization ability, emer-\n",
      "gent ability, and interactivity [58]. To realize visual reasoning\n",
      "with the help of LLMs, prompting and fine-tuning techniques\n",
      "can also be utilized: for example, PointClip V2 [292] applies\n",
      "LLMs to generate 3D-specific prompts, which are encoded as\n",
      "----\n",
      "LLMs to generate 3D-specific prompts, which are encoded as\n",
      "textual features and then combined with visual features for\n",
      "3D recognition; and GPT4Tools [31] employs LoRA [250] to\n",
      "fine-tune LLMs following tool-related instructions.\n",
      "Serving\n",
      "as a controller [293], decision maker [296], or semantics re-\n",
      "finer [291, 297], LLMs significantly facilitates the progress of\n",
      "visual reasoning research.\n",
      "3.8. Summary and Discussion\n",
      "3.8.1. Architecture\n",
      "Due to the gigantic scale of LLMs, minor changes in archi-\n",
      "----\n",
      "3.8.1. Architecture\n",
      "Due to the gigantic scale of LLMs, minor changes in archi-\n",
      "tecture and training strategies have a big impact on performance\n",
      "and stability. Here, we summarize key architectural modules\n",
      "used in various LLMs, leading to better performance, reduced\n",
      "training time and memory, and better training stability.\n",
      "Layer Normalization: The performance and training stability\n",
      "of LLMs are affected significantly by layer normalization. Pre-\n",
      "----\n",
      "of LLMs are affected significantly by layer normalization. Pre-\n",
      "norm, that is normalizing inputs rather than outputs, is more\n",
      "common among LLMs stabilizing the training [6, 127, 108].\n",
      "BLOOM [13] and AlexaTM [122] utilize an additional layer\n",
      "normalization before embedding layer to stabilize the training\n",
      "of large-scale models, while the model’s zero-shot generaliza-\n",
      "tion ability can be negatively impacted [13]. However, another\n",
      "study [33] finds that pre-norm degrades fine-tuned model per-\n",
      "----\n",
      "study [33] finds that pre-norm degrades fine-tuned model per-\n",
      "formance as compared to post-norm, and there are no stability\n",
      "benefits of pre-norm beyond the 100B scale. Therefore, GLM-\n",
      "130B [33] used deep-norm which is a variant of post-norm for\n",
      "better downstream task performance after fine-tuning.\n",
      "Positional Encoding: Like other building blocks of the model,\n",
      "positional encoding also affects the performance and training\n",
      "stability of LLMs.\n",
      "BLOOM [13] finds ALiBi outperforms\n",
      "----\n",
      "stability of LLMs.\n",
      "BLOOM [13] finds ALiBi outperforms\n",
      "learned and rotary positional encodings.\n",
      "Contrary to this,\n",
      "GLM-130B [33] identifies rotary positional encoding as being\n",
      "better than ALiBi. So, there is no conclusion in the literature\n",
      "about positional encodings yet.\n",
      "Parallel Attention: In this type of attention, feed-forward and\n",
      "attention layers are parallel to each other rather than sequen-\n",
      "tial in a transformer block. It has been shown to reduce train-\n",
      "----\n",
      "tial in a transformer block. It has been shown to reduce train-\n",
      "ing time by 15%. There is no evidence of performance drop\n",
      "due to this change in the literature and it is used by the models\n",
      "PaLM [15], GPT-NeoX [118], and CodeGen [140].\n",
      "Multi-Query Attention It has shared key and value attention\n",
      "heads in a transformer block while query attention heads are\n",
      "projected as usual. This reduces memory usage and speeds up\n",
      "sampling in autoregressive decoding. No performance degrada-\n",
      "----\n",
      "sampling in autoregressive decoding. No performance degrada-\n",
      "tion has been observed with this change and it makes the train-\n",
      "ing efficient allowing larger batch sizes. Multi-query attention\n",
      "is used in [15, 142].\n",
      "Mixture of Experts: This type of architecture enables eas-\n",
      "ily scaling models to trillions of parameters [92, 91]. Only a\n",
      "few experts are activated during the computation making them\n",
      "compute-efficient. The performance of MoE models is better\n",
      "----\n",
      "compute-efficient. The performance of MoE models is better\n",
      "than dense models for the same amount of data and requires less\n",
      "computation during fine-tuning to achieve performance similar\n",
      "to dense models as discussed in [91]. MoE architectures are\n",
      "less prone to catastrophic forgetting, therefore are more suited\n",
      "for continual learning [92]. Extracting smaller sub-models for\n",
      "downstream tasks is possible without losing any performance,\n",
      "making MoE architecture hardware-friendly [92].\n",
      "----\n",
      "making MoE architecture hardware-friendly [92].\n",
      "Sparse vs Dense Activated: GPT-3 [6] uses sparse transform-\n",
      "ers [67] whereas GLaM [91] and PanGu-P [92] use MoE [121]\n",
      "architectures to lower computational costs and increase the\n",
      "model size and capacity. According to the literature, sparse\n",
      "modules do not degrade the model’s performance [67]. How-\n",
      "ever, more experiments are required to verify this statement.\n",
      "3.8.2. Training Strategies\n",
      "Training models at a huge scale require tricks to reduce train-\n",
      "----\n",
      "3.8.2. Training Strategies\n",
      "Training models at a huge scale require tricks to reduce train-\n",
      "ing costs, avoid loss divergence, and achieve better perfor-\n",
      "mance. We summarize and discuss some of these key tricks\n",
      "used in different LLMs.\n",
      "Mixed Precision: It is a famous method for LLMs to reduce\n",
      "memory usage and improve training efficiency. In mixed pre-\n",
      "cision, forward and backward passes are performed in FP16\n",
      "format whereas optimizer states and master weights are kept\n",
      "----\n",
      "format whereas optimizer states and master weights are kept\n",
      "in FP32 format [120]. A drawback associated with this for-\n",
      "mat change is training instability due to a smaller value range\n",
      "resulting in loss spikes [33]. An alternative to FP16 is BF16\n",
      "which has a comparatively larger range and performs precision-\n",
      "sensitive operations like gradient accumulation and softmax in\n",
      "FP32 [13]. BF16 has better performance and training stability\n",
      "but uses more memory and is supported on specific hardware,\n",
      "----\n",
      "but uses more memory and is supported on specific hardware,\n",
      "for example, A100 GPUs. Therefore, its adoption in LLMs is\n",
      "limited.\n",
      "Training Instability: Loss divergence or spiking is a common\n",
      "issue in LLMs that occurs multiple times during training. This\n",
      "23\n",
      "----\n",
      "happens in the presence of gradient clipping [15]. To mitigate\n",
      "this problem, many approaches suggest restarting training from\n",
      "an earlier checkpoint [15, 33, 91], skipping 200-500 earlier\n",
      "data batches at the point of divergence in [15] and re-shuffling\n",
      "batches in [91]. The embedding layer gradient shrink proves to\n",
      "further stabilize the training as its gradient norm is significantly\n",
      "larger than the other layers [33]. Another suggestion to improve\n",
      "----\n",
      "larger than the other layers [33]. Another suggestion to improve\n",
      "training stability for larger models is not to use biases in dense\n",
      "and norm layers as in [15].\n",
      "Weight Initialization: It plays a significant role in model con-\n",
      "vergence and training stability.\n",
      "GPT-NeoX [118] initializes\n",
      "feed-forward layers before residuals with\n",
      "2\n",
      "L\n",
      "√\n",
      "d as in [153] and\n",
      "other layers with the small initialization scheme [298]. This\n",
      "avoids activations growing exponentially with increasing depth.\n",
      "----\n",
      "avoids activations growing exponentially with increasing depth.\n",
      "MT-NLG [117] found higher variance for weight initialization\n",
      "leads to unstable training, hence validating small initialization\n",
      "scheme [298]. Various models perform random weight initial-\n",
      "ization which can cause bad initialization, Galactica [148] sug-\n",
      "gests a longer warmup to negate the effect.\n",
      "Learning Rate: A suitable learning rate is important for sta-\n",
      "ble training. It is suggested to use a lower value [13, 15, 124]\n",
      "----\n",
      "ble training. It is suggested to use a lower value [13, 15, 124]\n",
      "with warmup and decay (cosine or linear). Usually, the learn-\n",
      "ing rate is within the range 1e−4 to 8e−4. Moreover, MT-NLG\n",
      "(530B) [117] and GPT-NeoX (20B) [118] suggest interpolat-\n",
      "ing learning rates based on the model size using the GPT-3 [6]\n",
      "models ranging between 13B and 175B. This avoids tuning the\n",
      "learning rate hyperparameter.\n",
      "Training Parallelism: 3D parallelism, a combination of data,\n",
      "----\n",
      "learning rate hyperparameter.\n",
      "Training Parallelism: 3D parallelism, a combination of data,\n",
      "pipeline, and tensor parallelism, is the most utilized training\n",
      "parallelism approach in LLMs [33, 15, 14, 13, 117, 115, 112].\n",
      "In addition to 3D parallelism, BLOOM [13] uses a zero op-\n",
      "timizer [37] to shard optimizer states.\n",
      "PanGu-α [108] and\n",
      "PanGu-Σ [92] go beyond 3D parallelism and apply 5D paral-\n",
      "lelism which additionally contains optimizer parallelism and\n",
      "rematerialization.\n",
      "----\n",
      "lelism which additionally contains optimizer parallelism and\n",
      "rematerialization.\n",
      "Mode Switching: It adds task-related tokens at the beginning\n",
      "of the text during training. These tokens refer to the natural\n",
      "language understanding and natural language generation tasks\n",
      "which are shown to improve downstream task performance\n",
      "in [125, 124, 122]. During fine-tuning and inference, tokens\n",
      "are appended based on the downstream tasks.\n",
      "Controllable Text Generation: Generating credible and con-\n",
      "----\n",
      "Controllable Text Generation: Generating credible and con-\n",
      "trolled text from a pre-trained model is challenging. GPT-3 [6]\n",
      "and other LLMs use in-context learning to control generated\n",
      "text. While in-context learning helps in controlling the gener-\n",
      "ated text, ERNIE 3.0 Titan [35] suggests using adversarial loss\n",
      "to rank its generated text for credibility and soft prompts such as\n",
      "genre, topic, keywords, sentiment, and length for better control\n",
      "on generated text.\n",
      "----\n",
      "genre, topic, keywords, sentiment, and length for better control\n",
      "on generated text.\n",
      "3.8.3. Supervised Models vs Generalized Models\n",
      "Although generalized models are capable of performing di-\n",
      "verse tasks with good performance they have not yet outper-\n",
      "formed models trained in supervised settings. The supervised\n",
      "trained models are still state-of-the-art in various NLP tasks by\n",
      "a large margin as shown in [6, 15, 18].\n",
      "3.8.4. Zero-Shot vs Few-Shot\n",
      "----\n",
      "a large margin as shown in [6, 15, 18].\n",
      "3.8.4. Zero-Shot vs Few-Shot\n",
      "LLMs perform well in zero-shot and few-shot settings. But\n",
      "the performance difference between zero-shot and few-shot is\n",
      "large for pre-trained models [6, 15], naming LLMs as meta-\n",
      "learners [6]. LLMs zero-shot evaluations underperform unsu-\n",
      "pervised methods in neural machine translation [6]. The liter-\n",
      "ature shows pre-training is not enough for good zero-shot per-\n",
      "formance [15, 16]. To improve the zero-shot performance the\n",
      "----\n",
      "formance [15, 16]. To improve the zero-shot performance the\n",
      "literature suggests using instruction fine-tuning that improves\n",
      "the zero-shot performance significantly and outperforms base-\n",
      "lines. Instruction fine-tuning has also been shown to improve\n",
      "zero-shot generalization to unseen tasks. Another model, Flan-\n",
      "PaLM [16], unlocks zero-shot reasoning with CoT training.\n",
      "3.8.5. Encoder vs Decoder vs Encoder-Decoder\n",
      "Traditionally, these architectures perform well for different\n",
      "----\n",
      "Traditionally, these architectures perform well for different\n",
      "tasks, for example, encoder-only for NLU tasks, decoder-only\n",
      "for NLG, and encoder-decoder for sequence2sequence model-\n",
      "ing. Encoder-only models are famous for smaller models such\n",
      "as Bert [7], RoBERTa [299], etc., whereas LLMs are either\n",
      "decoder-only [6, 118, 13] or encoder-decoder [10, 11, 122].\n",
      "While decoder-only models are good at NLG tasks, various\n",
      "LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\n",
      "----\n",
      "LLMs, PaLM [15], OPT [14], GPT-3 [6], BLOOM [13],\n",
      "LLaMA [156], are decoder-only models with significant per-\n",
      "formance gains on both NLU and NLG tasks. In contradic-\n",
      "tion to this, T5 [10] and UL2 [125] identify encoder-decoder\n",
      "models out-performing decoder-only models. In another study,\n",
      "PaLM [15] finds increasing the size of decoder-only models\n",
      "can reduce the performance gap between decoder-only and\n",
      "encoder-decoder architectures.\n",
      "Although decoder-only architectures have become a trend for\n",
      "----\n",
      "encoder-decoder architectures.\n",
      "Although decoder-only architectures have become a trend for\n",
      "LLMs, many recently proposed approaches [125, 122] use\n",
      "mode-switching tokens in text with encoder-decoder architec-\n",
      "tures to enable task-specific modes. Similarly, CodeT5+ [34]\n",
      "uses an encoder-decoder architecture with multiple training ob-\n",
      "jectives for different tasks, activating the encoder, decoder, or\n",
      "both according to the tasks. These variations in architecture\n",
      "----\n",
      "both according to the tasks. These variations in architecture\n",
      "and training objectives allow a model to perform well in differ-\n",
      "ent settings. Because of this dynamic configuration, the future\n",
      "of LLMs can be attributed to encoder-decoder architectures.\n",
      "4. Model Configurations\n",
      "We provide different statistics of pre-trained and instruction-\n",
      "tuned models in this section. This includes information such as\n",
      "publication venue, license type, model creators, steps trained,\n",
      "----\n",
      "publication venue, license type, model creators, steps trained,\n",
      "parallelism, etc in Table 3 and Table 4. Architecture details\n",
      "of pre-trained LLMs are available in Table 5. Providing these\n",
      "details for instruction-tuned models is unnecessary because it\n",
      "fine-tunes pre-trained models for instruction datasets. Hence,\n",
      "architectural details are the same as the baselines. Moreover,\n",
      "optimization settings for various LLMs are available in Table 6\n",
      "----\n",
      "optimization settings for various LLMs are available in Table 6\n",
      "and Table 7. We do not include details on precision, warmup,\n",
      "and weight decay in Table 7. These details are not as important\n",
      "as others to mention for instruction-tuned models, and are not\n",
      "provided by the papers.\n",
      "24\n",
      "----\n",
      "Table 3: Summary of pre-trained LLMs (>10B). Only the LLMs discussed individually in the previous sections are summarized. “Data/Tokens” is the model’s\n",
      "pre-training data, which is either the number of tokens or data size. “Data Cleaning” indicates whether data cleaning is performed or not. This includes heuristics\n",
      "(Heur), deduplication (Dedup), quality filtering (QF), and privacy filtering (PF), “Cost” is the calculated training cost obtained by multiplying the GPUs/TPUs\n",
      "----\n",
      "hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting a discounted rate,\n",
      "re-training, number of employees working on the problem, etc. “Training Parallelism” indicates distributed training using data parallelism (D), tensor parallelism\n",
      "(T), pipeline parallelism (P), context parallelism (C), model parallelism (M), optimizer parallelism (OP), and rematerialization (R), where for “Library” column,\n",
      "----\n",
      "“DS” is a short form for Deep Speed. In column “Commercial Use”, we assumed a model is for non-commercial purposes if its license is unavailable.\n",
      "Models\n",
      "Publication\n",
      "Venue\n",
      "License\n",
      "Type\n",
      "Model\n",
      "Creators Purpose\n",
      "No. of\n",
      "Params\n",
      "Commercial\n",
      "Use\n",
      "Steps\n",
      "Trained\n",
      "Data/\n",
      "Tokens\n",
      "Data\n",
      "Cleaning\n",
      "No. of\n",
      "Processing Units\n",
      "Processing\n",
      "Unit Type\n",
      "Training\n",
      "Time\n",
      "Calculated\n",
      "Train. Cost\n",
      "Training\n",
      "Parallelism\n",
      "Library\n",
      "T5 [10]\n",
      "JMLR'20\n",
      "Apache-2.0\n",
      "Google\n",
      "General\n",
      "11B\n",
      "✓\n",
      "1M\n",
      "1T\n",
      "Heur+Dedup\n",
      "1024\n",
      "TPU v3\n",
      "-\n",
      "-\n",
      "D+M\n",
      "Mesh TensorFlow\n",
      "GPT-3 [6]\n",
      "----\n",
      "Apache-2.0\n",
      "Google\n",
      "General\n",
      "11B\n",
      "✓\n",
      "1M\n",
      "1T\n",
      "Heur+Dedup\n",
      "1024\n",
      "TPU v3\n",
      "-\n",
      "-\n",
      "D+M\n",
      "Mesh TensorFlow\n",
      "GPT-3 [6]\n",
      "NeurIPS'20\n",
      "-\n",
      "OpenAI\n",
      "General\n",
      "175B\n",
      "×\n",
      "-\n",
      "300B\n",
      "Dedup+QF\n",
      "-\n",
      "V100\n",
      "-\n",
      "-\n",
      "M\n",
      "-\n",
      "mT5 [11]\n",
      "NAACL'21\n",
      "Apache-2.0\n",
      "Google\n",
      "General\n",
      "13B\n",
      "✓\n",
      "1M\n",
      "1T\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "PanGu-α [108]\n",
      "arXiv'21\n",
      "Apache-2.0\n",
      "Huawei\n",
      "General\n",
      "200B\n",
      "✓\n",
      "260k\n",
      "1.1TB\n",
      "Heur+Dedup\n",
      "2048\n",
      "Ascend 910\n",
      "-\n",
      "-\n",
      "D+OP+P+O+R\n",
      "MindSpore\n",
      "CPM-2 [12]\n",
      "AI Open'21\n",
      "MIT\n",
      "Tsinghua\n",
      "General\n",
      "198B\n",
      "✓\n",
      "1M\n",
      "2.6TB\n",
      "Dedup\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "D+M\n",
      "JAXFormer\n",
      "Codex [141]\n",
      "arXiv'21\n",
      "-\n",
      "OpenAI\n",
      "Coding\n",
      "12B\n",
      "×\n",
      "-\n",
      "100B\n",
      "Heur\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "----\n",
      "1M\n",
      "2.6TB\n",
      "Dedup\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "D+M\n",
      "JAXFormer\n",
      "Codex [141]\n",
      "arXiv'21\n",
      "-\n",
      "OpenAI\n",
      "Coding\n",
      "12B\n",
      "×\n",
      "-\n",
      "100B\n",
      "Heur\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "ERNIE 3.0 [110]\n",
      "arXiv'21\n",
      "-\n",
      "Baidu\n",
      "General\n",
      "10B\n",
      "×\n",
      "120k∗\n",
      "375B\n",
      "Heur+Dedup\n",
      "384\n",
      "V100\n",
      "-\n",
      "-\n",
      "M∗\n",
      "PaddlePaddle\n",
      "Jurassic-1 [112]\n",
      "White-Paper'21 Apache-2.0\n",
      "AI21\n",
      "General\n",
      "178B\n",
      "✓\n",
      "-\n",
      "300B\n",
      "-\n",
      "800\n",
      "GPU\n",
      "-\n",
      "-\n",
      "D+M+P\n",
      "Megatron+DS\n",
      "HyperCLOVA [114]\n",
      "EMNLP'21\n",
      "-\n",
      "Naver\n",
      "General\n",
      "82B\n",
      "×\n",
      "-\n",
      "300B\n",
      "Clf+Dedup+PF\n",
      "1024\n",
      "A100\n",
      "321h\n",
      "1.32 Mil\n",
      "M\n",
      "Megatron\n",
      "Yuan 1.0 [115]\n",
      "arXiv'21\n",
      "Apache-2.0\n",
      "-\n",
      "General\n",
      "245B\n",
      "✓\n",
      "26k∗\n",
      "180B Heur+Clf+Dedup\n",
      "2128\n",
      "GPU\n",
      "-\n",
      "-\n",
      "----\n",
      "Megatron\n",
      "Yuan 1.0 [115]\n",
      "arXiv'21\n",
      "Apache-2.0\n",
      "-\n",
      "General\n",
      "245B\n",
      "✓\n",
      "26k∗\n",
      "180B Heur+Clf+Dedup\n",
      "2128\n",
      "GPU\n",
      "-\n",
      "-\n",
      "D+T+P\n",
      "-\n",
      "Gopher [116]\n",
      "arXiv'21\n",
      "-\n",
      "Google\n",
      "General\n",
      "280B\n",
      "×\n",
      "-\n",
      "300B\n",
      "QF+Dedup\n",
      "4096\n",
      "TPU v3\n",
      "920h\n",
      "13.19 Mil\n",
      "D+M\n",
      "JAX+Haiku\n",
      "ERNIE 3.0 Titan [35]\n",
      "arXiv'21\n",
      "-\n",
      "Baidu\n",
      "General\n",
      "260B\n",
      "×\n",
      "-\n",
      "300B\n",
      "Heur+Dedup\n",
      "-\n",
      "Ascend 910\n",
      "-\n",
      "-\n",
      "D+M+P+D*\n",
      "PaddlePaddle\n",
      "GPT-NeoX-20B [118] BigScience'22\n",
      "Apache-2.0\n",
      "EleutherAI\n",
      "General\n",
      "20B\n",
      "✓\n",
      "150k\n",
      "825GB\n",
      "None\n",
      "96\n",
      "40G A100\n",
      "-\n",
      "-\n",
      "M\n",
      "Megatron+DS+PyTorch\n",
      "OPT [14]\n",
      "arXiv'22\n",
      "MIT\n",
      "Meta\n",
      "General\n",
      "175B\n",
      "✓\n",
      "150k\n",
      "180B\n",
      "----\n",
      "None\n",
      "96\n",
      "40G A100\n",
      "-\n",
      "-\n",
      "M\n",
      "Megatron+DS+PyTorch\n",
      "OPT [14]\n",
      "arXiv'22\n",
      "MIT\n",
      "Meta\n",
      "General\n",
      "175B\n",
      "✓\n",
      "150k\n",
      "180B\n",
      "Dedup\n",
      "992\n",
      "80G A100\n",
      "-\n",
      "-\n",
      "D+T\n",
      "Megatron\n",
      "BLOOM [13]\n",
      "arXiv'22\n",
      "RAIL-1.0\n",
      "BigScience\n",
      "General\n",
      "176B\n",
      "✓\n",
      "-\n",
      "366B\n",
      "Dedup+PR\n",
      "384\n",
      "80G A100\n",
      "2520h\n",
      "3.87 Mil\n",
      "D+T+P\n",
      "Megatron+DS\n",
      "Galactica [148]\n",
      "arXiv'22\n",
      "Apache-2.0\n",
      "Meta\n",
      "Science\n",
      "120B\n",
      "×\n",
      "225k\n",
      "106B\n",
      "Dedup\n",
      "128\n",
      "80GB A100\n",
      "-\n",
      "-\n",
      "-\n",
      "Metaseq\n",
      "GLaM [91]\n",
      "ICML'22\n",
      "-\n",
      "Google\n",
      "General\n",
      "1.2T\n",
      "×\n",
      "600k∗\n",
      "600B\n",
      "Clf\n",
      "1024\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "M\n",
      "GSPMD\n",
      "LaMDA [150]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "Dialog\n",
      "137B\n",
      "×\n",
      "3M\n",
      "2.81T\n",
      "Filtered\n",
      "1024\n",
      "----\n",
      "600B\n",
      "Clf\n",
      "1024\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "M\n",
      "GSPMD\n",
      "LaMDA [150]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "Dialog\n",
      "137B\n",
      "×\n",
      "3M\n",
      "2.81T\n",
      "Filtered\n",
      "1024\n",
      "TPU v3\n",
      "1384h\n",
      "4.96 Mil\n",
      "D+M\n",
      "Lingvo\n",
      "MT-NLG [117]\n",
      "arXiv'22\n",
      "Apache-v2.0 MS.+Nvidia General\n",
      "530B\n",
      "×\n",
      "-\n",
      "270B\n",
      "-\n",
      "4480\n",
      "80G A100\n",
      "-\n",
      "-\n",
      "D+T+P\n",
      "Megatron+DS\n",
      "AlphaCode [142]\n",
      "Science'22\n",
      "Apache-v2.0\n",
      "Google\n",
      "Coding\n",
      "41B\n",
      "✓\n",
      "205k\n",
      "967B\n",
      "Heur+Dedup\n",
      "-\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "M\n",
      "JAX+Haiku\n",
      "Chinchilla [96]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "General\n",
      "70B\n",
      "×\n",
      "-\n",
      "1.4T\n",
      "QF+Dedup\n",
      "-\n",
      "TPUv4\n",
      "-\n",
      "-\n",
      "-\n",
      "JAX+Haiku\n",
      "PaLM [15]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "General\n",
      "540B\n",
      "×\n",
      "255k\n",
      "780B\n",
      "Heur\n",
      "6144\n",
      "----\n",
      "QF+Dedup\n",
      "-\n",
      "TPUv4\n",
      "-\n",
      "-\n",
      "-\n",
      "JAX+Haiku\n",
      "PaLM [15]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "General\n",
      "540B\n",
      "×\n",
      "255k\n",
      "780B\n",
      "Heur\n",
      "6144\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "D+M\n",
      "JAX+T5X\n",
      "AlexaTM [122]\n",
      "arXiv'22\n",
      "Apache v2.0\n",
      "Amazon\n",
      "General\n",
      "20B\n",
      "×\n",
      "500k\n",
      "1.1T\n",
      "Filtered\n",
      "128\n",
      "A100\n",
      "2880h\n",
      "1.47 Mil\n",
      "M\n",
      "DS\n",
      "U-PaLM [124]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "General\n",
      "540B\n",
      "×\n",
      "20k\n",
      "-\n",
      "-\n",
      "512\n",
      "TPU v4\n",
      "120h\n",
      "0.25 Mil\n",
      "-\n",
      "-\n",
      "UL2 [125]\n",
      "ICLR'23\n",
      "Apache-2.0\n",
      "Google\n",
      "General\n",
      "20B\n",
      "✓\n",
      "2M\n",
      "1T\n",
      "-\n",
      "512\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "M\n",
      "JAX+T5X\n",
      "GLM [33]\n",
      "ICLR'23\n",
      "Apache-2.0\n",
      "Multiple\n",
      "General\n",
      "130B\n",
      "×\n",
      "-\n",
      "400B\n",
      "-\n",
      "768\n",
      "40G A100\n",
      "1440h\n",
      "3.37 Mil\n",
      "M\n",
      "-\n",
      "CodeGen [140]\n",
      "----\n",
      "ICLR'23\n",
      "Apache-2.0\n",
      "Multiple\n",
      "General\n",
      "130B\n",
      "×\n",
      "-\n",
      "400B\n",
      "-\n",
      "768\n",
      "40G A100\n",
      "1440h\n",
      "3.37 Mil\n",
      "M\n",
      "-\n",
      "CodeGen [140]\n",
      "ICLR'23\n",
      "Apache-2.0\n",
      "Salesforce\n",
      "Coding\n",
      "16B\n",
      "✓\n",
      "650k\n",
      "577B\n",
      "Heur+Dedup\n",
      "-\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "D+M\n",
      "JAXFormer\n",
      "LLaMA [127]\n",
      "arXiv'23\n",
      "-\n",
      "Meta\n",
      "General\n",
      "65B\n",
      "×\n",
      "350k\n",
      "1.4T Clf+Heur+Dedup\n",
      "2048\n",
      "80G A100\n",
      "504h\n",
      "4.12 Mil\n",
      "D+M\n",
      "xFormers\n",
      "PanGuΣ [92]\n",
      "arXiv'23\n",
      "-\n",
      "Huawei\n",
      "General 1.085T\n",
      "×\n",
      "-\n",
      "329B\n",
      "-\n",
      "512\n",
      "Ascend 910\n",
      "2400h\n",
      "-\n",
      "D+OP+P+O+R\n",
      "MindSpore\n",
      "BloombergGPT [151]\n",
      "arXiv23\n",
      "-\n",
      "Bloomberg\n",
      "Finance\n",
      "50B\n",
      "×\n",
      "139k\n",
      "569B\n",
      "Dedup\n",
      "512\n",
      "40G A100\n",
      "1272h\n",
      "1.97 Mil\n",
      "M\n",
      "----\n",
      "BloombergGPT [151]\n",
      "arXiv23\n",
      "-\n",
      "Bloomberg\n",
      "Finance\n",
      "50B\n",
      "×\n",
      "139k\n",
      "569B\n",
      "Dedup\n",
      "512\n",
      "40G A100\n",
      "1272h\n",
      "1.97 Mil\n",
      "M\n",
      "PyTorch\n",
      "Xuan Yuan 2.0 [152]\n",
      "arXiv23\n",
      "RAIL-1.0\n",
      "Du Xiaoman Finance\n",
      "176B\n",
      "✓\n",
      "-\n",
      "366B\n",
      "Filtered\n",
      "-\n",
      "80GB A100\n",
      "-\n",
      "-\n",
      "P\n",
      "DS\n",
      "CodeT5+ [34]\n",
      "arXiv'23\n",
      "BSD-3\n",
      "Salesforce\n",
      "Coding\n",
      "16B\n",
      "✓\n",
      "110k\n",
      "51.5B\n",
      "Dedup\n",
      "16\n",
      "40G A100\n",
      "-\n",
      "-\n",
      "-\n",
      "DS\n",
      "StarCoder [147]\n",
      "arXiv'23\n",
      "OpenRAIL-M BigCode\n",
      "Coding\n",
      "15.5B\n",
      "✓\n",
      "250k\n",
      "1T\n",
      "Dedup+QF+PF\n",
      "512\n",
      "80G A100\n",
      "624h\n",
      "1.28 Mil\n",
      "D+T+P\n",
      "Megatron-LM\n",
      "LLaMA-2 [21]\n",
      "arXiv'23\n",
      "LLaMA-2.0\n",
      "Meta\n",
      "General\n",
      "70B\n",
      "✓\n",
      "500k\n",
      "2T\n",
      "Minimal Filtering\n",
      "-\n",
      "----\n",
      "D+T+P\n",
      "Megatron-LM\n",
      "LLaMA-2 [21]\n",
      "arXiv'23\n",
      "LLaMA-2.0\n",
      "Meta\n",
      "General\n",
      "70B\n",
      "✓\n",
      "500k\n",
      "2T\n",
      "Minimal Filtering\n",
      "-\n",
      "80G A100\n",
      "1.7Mh\n",
      "-\n",
      "-\n",
      "-\n",
      "PaLM-2 [123]\n",
      "arXiv'23\n",
      "-\n",
      "Google\n",
      "General\n",
      "-\n",
      "×\n",
      "-\n",
      "-\n",
      "Ddedup+PF+QF\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "LLaMA-3.1 [130]\n",
      "arXiv'24\n",
      "LLaMA-3.0\n",
      "Meta\n",
      "General\n",
      "405B\n",
      "✓\n",
      "1.2M\n",
      "15T\n",
      "Dedup+QF\n",
      "16k\n",
      "80G H100 30.84Mh\n",
      "-\n",
      "D+T+P+C\n",
      "PyTorch\n",
      "Mixtral 8x22B [131]\n",
      "web'24\n",
      "Apache-2.0\n",
      "Mistral AI\n",
      "General\n",
      "141B\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Snowflake Arctic [132] web'24\n",
      "Apache-2.0\n",
      "Snowflake\n",
      "General\n",
      "480B\n",
      "✓\n",
      "-\n",
      "3.5T\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "T+P\n",
      "DS\n",
      "Nemotron-4 340B [137]web'24\n",
      "----\n",
      "Apache-2.0\n",
      "Snowflake\n",
      "General\n",
      "480B\n",
      "✓\n",
      "-\n",
      "3.5T\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "T+P\n",
      "DS\n",
      "Nemotron-4 340B [137]web'24\n",
      "Nvidia\n",
      "Nvidia\n",
      "General\n",
      "340B\n",
      "✓\n",
      "-\n",
      "9T\n",
      "-\n",
      "6144\n",
      "80G H100\n",
      "-\n",
      "-\n",
      "D+T+P\n",
      "-\n",
      "DeepSeek [138]\n",
      "arXiv'24\n",
      "MIT\n",
      "DeepSeek\n",
      "General\n",
      "67B\n",
      "✓\n",
      "-\n",
      "2T\n",
      "Dedup+QF\n",
      "-\n",
      "-\n",
      "300.6Kh\n",
      "-\n",
      "D+T+P\n",
      "DS\n",
      "DeepSeek-v2 [139]\n",
      "arXiv'24\n",
      "MIT\n",
      "DeepSeek\n",
      "General\n",
      "67B\n",
      "✓\n",
      "-\n",
      "8.1T\n",
      "QF\n",
      "-\n",
      "H800\n",
      "172.8Kh\n",
      "-\n",
      "D+P\n",
      "HAI-LLM\n",
      "Table 4: Summary of instruction tuned LLMs (>10B). All abbreviations are the same as Table 3. Entries in “Data/Tokens” starting with “S-” represent the number\n",
      "----\n",
      "of training samples.\n",
      "Models\n",
      "Publication\n",
      "Venue\n",
      "License\n",
      "Type\n",
      "Model\n",
      "Creators\n",
      "Purpose\n",
      "No. of\n",
      "Params\n",
      "Commercial\n",
      "Use\n",
      "Pre-trained\n",
      "Models\n",
      "Steps\n",
      "Trained\n",
      "Data/\n",
      "Tokens\n",
      "No. of\n",
      "Processing Units\n",
      "Processing\n",
      "Unit Type\n",
      "Train.\n",
      "Time\n",
      "Calculated\n",
      "Train. Cost\n",
      "Train.\n",
      "Parallelism\n",
      "Library\n",
      "WebGPT [166]\n",
      "arXiv'21\n",
      "-\n",
      "OpenAI\n",
      "General\n",
      "175B\n",
      "×\n",
      "GPT-3\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "T0 [17]\n",
      "ICLR'22\n",
      "Apache-2.0\n",
      "BigScience\n",
      "General\n",
      "11B\n",
      "✓\n",
      "T5\n",
      "-\n",
      "250B\n",
      "512\n",
      "TPU v3\n",
      "270h\n",
      "0.48 Mil\n",
      "-\n",
      "-\n",
      "Tk-Instruct [18]\n",
      "EMNLP'22\n",
      "MIT\n",
      "AI2+\n",
      "General\n",
      "11B\n",
      "✓\n",
      "T5\n",
      "1000\n",
      "-\n",
      "256\n",
      "TPU v3\n",
      "4h\n",
      "----\n",
      "TPU v3\n",
      "270h\n",
      "0.48 Mil\n",
      "-\n",
      "-\n",
      "Tk-Instruct [18]\n",
      "EMNLP'22\n",
      "MIT\n",
      "AI2+\n",
      "General\n",
      "11B\n",
      "✓\n",
      "T5\n",
      "1000\n",
      "-\n",
      "256\n",
      "TPU v3\n",
      "4h\n",
      "0.0036 Mil\n",
      "-\n",
      "Google T5\n",
      "OPT-IML [97]\n",
      "arXiv'22\n",
      "-\n",
      "Meta\n",
      "General\n",
      "175B\n",
      "×\n",
      "OPT\n",
      "8k\n",
      "2B\n",
      "128\n",
      "40G A100\n",
      "-\n",
      "-\n",
      "D+T\n",
      "Megatron\n",
      "Flan-U-PaLM [16] ICLR'22\n",
      "Apache-2.0\n",
      "Google\n",
      "General\n",
      "540B\n",
      "✓\n",
      "U-PaLM\n",
      "30k\n",
      "-\n",
      "512\n",
      "TPU v4\n",
      "-\n",
      "-\n",
      "-\n",
      "JAX+T5X\n",
      "mT0 [154]\n",
      "ACL'23\n",
      "Apache-2.0 HuggingFace+ General\n",
      "13B\n",
      "✓\n",
      "mT5\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Sparrow [167]\n",
      "arXiv'22\n",
      "-\n",
      "Google\n",
      "Dialog\n",
      "70B\n",
      "×\n",
      "Chinchilla\n",
      "-\n",
      "-\n",
      "64\n",
      "TPU v3\n",
      "-\n",
      "-\n",
      "M\n",
      "-\n",
      "WizardCoder [164] arXiv'23\n",
      "Apache-2.0\n",
      "----\n",
      "-\n",
      "Google\n",
      "Dialog\n",
      "70B\n",
      "×\n",
      "Chinchilla\n",
      "-\n",
      "-\n",
      "64\n",
      "TPU v3\n",
      "-\n",
      "-\n",
      "M\n",
      "-\n",
      "WizardCoder [164] arXiv'23\n",
      "Apache-2.0\n",
      "HK Bapt.\n",
      "Coding\n",
      "15B\n",
      "×\n",
      "StarCoder\n",
      "200\n",
      "S-78k\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Alpaca [158]\n",
      "Github'23\n",
      "Apache-2.0\n",
      "Stanford\n",
      "General\n",
      "13B\n",
      "✓\n",
      "LLaMA\n",
      "3-Epoch\n",
      "S-52k\n",
      "8\n",
      "80G A100\n",
      "3h\n",
      "600\n",
      "FSDP\n",
      "PyTorch\n",
      "Vicuna [159]\n",
      "Github'23\n",
      "Apache-2.0\n",
      "LMSYS\n",
      "General\n",
      "13B\n",
      "✓\n",
      "LLaMA\n",
      "3-Epoch S-125k\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "FSDP\n",
      "PyTorch\n",
      "LIMA [185]\n",
      "arXiv'23\n",
      "-\n",
      "Meta+\n",
      "General\n",
      "65B\n",
      "-\n",
      "LLaMA\n",
      "15-Epoch S-1000\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Koala [300]\n",
      "Github'23\n",
      "Apache-2.0\n",
      "UC-Berkley\n",
      "General\n",
      "13B\n",
      "×\n",
      "LLaMA\n",
      "----\n",
      "-\n",
      "LLaMA\n",
      "15-Epoch S-1000\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Koala [300]\n",
      "Github'23\n",
      "Apache-2.0\n",
      "UC-Berkley\n",
      "General\n",
      "13B\n",
      "×\n",
      "LLaMA\n",
      "2-Epoch S-472k\n",
      "8\n",
      "A100\n",
      "6h\n",
      "100\n",
      "-\n",
      "JAX/FLAX\n",
      "5. Datasets and Evaluation\n",
      "Generating training and evaluation datasets is expensive be-\n",
      "cause of the large-scale data demand of LLMs. Hence, datasets\n",
      "for training and benchmarking these models are topics of key\n",
      "importance. A summary of datasets commonly used by LLMs\n",
      "is provided next.\n",
      "5.1. Training Datasets\n",
      "----\n",
      "importance. A summary of datasets commonly used by LLMs\n",
      "is provided next.\n",
      "5.1. Training Datasets\n",
      "The performance of LLMs largely depends on the training\n",
      "data’s quality, size, and diversity. Preparing training datasets\n",
      "of high quality at a large scale is laborious. Researchers have\n",
      "suggested various pre-training and fine-tuning datasets to en-\n",
      "hance LLMs capabilities. We summarize these efforts in Ta-\n",
      "ble 8. While numerous training datasets are available in the\n",
      "----\n",
      "ble 8. While numerous training datasets are available in the\n",
      "literature, we cover the most widely used ones in our summary.\n",
      "25\n",
      "----\n",
      "Table 5: Architecture details of LLMs. Here, “PE” is the positional embedding, “nL” is the number of layers, “nH” is the number of attention heads, “HS” is the\n",
      "size of hidden states.\n",
      "Models\n",
      "Type\n",
      "Training\n",
      "Objective\n",
      "Attention\n",
      "Vocab\n",
      "Tokenizer\n",
      "Norm\n",
      "PE\n",
      "Activation\n",
      "Bias\n",
      "nL\n",
      "nH\n",
      "HS\n",
      "T5 (11B)\n",
      "Enc-Dec\n",
      "Span Corruption\n",
      "Standard\n",
      "32k\n",
      "SentencePiece\n",
      "Pre-RMS\n",
      "Relative\n",
      "ReLU\n",
      "×\n",
      "24\n",
      "128\n",
      "1024\n",
      "GPT3 (175B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Dense+Sparse\n",
      "-\n",
      "-\n",
      "Layer\n",
      "Learned\n",
      "GeLU\n",
      "✓\n",
      "96\n",
      "96\n",
      "12288\n",
      "mT5 (13B)\n",
      "Enc-Dec\n",
      "Span Corruption\n",
      "Standard\n",
      "250k\n",
      "----\n",
      "Dense+Sparse\n",
      "-\n",
      "-\n",
      "Layer\n",
      "Learned\n",
      "GeLU\n",
      "✓\n",
      "96\n",
      "96\n",
      "12288\n",
      "mT5 (13B)\n",
      "Enc-Dec\n",
      "Span Corruption\n",
      "Standard\n",
      "250k\n",
      "SentencePiece\n",
      "Pre-RMS\n",
      "Relative\n",
      "ReLU\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "PanGu-α (200B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "40k\n",
      "BPE\n",
      "Layer\n",
      "-\n",
      "-\n",
      "-\n",
      "64\n",
      "128\n",
      "16384\n",
      "CPM-2 (198B)\n",
      "Enc-Dec\n",
      "Span Corruption\n",
      "Standard\n",
      "250k\n",
      "SentencePiece\n",
      "Pre-RMS\n",
      "Relative\n",
      "ReLU\n",
      "-\n",
      "24\n",
      "64\n",
      "-\n",
      "Codex (12B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "BPE+\n",
      "Pre-Layer\n",
      "Learned\n",
      "GeLU\n",
      "-\n",
      "96\n",
      "96\n",
      "12288\n",
      "ERNIE 3.0 (10B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "WordPiece\n",
      "Post-Layer\n",
      "Relative\n",
      "GeLU\n",
      "-\n",
      "48\n",
      "64\n",
      "----\n",
      "12288\n",
      "ERNIE 3.0 (10B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "WordPiece\n",
      "Post-Layer\n",
      "Relative\n",
      "GeLU\n",
      "-\n",
      "48\n",
      "64\n",
      "4096\n",
      "Jurassic-1 (178B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "256k\n",
      "SentencePiece∗\n",
      "Pre-Layer\n",
      "Learned\n",
      "GeLU\n",
      "✓\n",
      "76\n",
      "96\n",
      "13824\n",
      "HyperCLOVA (82B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Dense+Sparse\n",
      "-\n",
      "BPE*\n",
      "Pre-Layer\n",
      "Learned\n",
      "GeLU\n",
      "-\n",
      "64\n",
      "80\n",
      "10240\n",
      "Yuan 1.0 (245B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "76\n",
      "-\n",
      "16384\n",
      "Gopher (280B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "32k\n",
      "SentencePiece\n",
      "Pre-RMS\n",
      "Relative\n",
      "GeLU\n",
      "✓\n",
      "80\n",
      "128\n",
      "16384\n",
      "ERNIE 3.0 Titan (260B)\n",
      "----\n",
      "Next Token\n",
      "Standard\n",
      "32k\n",
      "SentencePiece\n",
      "Pre-RMS\n",
      "Relative\n",
      "GeLU\n",
      "✓\n",
      "80\n",
      "128\n",
      "16384\n",
      "ERNIE 3.0 Titan (260B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "WordPiece\n",
      "Post-Layer\n",
      "Relative\n",
      "GeLU\n",
      "-\n",
      "48\n",
      "192\n",
      "12288\n",
      "GPT-NeoX-20B\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Parallel\n",
      "50k\n",
      "BPE\n",
      "Layer\n",
      "Rotary\n",
      "GeLU\n",
      "✓\n",
      "44\n",
      "64\n",
      "-\n",
      "OPT (175B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "BPE\n",
      "-\n",
      "-\n",
      "ReLU\n",
      "✓\n",
      "96\n",
      "96\n",
      "-\n",
      "BLOOM (176B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "250k\n",
      "BPE\n",
      "Layer\n",
      "ALiBi\n",
      "GeLU\n",
      "✓\n",
      "70\n",
      "112\n",
      "14336\n",
      "Galactica (120B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "50k\n",
      "BPE+custom\n",
      "Layer\n",
      "Learned\n",
      "GeLU\n",
      "×\n",
      "96\n",
      "----\n",
      "70\n",
      "112\n",
      "14336\n",
      "Galactica (120B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "50k\n",
      "BPE+custom\n",
      "Layer\n",
      "Learned\n",
      "GeLU\n",
      "×\n",
      "96\n",
      "80\n",
      "10240\n",
      "GLaM (1.2T)\n",
      "MoE-Dec\n",
      "Next Token\n",
      "Standard\n",
      "256k\n",
      "SentencePiece\n",
      "Layer\n",
      "Relative\n",
      "GeLU\n",
      "✓\n",
      "64\n",
      "128\n",
      "32768\n",
      "LaMDA (137B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "32k\n",
      "BPE\n",
      "Layer\n",
      "Relative\n",
      "GeGLU\n",
      "-\n",
      "64\n",
      "128\n",
      "8192\n",
      "MT-NLG (530B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "50k\n",
      "BPE\n",
      "Pre-Layer\n",
      "Learned\n",
      "GeLU\n",
      "✓\n",
      "105\n",
      "128\n",
      "20480\n",
      "AlphaCode (41B)\n",
      "Enc-Dec\n",
      "Next Token\n",
      "Multi-query\n",
      "8k\n",
      "SentencePiece\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "64\n",
      "128\n",
      "6144\n",
      "Chinchilla (70B)\n",
      "Causal-Dec\n",
      "----\n",
      "Enc-Dec\n",
      "Next Token\n",
      "Multi-query\n",
      "8k\n",
      "SentencePiece\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "64\n",
      "128\n",
      "6144\n",
      "Chinchilla (70B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "32k\n",
      "SentencePiece-NFKC\n",
      "Pre-RMS\n",
      "Relative\n",
      "GeLU\n",
      "✓\n",
      "80\n",
      "64\n",
      "8192\n",
      "PaLM (540B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Parallel+Multi-query\n",
      "256k\n",
      "SentencePiece\n",
      "Layer\n",
      "RoPE\n",
      "SwiGLU\n",
      "×\n",
      "118\n",
      "48\n",
      "18432\n",
      "AlexaTM (20B)\n",
      "Enc-Dec\n",
      "Denoising\n",
      "Standard\n",
      "150k\n",
      "SentencePiece\n",
      "Pre-Layer\n",
      "Learned\n",
      "GeLU\n",
      "✓\n",
      "78\n",
      "32\n",
      "4096\n",
      "Sparrow (70B)\n",
      "Causal-Dec\n",
      "Pref.&Rule RM\n",
      "-\n",
      "32k\n",
      "SentencePiece-NFKC\n",
      "Pre-RMS\n",
      "Relative\n",
      "GeLU\n",
      "✓\n",
      "16∗\n",
      "64\n",
      "8192\n",
      "U-PaLM (540B)\n",
      "----\n",
      "Causal-Dec\n",
      "Pref.&Rule RM\n",
      "-\n",
      "32k\n",
      "SentencePiece-NFKC\n",
      "Pre-RMS\n",
      "Relative\n",
      "GeLU\n",
      "✓\n",
      "16∗\n",
      "64\n",
      "8192\n",
      "U-PaLM (540B)\n",
      "Non-Causal-Dec\n",
      "MoD\n",
      "Parallel+Multi-query\n",
      "256k\n",
      "SentencePiece\n",
      "Layer\n",
      "RoPE\n",
      "SwiGLU\n",
      "×\n",
      "118\n",
      "48\n",
      "18432\n",
      "UL2 (20B)\n",
      "Enc-Dec\n",
      "MoD\n",
      "Standard\n",
      "32k\n",
      "SentencePiece\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "64\n",
      "16\n",
      "4096\n",
      "GLM (130B)\n",
      "Non-Causal-Dec\n",
      "AR Blank Infilling\n",
      "Standard\n",
      "130k\n",
      "SentencePiece\n",
      "Deep\n",
      "RoPE\n",
      "GeGLU\n",
      "✓\n",
      "70\n",
      "96\n",
      "12288\n",
      "CodeGen (16B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Parallel\n",
      "-\n",
      "BPE\n",
      "Layer\n",
      "RoPE\n",
      "-\n",
      "-\n",
      "34\n",
      "24\n",
      "-\n",
      "LLaMA (65B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "32k\n",
      "BPE\n",
      "Pre-RMS\n",
      "----\n",
      "Parallel\n",
      "-\n",
      "BPE\n",
      "Layer\n",
      "RoPE\n",
      "-\n",
      "-\n",
      "34\n",
      "24\n",
      "-\n",
      "LLaMA (65B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "32k\n",
      "BPE\n",
      "Pre-RMS\n",
      "RoPE\n",
      "SwiGLU\n",
      "-\n",
      "80\n",
      "64\n",
      "8192\n",
      "PanGu-Σ (1085B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "-\n",
      "BPE\n",
      "Fused Layer\n",
      "-\n",
      "FastGeLU\n",
      "-\n",
      "40\n",
      "40\n",
      "5120\n",
      "BloombergGPT (50B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "131k\n",
      "Unigram\n",
      "Layer\n",
      "ALiBi\n",
      "GeLU\n",
      "✓\n",
      "70\n",
      "40\n",
      "7680\n",
      "Xuan Yuan 2.0 (176B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Self\n",
      "250k\n",
      "BPE\n",
      "Layer\n",
      "ALiBi\n",
      "GeLU\n",
      "✓\n",
      "70\n",
      "112\n",
      "14336\n",
      "CodeT5+ (16B)\n",
      "Enc-Dec\n",
      "SC+NT+Cont.+Match\n",
      "Standard\n",
      "-\n",
      "Code-Specific\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "StarCoder (15.5B)\n",
      "----\n",
      "CodeT5+ (16B)\n",
      "Enc-Dec\n",
      "SC+NT+Cont.+Match\n",
      "Standard\n",
      "-\n",
      "Code-Specific\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "StarCoder (15.5B)\n",
      "Causal-Dec\n",
      "FIM\n",
      "Multi-query\n",
      "49k\n",
      "BPE\n",
      "-\n",
      "Learned\n",
      "-\n",
      "-\n",
      "40\n",
      "48\n",
      "6144\n",
      "LLaMA-2 (70B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Grouped-query\n",
      "32k\n",
      "BPE\n",
      "Pre-RMS\n",
      "RoPE\n",
      "SwiGLUE\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "PaLM-2\n",
      "-\n",
      "MoD\n",
      "Parallel\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "LLaMA-3.1 (405B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Grouped-query\n",
      "128k\n",
      "BPE\n",
      "Pre-RMS\n",
      "RoPE\n",
      "SwiGLU\n",
      "-\n",
      "126\n",
      "128\n",
      "16384\n",
      "Nemotron-4 (340B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "256k\n",
      "SentencePiece\n",
      "-\n",
      "RoPE\n",
      "ReLU\n",
      "×\n",
      "96\n",
      "96\n",
      "18432\n",
      "DeepSeek (67B)\n",
      "----\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Standard\n",
      "256k\n",
      "SentencePiece\n",
      "-\n",
      "RoPE\n",
      "ReLU\n",
      "×\n",
      "96\n",
      "96\n",
      "18432\n",
      "DeepSeek (67B)\n",
      "Causal-Dec\n",
      "Next Token\n",
      "Grouped-query\n",
      "100k\n",
      "BBPE\n",
      "Pre-RMS\n",
      "RoPE\n",
      "SwiGLU\n",
      "-\n",
      "95\n",
      "64\n",
      "8192\n",
      "DeepSeek-v2 (67B)\n",
      "MoE-Dec\n",
      "Next Token\n",
      "Multi-Head Latent\n",
      "100k\n",
      "BBPE\n",
      "Pre-RMS\n",
      "RoPE\n",
      "SwiGLU\n",
      "-\n",
      "60\n",
      "128\n",
      "5120\n",
      "5.2. Evaluation Datasets and Tasks\n",
      "The evaluation of LLMs is important in gauging their profi-\n",
      "ciency and limitations. This process measures the model’s abil-\n",
      "ity to comprehend, generate, and interact with human language\n",
      "----\n",
      "ity to comprehend, generate, and interact with human language\n",
      "across a spectrum of tasks. Evaluating a language model (LM)\n",
      "is divided into two broader categories: 1) natural language un-\n",
      "derstanding (NLU) and 2) natural language generation (NLG).\n",
      "It is emphasized that tasks in NLU and NLG are softly catego-\n",
      "rized and are often used interchangeably in the literature.\n",
      "Natural Language Understanding: It measures the language\n",
      "understanding capacity of LMs. It encompasses multiple tasks,\n",
      "----\n",
      "understanding capacity of LMs. It encompasses multiple tasks,\n",
      "including sentiment analysis, text classification, natural lan-\n",
      "guage inference (NLI), question answering (QA), common-\n",
      "sense reasoning (CR), mathematical reasoning (MR), reading\n",
      "comprehension (RC), etc.\n",
      "Natural Language Generation: It assesses the language gener-\n",
      "ation capabilities of LLMs by understanding the provided input\n",
      "context. It includes tasks such as summarization, sentence com-\n",
      "----\n",
      "context. It includes tasks such as summarization, sentence com-\n",
      "pletion, machine translation (MT), dialogue generation, etc.\n",
      "Numerous datasets are proposed for each task, evaluating\n",
      "LLMs against different characteristics. To provide an overview\n",
      "of evaluation datasets, we briefly discuss a few famous datasets\n",
      "within each category and offer a comprehensive list of datasets\n",
      "in Table 9. Moreover, we show a detailed overview of the train-\n",
      "----\n",
      "in Table 9. Moreover, we show a detailed overview of the train-\n",
      "ing datasets and evaluation tasks and benchmarks used by vari-\n",
      "ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta-\n",
      "ble 11. We also compare the top-performing LLMs in various\n",
      "NLP tasks in Table 12.\n",
      "5.2.1. Multi-task\n",
      "MMLU [307]: A benchmark that measures the knowledge\n",
      "acquired by models during pretraining and evaluates models in\n",
      "zero-shot and few-shot settings across 57 subjects, testing both\n",
      "----\n",
      "zero-shot and few-shot settings across 57 subjects, testing both\n",
      "world knowledge and problem-solving ability.\n",
      "SuperGLUE [2]: A more challenging and diverse successor\n",
      "to the GLUE [309] benchmark, SuperGLUE includes a variety\n",
      "of language understanding tasks, such as question answering,\n",
      "natural language inference, and co-reference resolution. It is\n",
      "designed to provide a rigorous test of language understanding\n",
      "and requires significant progress in areas like sample-efficient,\n",
      "----\n",
      "and requires significant progress in areas like sample-efficient,\n",
      "transfer, multi-task, and unsupervised or self-supervised learn-\n",
      "ing.\n",
      "BIG-bench [308]: The BIG-bench (Behavior of Intelligent\n",
      "Generative Models Benchmark) is a large-scale benchmark de-\n",
      "signed to test the abilities of LLMs across a wide range of\n",
      "tasks, including reasoning, creativity, ethics, and understanding\n",
      "of specific domains.\n",
      "GLUE [309]: The General Language Understanding Evalua-\n",
      "----\n",
      "of specific domains.\n",
      "GLUE [309]: The General Language Understanding Evalua-\n",
      "tion (GLUE) benchmark is a collection of resources for train-\n",
      "ing, evaluating, and analyzing natural language understanding\n",
      "26\n",
      "----\n",
      "Table 6: Summary of optimization settings used for pre-trained LLMs. The values for weight decay, gradient clipping, and dropout are 0.1, 1.0, and 0.1, respectively,\n",
      "for most of the LLMs.\n",
      "Sequence\n",
      "LR\n",
      "Optimizers\n",
      "Precision\n",
      "Weight\n",
      "Grad\n",
      "Models\n",
      "Batch Size\n",
      "Length\n",
      "LR\n",
      "Warmup\n",
      "Decay\n",
      "AdaFactorAdam AdamWFP16 BF16 Mixed Decay\n",
      "Clip\n",
      "Dropout\n",
      "T5 (11B)\n",
      "211\n",
      "512\n",
      "0.01\n",
      "×\n",
      "inverse square root\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "GPT3 (175B)\n",
      "32K\n",
      "-\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "mT5 (13B)\n",
      "1024\n",
      "1024\n",
      "0.01\n",
      "-\n",
      "inverse square root\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "----\n",
      "32K\n",
      "-\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "mT5 (13B)\n",
      "1024\n",
      "1024\n",
      "0.01\n",
      "-\n",
      "inverse square root\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "PanGu-α (200B)\n",
      "-\n",
      "1024\n",
      "2e-5\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "CPM-2 (198B)\n",
      "1024\n",
      "1024\n",
      "0.001\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "Codex (12B)\n",
      "-\n",
      "-\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "ERNIE 3.0 (12B)\n",
      "6144\n",
      "512\n",
      "1e-4\n",
      "✓\n",
      "linear\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "Jurassic-1 (178B)\n",
      "3.2M\n",
      "2048\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "HyperCLOVA (82B)\n",
      "1024\n",
      "-\n",
      "6e-5\n",
      "-\n",
      "cosine\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "Yuan 1.0 (245B)\n",
      "<10M\n",
      "2048\n",
      "1.6e-4\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "Gopher (280B)\n",
      "3M\n",
      "2048\n",
      "4e-5\n",
      "✓\n",
      "----\n",
      "-\n",
      "Yuan 1.0 (245B)\n",
      "<10M\n",
      "2048\n",
      "1.6e-4\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "Gopher (280B)\n",
      "3M\n",
      "2048\n",
      "4e-5\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "✓\n",
      "-\n",
      "✓\n",
      "-\n",
      "ERNIE 3.0 Titan (260B)\n",
      "-\n",
      "512\n",
      "1e-4\n",
      "✓\n",
      "linear\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "GPT-NeoX-20B\n",
      "1538\n",
      "2048\n",
      "0.97e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "×\n",
      "OPT (175B)\n",
      "2M\n",
      "2048\n",
      "1.2e-4\n",
      "-\n",
      "linear\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "BLOOM (176B)\n",
      "2048\n",
      "2048\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "×\n",
      "Galactica (120B)\n",
      "2M\n",
      "2048\n",
      "7e-6\n",
      "✓\n",
      "linear decay to 10%\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GLaM (1.2T)\n",
      "1M\n",
      "1024\n",
      "0.01\n",
      "-\n",
      "inverse square root\n",
      "✓\n",
      "FP32 + ✓\n",
      "-\n",
      "✓\n",
      "×\n",
      "LaMDA (137B)\n",
      "256K\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "----\n",
      "1M\n",
      "1024\n",
      "0.01\n",
      "-\n",
      "inverse square root\n",
      "✓\n",
      "FP32 + ✓\n",
      "-\n",
      "✓\n",
      "×\n",
      "LaMDA (137B)\n",
      "256K\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "MT-NLG (530B)\n",
      "1920\n",
      "2048\n",
      "5e-5\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "AlphaCode (41B)\n",
      "2048\n",
      "1536+768\n",
      "1e-4\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "Chinchilla (70B)\n",
      "1.5M\n",
      "2048\n",
      "1e-4\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "PaLM (540B)\n",
      "2048\n",
      "2048\n",
      "0.01\n",
      "-\n",
      "inverse square root\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "×\n",
      "AlexaTM (20B)\n",
      "2M\n",
      "1024\n",
      "1e-4\n",
      "-\n",
      "linear decay to 5%\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "✓\n",
      "U-PaLM (540B)\n",
      "32\n",
      "2048\n",
      "1e-4\n",
      "-\n",
      "cosine\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "UL2 (20B)\n",
      "1024\n",
      "1024\n",
      "-\n",
      "-\n",
      "----\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "✓\n",
      "U-PaLM (540B)\n",
      "32\n",
      "2048\n",
      "1e-4\n",
      "-\n",
      "cosine\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "UL2 (20B)\n",
      "1024\n",
      "1024\n",
      "-\n",
      "-\n",
      "inverse square root\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "×\n",
      "-\n",
      "-\n",
      "GLM (130B)\n",
      "4224\n",
      "2048\n",
      "8e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "CodeGen (16B)\n",
      "2M\n",
      "2048\n",
      "5e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "-\n",
      "LLaMA (65B)\n",
      "4M Tokens\n",
      "2048\n",
      "1.5e-4\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "-\n",
      "PanGu-Σ (1.085T)\n",
      "512\n",
      "1024\n",
      "2e-5\n",
      "✓\n",
      "-\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "BloombergGPT (50B)\n",
      "2048\n",
      "2048\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "×\n",
      "Xuan Yuan 2.0 (176B)\n",
      "2048\n",
      "2048\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "CodeT5+ (16B)\n",
      "2048\n",
      "1024\n",
      "2e-4\n",
      "-\n",
      "linear\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "----\n",
      "2048\n",
      "2048\n",
      "6e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "CodeT5+ (16B)\n",
      "2048\n",
      "1024\n",
      "2e-4\n",
      "-\n",
      "linear\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "StarCoder (15.5B)\n",
      "512\n",
      "8k\n",
      "3e-4\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "LLaMA-2 (70B)\n",
      "4M Tokens\n",
      "4k\n",
      "1.5e-4\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "LLaMA-3.1 (405B)\n",
      "16M\n",
      "8192\n",
      "8e-5\n",
      "✓\n",
      "linear+cosine\n",
      "✓\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "Nemotron-4 (340B)\n",
      "2304\n",
      "4096\n",
      "-\n",
      "-\n",
      "linear\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "×\n",
      "DeepSeek (67B)\n",
      "4608\n",
      "4096\n",
      "3.2e-4\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "DeepSeek-v2 (67B)\n",
      "9216\n",
      "4k\n",
      "2.4e-4\n",
      "✓\n",
      "step-decay\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "-\n",
      "----\n",
      "4608\n",
      "4096\n",
      "3.2e-4\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "-\n",
      "DeepSeek-v2 (67B)\n",
      "9216\n",
      "4k\n",
      "2.4e-4\n",
      "✓\n",
      "step-decay\n",
      "✓\n",
      "-\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "-\n",
      "Table 7: Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models, while\n",
      "no model uses weight decay for instruction tuning.\n",
      "Sequence\n",
      "Optimizers\n",
      "Grad\n",
      "Models\n",
      "Batch Size\n",
      "Length\n",
      "LR\n",
      "Warmup\n",
      "LR_Decay\n",
      "AdaFactor\n",
      "Adam\n",
      "AdamW\n",
      "Clip\n",
      "Dropout\n",
      "WebGPT (175B)\n",
      "BC:512, RM:32\n",
      "-\n",
      "6e-5\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "T0 (11B)\n",
      "1024\n",
      "1280\n",
      "1e-3\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "✓\n",
      "----\n",
      "AdamW\n",
      "Clip\n",
      "Dropout\n",
      "WebGPT (175B)\n",
      "BC:512, RM:32\n",
      "-\n",
      "6e-5\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "-\n",
      "T0 (11B)\n",
      "1024\n",
      "1280\n",
      "1e-3\n",
      "-\n",
      "-\n",
      "✓\n",
      "-\n",
      "✓\n",
      "Tk-Instruct (11B)\n",
      "1024\n",
      "-\n",
      "1e-5\n",
      "-\n",
      "constant\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "OPT-IML (175B)\n",
      "128\n",
      "2048\n",
      "5e-5\n",
      "×\n",
      "linear\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Flan-U-PaLM (540B)\n",
      "32\n",
      "-\n",
      "1e-3\n",
      "-\n",
      "constant\n",
      "✓\n",
      "-\n",
      "✓\n",
      "Sparrow (70B)\n",
      "RM: 8+16, RL:16\n",
      "-\n",
      "2e-6\n",
      "✓\n",
      "cosine decay to 10%\n",
      "✓\n",
      "✓\n",
      "×\n",
      "WizardCoder (15B)\n",
      "512\n",
      "2048\n",
      "2e-5\n",
      "✓\n",
      "cosine\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Alpaca (13B)\n",
      "128\n",
      "512\n",
      "1e-5\n",
      "✓\n",
      "cosine\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "×\n",
      "Vicuna (13B)\n",
      "128\n",
      "-2048\n",
      "2e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "-\n",
      "×\n",
      "LIMA (65B)\n",
      "32\n",
      "2048\n",
      "1e-5\n",
      "×\n",
      "linear\n",
      "✓\n",
      "-\n",
      "✓\n",
      "----\n",
      "cosine\n",
      "-\n",
      "-\n",
      "✓\n",
      "✓\n",
      "×\n",
      "Vicuna (13B)\n",
      "128\n",
      "-2048\n",
      "2e-5\n",
      "✓\n",
      "cosine\n",
      "✓\n",
      "-\n",
      "×\n",
      "LIMA (65B)\n",
      "32\n",
      "2048\n",
      "1e-5\n",
      "×\n",
      "linear\n",
      "✓\n",
      "-\n",
      "✓\n",
      "systems. It includes a variety of tasks that test a wide range of\n",
      "linguistic phenomena, making it a comprehensive tool for eval-\n",
      "uating language understanding in AI.\n",
      "5.2.2. Language Understanding\n",
      "WinoGrande [354]: A large-scale dataset inspired by the orig-\n",
      "inal Winograd [357] Schema Challenge tests models on their\n",
      "ability to resolve pronoun ambiguity and encourages the devel-\n",
      "----\n",
      "ability to resolve pronoun ambiguity and encourages the devel-\n",
      "opment of models that understand the broad context in natural\n",
      "language text.\n",
      "CoQA [316]: A conversational question-answering dataset,\n",
      "CoQA challenges models with questions that rely on conver-\n",
      "sation history and require free-form text answers. Its diverse\n",
      "content from seven domains makes it a rigorous test for mod-\n",
      "els’ ability to handle a wide range of topics and conversational\n",
      "contexts.\n",
      "----\n",
      "els’ ability to handle a wide range of topics and conversational\n",
      "contexts.\n",
      "WiC [317]: This dataset assesses a model’s ability to dis-\n",
      "cern word meanings based on context, aiding in tasks related\n",
      "27\n",
      "----\n",
      "Table 8: Details of various well-known pre-training and fine-tuning datasets. Here, alignment means aligning with human preferences.\n",
      "Dataset\n",
      "Type\n",
      "Size/Samples\n",
      "Tasks\n",
      "Source\n",
      "Creation\n",
      "Comments\n",
      "C4 [10]\n",
      "Pretrain\n",
      "806GB\n",
      "-\n",
      "Common Crawl\n",
      "Automated\n",
      "A clean, multilingual dataset with billions\n",
      "of tokens\n",
      "mC4 [11]\n",
      "Pretrain\n",
      "38.49TB\n",
      "-\n",
      "Common Crawl\n",
      "Automated\n",
      "A multilingual extension of the C4\n",
      "dataset, mC4 identifies over 100 lan-\n",
      "guages using cld3 from 71 monthly web\n",
      "scrapes of Common Crawl.\n",
      "PILE [301]\n",
      "Pretrain\n",
      "----\n",
      "guages using cld3 from 71 monthly web\n",
      "scrapes of Common Crawl.\n",
      "PILE [301]\n",
      "Pretrain\n",
      "825GB\n",
      "-\n",
      "Common Crawl, PubMed Central,\n",
      "OpenWebText2, ArXiv, GitHub,\n",
      "Books3, and others\n",
      "Automated\n",
      "A massive dataset comprised of 22 con-\n",
      "stituent sub-datasets\n",
      "ROOTs [302]\n",
      "Pretrain\n",
      "1.61TB\n",
      "-\n",
      "498 Hugging Face datasets\n",
      "Automated\n",
      "46 natural and 13 programming lan-\n",
      "guages\n",
      "MassiveText [116]\n",
      "Pretrain\n",
      "10.5TB\n",
      "-\n",
      "MassiveWeb, Books, News,\n",
      "Wikipedia, Github, C4\n",
      "Automated\n",
      "99% of the data is in English\n",
      "Wikipedia [303]\n",
      "Pretrain\n",
      "-\n",
      "-\n",
      "----\n",
      "Wikipedia, Github, C4\n",
      "Automated\n",
      "99% of the data is in English\n",
      "Wikipedia [303]\n",
      "Pretrain\n",
      "-\n",
      "-\n",
      "Wikipedia\n",
      "Automated\n",
      "Dump of wikipedia\n",
      "RedPajama [304]\n",
      "Pretrain\n",
      "5TB\n",
      "-\n",
      "CommonCrawl, C4, Wikipedia,\n",
      "Github, Books, StackExchange\n",
      "Automated\n",
      "Open-source replica of LLaMA dataset\n",
      "PushShift.io Reddit\n",
      "Pretrain\n",
      "21.1GB\n",
      "-\n",
      "Reddit\n",
      "Automated\n",
      "Submissions and comments on Reddit\n",
      "from 2005 to 2019\n",
      "BigPython [140]\n",
      "Pretrain\n",
      "5.5TB\n",
      "Coding\n",
      "GitHub\n",
      "Automated\n",
      "-\n",
      "Pool of Prompt (P3) [17]\n",
      "Instructions\n",
      "12M\n",
      "62\n",
      "PromptSource\n",
      "Manual\n",
      "----\n",
      "5.5TB\n",
      "Coding\n",
      "GitHub\n",
      "Automated\n",
      "-\n",
      "Pool of Prompt (P3) [17]\n",
      "Instructions\n",
      "12M\n",
      "62\n",
      "PromptSource\n",
      "Manual\n",
      "A Subset of PromptSource, created from\n",
      "177 datasets including summarization,\n",
      "QA, classification, etc.\n",
      "xP3 [154]\n",
      "Instructions\n",
      "81M\n",
      "71\n",
      "P3+Multilingual datasets\n",
      "Manual\n",
      "Extending P3 to total 46 languages\n",
      "Super-NaturalInstructions (SNI) [18]\n",
      "Instructions\n",
      "12.4M\n",
      "1616\n",
      "Multiple datasets\n",
      "Manual\n",
      "Extending P3 with additional multi-\n",
      "lingual datasets, total 46 languages\n",
      "Flan [16]\n",
      "Instructions\n",
      "15M\n",
      "1836\n",
      "----\n",
      "lingual datasets, total 46 languages\n",
      "Flan [16]\n",
      "Instructions\n",
      "15M\n",
      "1836\n",
      "Muffin+T0-SF+NIV2\n",
      "Manual\n",
      "Total 60 languages\n",
      "OPT-IML [97]\n",
      "Instructions\n",
      "18.1M\n",
      "1667\n",
      "-\n",
      "Manual\n",
      "-\n",
      "Self-Instruct [19]\n",
      "Instructions\n",
      "82k\n",
      "175\n",
      "-\n",
      "Automated\n",
      "Generated 52k instructions with 82k sam-\n",
      "ples from 175 seed tasks using GPT-3\n",
      "Alpaca [158]\n",
      "Instructions\n",
      "52k\n",
      "-\n",
      "-\n",
      "Automated\n",
      "Employed self-instruct method to gener-\n",
      "ate data from text-davinci-003\n",
      "Vicuna [159]\n",
      "Instructions\n",
      "125k\n",
      "-\n",
      "ShareGPT\n",
      "Automated\n",
      "Conversations\n",
      "shared\n",
      "by\n",
      "users\n",
      "on\n",
      "----\n",
      "Vicuna [159]\n",
      "Instructions\n",
      "125k\n",
      "-\n",
      "ShareGPT\n",
      "Automated\n",
      "Conversations\n",
      "shared\n",
      "by\n",
      "users\n",
      "on\n",
      "ShareGPT using public APIs\n",
      "LLaMA-GPT-4 [160]\n",
      "Instructions\n",
      "52k\n",
      "-\n",
      "Alpaca\n",
      "Automated\n",
      "Recreated Alpaca dataset with GPT-4 in\n",
      "English and Chinese\n",
      "Unnatural Instructions [305]\n",
      "Instructions\n",
      "68k\n",
      "-\n",
      "15-Seeds (SNI)\n",
      "Automated\n",
      "-\n",
      "LIMA [185]\n",
      "Instructions\n",
      "1k\n",
      "-\n",
      "Multiple datasets\n",
      "Manual\n",
      "Carefully created samples to test perfor-\n",
      "mance with fine-tuning on less data\n",
      "Anthropic-HH-RLHF [306]\n",
      "Alignment\n",
      "142k\n",
      "-\n",
      "-\n",
      "Manual\n",
      "----\n",
      "mance with fine-tuning on less data\n",
      "Anthropic-HH-RLHF [306]\n",
      "Alignment\n",
      "142k\n",
      "-\n",
      "-\n",
      "Manual\n",
      "Anthropic-HH-RLHF-2 [178]\n",
      "Alignment\n",
      "39k\n",
      "-\n",
      "-\n",
      "Manual\n",
      "to Word Sense Disambiguation.\n",
      "Wikitext103 [318]:\n",
      "With over 100 million tokens from\n",
      "Wikipedia’s top articles, this dataset is a rich resource for tasks\n",
      "that require understanding long-term dependencies, such as lan-\n",
      "guage modeling and translation.\n",
      "PG19 [319]: This is a digital library of diverse books from\n",
      "----\n",
      "guage modeling and translation.\n",
      "PG19 [319]: This is a digital library of diverse books from\n",
      "Project Gutenberg. It is specifically designed to facilitate re-\n",
      "search in unsupervised learning and language modeling, with a\n",
      "special focus on long-form content.\n",
      "C4 [10]: A clean, multilingual dataset, C4 offers billions of to-\n",
      "kens from web-crawled data. It is a comprehensive resource for\n",
      "training advanced Transformer models on various languages.\n",
      "LCQMC [320]: The Large-scale Chinese Question Matching\n",
      "----\n",
      "LCQMC [320]: The Large-scale Chinese Question Matching\n",
      "Corpus (LCQMC) is a dataset for evaluating the performance\n",
      "of models in semantic matching tasks. It contains pairs of ques-\n",
      "tions in Chinese and their matching status, making it a valuable\n",
      "resource for research in Chinese language understanding.\n",
      "5.2.3. Story Cloze and Sentence Completion\n",
      "StoryCloze [334]: It introduces a new “StoryCloze Test”, a\n",
      "commonsense reasoning framework for evaluating story under-\n",
      "----\n",
      "commonsense reasoning framework for evaluating story under-\n",
      "standing, generation, and script learning. It considers a model’s\n",
      "ability to understand and generate coherent and sensible stories.\n",
      "LAMBADA [335]: This dataset evaluates contextual text un-\n",
      "derstanding through a word prediction task. Models must pre-\n",
      "dict the last word of a passage, which is easy for humans when\n",
      "given the whole passage, but not when given only the last sen-\n",
      "tence.\n",
      "5.2.4. Physical Knowledge and World Understanding\n",
      "----\n",
      "tence.\n",
      "5.2.4. Physical Knowledge and World Understanding\n",
      "PIQA [340]: A dataset that probes the physical knowledge of\n",
      "models, aiming to understand how well they are learning about\n",
      "the real world.\n",
      "TriviaQA [341]: A dataset that tests models on reading com-\n",
      "prehension and open domain question answering (QA) tasks,\n",
      "with a focus on Information Retrieval (IR)-style QA.\n",
      "ARC [342]: A larger version of the ARC-Challenge, this\n",
      "dataset contains both easy and challenging grade-school level,\n",
      "----\n",
      "dataset contains both easy and challenging grade-school level,\n",
      "multiple-choice science questions. It is a comprehensive test of\n",
      "a model’s ability to understand and answer complex questions.\n",
      "ARC-Easy [342]:\n",
      "A subset of the ARC dataset, ARC-\n",
      "Easy, contains questions that are answered correctly by either\n",
      "a retrieval-based algorithm or a word co-occurrence algorithm.\n",
      "28\n",
      "----\n",
      "Table 9: Categorized evaluation datasets used in evaluating LLMs.\n",
      "Type\n",
      "Datasets/Benchmarks\n",
      "Multi-Task\n",
      "MMLU [307], SuperGLUE [2], BIG-bench [308], GLUE [309], BBH [308], CUGE [310], Zero-\n",
      "CLUE [311], FewCLUE [312], Blended Skill Talk [313], HELM [314], KLUE-STS [315]\n",
      "Language Understanding\n",
      "CoQA [316], WiC [317], Wikitext103 [318], PG19 [319], LCQMC [320], QQP [321], WinoGender [322],\n",
      "CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\n",
      "----\n",
      "CB [323], FinRE [324], SanWen [325], AFQMC [311], BQ Corpus [326], CNSS [327], CKBQA 13 [328],\n",
      "CLUENER [311], Weibo [329], AQuA [330], OntoNotes [331], HeadQA [332], Twitter Dataset [333]\n",
      "Story Cloze and\n",
      "Sentence Completion\n",
      "StoryCloze [334], LAMBADA [335], LCSTS [336], AdGen [337], E2E [338], CHID [339], CHID-\n",
      "FC [312]\n",
      "Physical Knowledge and\n",
      "World Understanding\n",
      "PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\n",
      "----\n",
      "PIQA [340], TriviaQA [341], ARC [342], ARC-Easy [342], ARC-Challenge [342], PROST [343], Open-\n",
      "BookQA [344], WebNLG [345], DogWhistle Insider & Outsider [346]\n",
      "Contextual Language\n",
      "Understanding\n",
      "RACE [347], RACE-Middle [347], RACE-High [347], QuAC [348], StrategyQA [349], Quiz Bowl [350],\n",
      "cMedQA [351],cMedQA2 [352], MATINF-QA [353]\n",
      "Commonsense Reasoning\n",
      "WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\n",
      "----\n",
      "WinoGrande [354], HellaSwag [355], COPA [356], WSC [357], CSQA [358], SIQA [359], C3 [360],\n",
      "CLUEWSC2020 [311], CLUEWSC [311], CLUEWSC-FC [312], ReCoRD [361]\n",
      "Reading Comprehension\n",
      "SQuAD [362], BoolQ [363], SQUADv2 [364], DROP [365], RTE [366], WebQA [367], CMRC2017 [368],\n",
      "CMRC2018 [369], CMRC2019 [370], COTE-BD [371], COTE-DP [371], COTE-MFW [371], Mul-\n",
      "tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\n",
      "----\n",
      "tiRC [372], Natural Questions [373], CNSE [327], DRCD [374], DuReader [375], Dureaderrobust [376],\n",
      "DuReader-QG [375], SciQ [377], Sogou-log [378], Dureaderrobust-QG [376], QA4MRE [379], KorQuAD\n",
      "1.0 [380], CAIL2018-Task1 & Task2 [381]\n",
      "Mathematical Reasoning\n",
      "MATH [382], Math23k [383], GSM8K [384], MathQA [385], MGSM [386], MultiArith [387], AS-\n",
      "Div [388], MAWPS [389], SVAMP [390]\n",
      "Problem Solving\n",
      "HumanEval [141], DS-1000 [391], MBPP [392], APPS [382], CodeContests [142]\n",
      "Natural Language Inference\n",
      "----\n",
      "Natural Language Inference\n",
      "& Logical Reasoning\n",
      "ANLI [393], MNLI-m [394], MNLI-mm [394],QNLI [362], WNLI [357], OCNLI [311], CMNLI [311],\n",
      "ANLI R1 [393], ANLI R2 [393], ANLI R3 [393], HANS [395], OCNLI-FC [312], LogiQA [396], Strate-\n",
      "gyQA [349]\n",
      "Cross-Lingual Understanding\n",
      "MLQA [397], XNLI [398], PAWS-X [399], XSum [400], XCOPA [401], XWinograd [402], TyDiQA-\n",
      "GoldP [403], MLSum [404]\n",
      "Truthfulness and Fact Checking\n",
      "TruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\n",
      "----\n",
      "Truthfulness and Fact Checking\n",
      "TruthfulQA [405], MultiFC [406], Fact Checking on Fever [407]\n",
      "Biases and Ethics in AI\n",
      "ETHOS [408], StereoSet [409], BBQ [410], Winobias [411], CrowS-Pairs [412]\n",
      "Toxicity\n",
      "RealToxicityPrompts [413], CivilComments toxicity classification [414]\n",
      "Language Translation\n",
      "WMT [415], WMT20 [416], WMT20-enzh [416], EPRSTMT [312], CCPM [417]\n",
      "Scientific Knowledge\n",
      "AminoProbe [148], BioLAMA [148], Chemical Reactions [148], Galaxy Clusters [148], Mineral\n",
      "Groups [148]\n",
      "Dialogue\n",
      "----\n",
      "Groups [148]\n",
      "Dialogue\n",
      "Wizard of Wikipedia [418], Empathetic Dialogues [419], DPC-generated [96] dialogues, ConvAI2 [420],\n",
      "KdConv [421]\n",
      "Topic Classification\n",
      "TNEWS-FC [312], YNAT [315], KLUE-TC [315], CSL [311], CSL-FC [312], IFLYTEK [422]\n",
      "It is a great starting point for models beginning to explore ad-\n",
      "vanced question-answering.\n",
      "ARC-Challenge [342]:\n",
      "A rigorous question-answering\n",
      "dataset, ARC-Challenge includes complex, grade-school level\n",
      "----\n",
      "A rigorous question-answering\n",
      "dataset, ARC-Challenge includes complex, grade-school level\n",
      "questions that demand reasoning beyond simple retrieval, test-\n",
      "ing the true comprehension capabilities of models.\n",
      "5.2.5. Contextual Language Understanding\n",
      "RACE [347]: The RACE dataset is a reading comprehension\n",
      "dataset collected from English examinations in China, which\n",
      "benchmarks AI models for understanding and answering ques-\n",
      "tions on long and complex passages, simulating the challenge\n",
      "----\n",
      "tions on long and complex passages, simulating the challenge\n",
      "of a real-world examination.\n",
      "RACE-Middle [347]: Another subset of the RACE [347]\n",
      "dataset, RACE-Middle, contains middle school-level English\n",
      "exam questions. It offers a slightly less challenging but academ-\n",
      "ically oriented evaluation of a model’s comprehension skills.\n",
      "RACE-High [347]: A subset of the RACE [347] dataset,\n",
      "RACE-High consists of high school-level English exam ques-\n",
      "----\n",
      "RACE-High consists of high school-level English exam ques-\n",
      "tions. It is designed to evaluate the comprehension ability of\n",
      "models in a more academic and challenging context.\n",
      "QuAC [348]: This dataset simulates an information-seeking\n",
      "dialog between students and teachers using hidden Wikipedia\n",
      "text. It introduces unique challenges not found in machine com-\n",
      "prehension datasets, making it a valuable resource for advanc-\n",
      "ing dialog systems.\n",
      "5.2.6. Commonsense Reasoning\n",
      "----\n",
      "ing dialog systems.\n",
      "5.2.6. Commonsense Reasoning\n",
      "HellaSwag [355]: A dataset that challenges models to pick the\n",
      "best ending to a context uses Adversarial Filtering to create a\n",
      "‘Goldilocks’ zone of complexity, where generated text is absurd\n",
      "to humans but often misclassified by models.\n",
      "COPA [401]: This dataset evaluates a model’s progress in\n",
      "open-domain commonsense causal reasoning. Each question\n",
      "comprises a premise and two alternatives, and the model must\n",
      "----\n",
      "comprises a premise and two alternatives, and the model must\n",
      "select the more plausible alternative, testing a model’s ability to\n",
      "understand and reason about cause and effect.\n",
      "WSC [357]: The Winograd Schema Challenge (WSC) is a\n",
      "29\n",
      "----\n",
      "Table 10: An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here, “QA” is question-answering, “Clf” is classification, “NLI”\n",
      "is natural language inference, “MT” is machine translation, “RC” is reading comprehension, “CR” is commonsense reasoning, “MR” is mathematical reasoning,\n",
      "“Mem.” is memorization.\n",
      "Benchmark\n",
      "Models\n",
      "Training Dataset\n",
      "BIG-\n",
      "bench\n",
      "MMLU\n",
      "Super\n",
      "GLUE\n",
      "QA\n",
      "Clf\n",
      "NLI\n",
      "MT\n",
      "Cloze/\n",
      "Completion\n",
      "RC\n",
      "CR\n",
      "MR\n",
      "Coding\n",
      "Truthful/\n",
      "Bias/\n",
      "Toxicity/\n",
      "Mem.\n",
      "T5\n",
      "C4 [10]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "----\n",
      "QA\n",
      "Clf\n",
      "NLI\n",
      "MT\n",
      "Cloze/\n",
      "Completion\n",
      "RC\n",
      "CR\n",
      "MR\n",
      "Coding\n",
      "Truthful/\n",
      "Bias/\n",
      "Toxicity/\n",
      "Mem.\n",
      "T5\n",
      "C4 [10]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GPT-3\n",
      "Common Crawl, WebText, Books Cor-\n",
      "pora, Wikipedia\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "mT5\n",
      "mC4 [11]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "PanGu-α\n",
      "1.1TB Chinese Text Corpus\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "CPM-2\n",
      "WuDaoCorpus [109]\n",
      "✓\n",
      "✓\n",
      "Codex\n",
      "54 million public repositories from Github\n",
      "✓\n",
      "ERNIE-3.0\n",
      "Chinese text corpora, Baidu Search, Web\n",
      "text, QA-long, QA-short, Poetry and Cou-\n",
      "plet Domain-specific data from medical,\n",
      "law, and financial area Baidu knowledge\n",
      "----\n",
      "plet Domain-specific data from medical,\n",
      "law, and financial area Baidu knowledge\n",
      "graph with more than 50 million facts\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Jurassic-1\n",
      "Wikipedia, OWT, Books, C4, Pile [301],\n",
      "arXiv, GitHub\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "HyperCLOVA\n",
      "Korean blogs, Community sites, News,\n",
      "KiN Korean Wikipedia, Wikipedia (En-\n",
      "glish and Japanese), Modu-Corpus: Mes-\n",
      "senger, News, Spoken and written lan-\n",
      "guage corpus, Web corpus\n",
      "✓\n",
      "Yuan 1.0\n",
      "Common Crawl, SogouT, Sogou News,\n",
      "Baidu Baike, Wikipedia, Books\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Gopher\n",
      "----\n",
      "✓\n",
      "Yuan 1.0\n",
      "Common Crawl, SogouT, Sogou News,\n",
      "Baidu Baike, Wikipedia, Books\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Gopher\n",
      "subsets of MassiveWeb Books, C4, News,\n",
      "GitHub and Wikipedia samples from Mas-\n",
      "siveText\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "ERNIE-3.0 TITAN\n",
      "Same as ERNIE 3.0 and ERNIE 3.0 ad-\n",
      "versarial dataset, ERNIE 3.0 controllable\n",
      "dataset\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GPT-NeoX-20B\n",
      "Pile [301]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "OPT\n",
      "RoBERTa [299], Pile [301], PushShift.io\n",
      "Reddit [423]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "BLOOM\n",
      "ROOTs [13]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Galactica\n",
      "arXiv, PMC, Semantic Scholar, Wikipedia,\n",
      "----\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "BLOOM\n",
      "ROOTs [13]\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Galactica\n",
      "arXiv, PMC, Semantic Scholar, Wikipedia,\n",
      "StackExchange, LibreText, Open Text-\n",
      "books, RefSeq Genome, OEIS, LIPID\n",
      "MAPS, NASAExoplanet, Common Crawl,\n",
      "ScientificCC, AcademicCC, GitHub repos-\n",
      "itories Khan Problems, GSM8K, OneS-\n",
      "mallStep\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GLaM\n",
      "Filtered Webpages, Social media conversa-\n",
      "tions Wikipedia, Forums, Books, News\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "LaMDA\n",
      "Infiniset : Public documents, Dialogs, Ut-\n",
      "terances\n",
      "✓\n",
      "MT-NLG\n",
      "Two snapshots of Common Crawl and\n",
      "----\n",
      "Infiniset : Public documents, Dialogs, Ut-\n",
      "terances\n",
      "✓\n",
      "MT-NLG\n",
      "Two snapshots of Common Crawl and\n",
      "Books3, OpenWebText2, Stack Exchange,\n",
      "PubMed Abstracts,\n",
      "Wikipedia,\n",
      "PG-19\n",
      "[242], BookCorpus2, NIH ExPorter, Pile,\n",
      "CC-Stories, RealNews\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "AlphaCode\n",
      "Selected GitHub repositories, CodeCon-\n",
      "tests: Codeforces, Description2Code, Co-\n",
      "deNet\n",
      "✓\n",
      "Chinchilla\n",
      "MassiveWeb,\n",
      "MassiveText Books,\n",
      "C4,\n",
      "News, GitHub, Wikipedia\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "PaLM\n",
      "webpages, books, Wikipedia, news, arti-\n",
      "----\n",
      "C4,\n",
      "News, GitHub, Wikipedia\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "PaLM\n",
      "webpages, books, Wikipedia, news, arti-\n",
      "cles, source code, social media conversa-\n",
      "tions\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "AlexaTM\n",
      "Wikipedia, mC4\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "U-PaLM\n",
      "Same as PaLM\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "UL2\n",
      "-\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "GLM-130B\n",
      "-\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "CodeGen\n",
      "Pile, BigQuery, BigPython\n",
      "✓\n",
      "LLaMA\n",
      "CommonCrawl, C4, Github, Wikipedia,\n",
      "Books, arXiv, StackExchange\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "PanGu-Σ\n",
      "WuDaoCorpora, CLUE, Pile, C4, Python\n",
      "code\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "BloombergGPT\n",
      "inPile, Pile, C4, Wikipedia\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "CodeT5+\n",
      "----\n",
      "code\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "BloombergGPT\n",
      "inPile, Pile, C4, Wikipedia\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "CodeT5+\n",
      "CodeSearchNet, Github Code\n",
      "✓\n",
      "✓\n",
      "StarCoder\n",
      "The Stack v1.2\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "LLaMA-2\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "PaLM-2\n",
      "Web documents, Code, Books, Maths,\n",
      "Conversation\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "30\n",
      "----\n",
      "Table 11: An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. “SNI” is a short of Super-NaturalInsturctions.\n",
      "Models\n",
      "Training Dataset\n",
      "BIG-\n",
      "bench\n",
      "MMLU\n",
      "BBH\n",
      "RAFT\n",
      "FLAN\n",
      "SNI\n",
      "PromptSource\n",
      "TyDiQA\n",
      "HumanEval\n",
      "MBPP\n",
      "Truthful/\n",
      "Bias/\n",
      "Toxicity\n",
      "T0\n",
      "Pool of Prompts\n",
      "✓\n",
      "WebGPT\n",
      "ELI5\n",
      "[424],\n",
      "ELI5\n",
      "fact-\n",
      "check\n",
      "[166],\n",
      "TriviaQA\n",
      "[341],\n",
      "ARC-Challenge\n",
      "[342],\n",
      "ARC-\n",
      "Easy\n",
      "[342],\n",
      "Hand-written\n",
      "data,\n",
      "Demonstrations of humans, Com-\n",
      "parisons between model-generated\n",
      "answers\n",
      "✓\n",
      "Tk-INSTRUCT\n",
      "SNI [18]\n",
      "✓\n",
      "mT0\n",
      "----\n",
      "parisons between model-generated\n",
      "answers\n",
      "✓\n",
      "Tk-INSTRUCT\n",
      "SNI [18]\n",
      "✓\n",
      "mT0\n",
      "xP3 [154]\n",
      "OPT-IML\n",
      "PromptSource [17], FLAN [16],\n",
      "SNI\n",
      "[425],\n",
      "UnifiedSKG\n",
      "[426],\n",
      "CrossFit\n",
      "[427],\n",
      "ExMix\n",
      "[428],\n",
      "T5 [10], Reasoning\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "Flan\n",
      "Muffin, T0-SF, NIv2, CoT\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "WizardCoder\n",
      "Code Alpaca\n",
      "✓\n",
      "✓\n",
      "reading comprehension task in which a system must resolve\n",
      "references in a text, often requiring world knowledge and rea-\n",
      "soning about the text.\n",
      "CSQA [358]: The CommonsenseQA is a question-answering\n",
      "----\n",
      "soning about the text.\n",
      "CSQA [358]: The CommonsenseQA is a question-answering\n",
      "dataset that requires commonsense knowledge to evaluate the\n",
      "ability of AI models to understand and answer questions.\n",
      "5.2.7. Reading Comprehension\n",
      "BoolQ [363]: A dataset derived from Google search queries,\n",
      "BoolQ challenges models to answer binary (yes/no) questions.\n",
      "The questions are naturally occurring and are paired with a\n",
      "paragraph from a Wikipedia article containing the answer. It\n",
      "----\n",
      "paragraph from a Wikipedia article containing the answer. It\n",
      "is a test of reading comprehension and reasoning.\n",
      "SQUADv2 [364]: The Stanford Question Answering Dataset\n",
      "(SQuAD) [362] is a collection of questions posed by crowd\n",
      "workers on a set of Wikipedia articles, where the answer to ev-\n",
      "ery question is a segment of text from the corresponding reading\n",
      "passage. SQuADv2 combines the original SQuAD1.1 dataset\n",
      "with over 50,000 unanswerable questions. The aim is to evalu-\n",
      "----\n",
      "with over 50,000 unanswerable questions. The aim is to evalu-\n",
      "ate a model’s ability to understand and answer questions based\n",
      "on a given context and to determine when a question is unan-\n",
      "swerable.\n",
      "DROP [365]: DROP, or Discrete Reasoning Over the con-\n",
      "tent of Paragraphs, is designed to test a model’s ability to un-\n",
      "derstand a wide variety of reading phenomena. It encourages\n",
      "comprehensive and reliable evaluation of reading comprehen-\n",
      "sion capabilities.\n",
      "RTE [366]:\n",
      "----\n",
      "comprehensive and reliable evaluation of reading comprehen-\n",
      "sion capabilities.\n",
      "RTE [366]:\n",
      "The Recognizing Textual Entailment (RTE)\n",
      "datasets come from a series of annual competitions on textual\n",
      "entailment, predicting whether a given sentence logically fol-\n",
      "lows from another and evaluating a model’s understanding of\n",
      "logical relationships in a text.\n",
      "WebQA [367]: A dataset for open-domain question answering,\n",
      "WebQA offers a large collection of web-based question-answer\n",
      "----\n",
      "WebQA offers a large collection of web-based question-answer\n",
      "pairs. It is designed to assess the ability of AI models to under-\n",
      "stand and answer questions based on web content.\n",
      "CMRC2018 [369]: This dataset is a test of Chinese language\n",
      "models’ ability to reason comprehensively and is designed with\n",
      "a challenging span-extraction format that pushes the boundaries\n",
      "of machine performance.\n",
      "5.2.8. Mathematical Reasoning\n",
      "MATH [382]: This dataset is a platform for evaluating the\n",
      "----\n",
      "5.2.8. Mathematical Reasoning\n",
      "MATH [382]: This dataset is a platform for evaluating the\n",
      "mathematical problem-solving abilities of AI models. It con-\n",
      "tains a diverse set of math problems, ranging from arithmetic\n",
      "to calculus, and is designed to test the model’s ability to under-\n",
      "stand and solve complex mathematical problems.\n",
      "Math23k [383]: This one challenges a model’s ability to un-\n",
      "derstand and solve mathematical word problems. It contains\n",
      "----\n",
      "derstand and solve mathematical word problems. It contains\n",
      "23,000 Chinese arithmetic word problems that require models\n",
      "to perform reasoning and computation based on the problem\n",
      "description.\n",
      "GSM8K [384]: A dataset of diverse grade school math word\n",
      "problems, testing a model’s ability to perform multi-step math-\n",
      "ematical reasoning.\n",
      "5.2.9. Problem Solving and Logical Reasoning\n",
      "ANLI [393]: A large-scale dataset designed to test the robust-\n",
      "----\n",
      "ANLI [393]: A large-scale dataset designed to test the robust-\n",
      "ness of machine learning models in Natural Language Inference\n",
      "(NLI) is created through an iterative, adversarial process where\n",
      "humans try to generate examples that models cannot correctly\n",
      "classify.\n",
      "HumanEval [141]: A dataset for evaluating the problem-\n",
      "solving ability of AI models, which includes a diverse set of\n",
      "tasks that require various cognitive abilities, making it a com-\n",
      "----\n",
      "tasks that require various cognitive abilities, making it a com-\n",
      "prehensive tool for assessing general intelligence in AI.\n",
      "StrategyQA [349]: A question-answering dataset that re-\n",
      "quires reasoning over multiple pieces of evidence to evaluate\n",
      "the strategic reasoning ability of AI models, pushing the bound-\n",
      "aries of what machines can understand and answer.\n",
      "5.2.10. Cross-Lingual Understanding\n",
      "XNLI [398]: A cross-lingual benchmark, XNLI extends the\n",
      "----\n",
      "5.2.10. Cross-Lingual Understanding\n",
      "XNLI [398]: A cross-lingual benchmark, XNLI extends the\n",
      "MultiNLI [429] corpus to 15 languages, including low-resource\n",
      "ones like Urdu. It tests models on cross-lingual sentence under-\n",
      "standing, with 112,500 annotated pairs across three categories:\n",
      "entailment, contradiction, and neutral.\n",
      "PAWS-X [399]: PAWS-X, or Cross-lingual Paraphrase Adver-\n",
      "saries from Word Scrambling, is a multilingual version of the\n",
      "31\n",
      "----\n",
      "PAWS [430] dataset for paraphrase identification. It includes\n",
      "examples in seven languages and is designed to evaluate the\n",
      "performance of cross-lingual paraphrase identification models.\n",
      "5.2.11. Truthfulness\n",
      "Truthful-QA [405]: A unique benchmark that measures a\n",
      "language model’s truthfulness when generating answers. The\n",
      "dataset includes questions across various categories like health,\n",
      "law, and politics, some designed to test the model against com-\n",
      "mon human misconceptions.\n",
      "----\n",
      "law, and politics, some designed to test the model against com-\n",
      "mon human misconceptions.\n",
      "5.2.12. Biases and Ethics in AI\n",
      "ETHOS [408]: ETHOS is a hate speech detection dataset\n",
      "built from YouTube and Reddit comments. It is a tool in the\n",
      "fight against online hate speech, offering binary and multi-label\n",
      "variants for robust content moderation.\n",
      "StereoSet [409]: StereoSet is a comprehensive dataset de-\n",
      "signed to measure and evaluate the presence of stereotypical\n",
      "----\n",
      "signed to measure and evaluate the presence of stereotypical\n",
      "biases in language models. It focuses on four key domains:\n",
      "gender, profession, race, and religion. Contrasting stereotypi-\n",
      "cal bias against language modeling ability provides a valuable\n",
      "tool for understanding and mitigating biases in large language\n",
      "models.\n",
      "6. Applications\n",
      "Applying Large Language Models (LLMs) to a variety of\n",
      "downstream tasks has become a popular trend in both AI-\n",
      "----\n",
      "downstream tasks has become a popular trend in both AI-\n",
      "related research communities and industries, with many emerg-\n",
      "ing uses being discovered and explored daily. LLMs, which are\n",
      "capable of understanding and generating human-like text, have\n",
      "found meaningful applications across a variety of fields. This\n",
      "section provides an overview of LLM applications in medicine,\n",
      "education, science, mathematics, law, finance, robotics, and\n",
      "coding. While each of these domains pose different challenges,\n",
      "----\n",
      "coding. While each of these domains pose different challenges,\n",
      "LLMs open up opportunities to make significant contributions\n",
      "to these domains through their generalizability.\n",
      "General Purpose:\n",
      "LLMs are being widely considered as\n",
      "general-purpose tools for a wide variety of tasks [431]. This\n",
      "is due to their inherent ability to understand, generate, and\n",
      "manipulate human-like text in a contextually relevant man-\n",
      "ner. This allows them to perform tasks ranging from simple\n",
      "----\n",
      "ner. This allows them to perform tasks ranging from simple\n",
      "language translation and question-answering to more complex\n",
      "tasks like summarization, text generation, and even program-\n",
      "ming help [432]. The utility of LLMs is further enhanced by\n",
      "their ability to adapt to the specific style and tone of the text\n",
      "they are processing, making the outputs more user-friendly and\n",
      "context-aware. In everyday applications, LLMs can be used\n",
      "as personal assistants, helping users draft emails or schedule\n",
      "----\n",
      "as personal assistants, helping users draft emails or schedule\n",
      "appointments [433]; they can also be deployed in customer ser-\n",
      "vice to handle common questions or applied to generate content\n",
      "for digital platforms like websites by creating human-like text\n",
      "based on given prompts [434]. Moreover, LLMs play a cru-\n",
      "cial role in data analysis, where they can filter large volumes of\n",
      "text data, summarize key points, and find patterns that would\n",
      "----\n",
      "text data, summarize key points, and find patterns that would\n",
      "take humans much longer to identify [435]. Despite their wide-\n",
      "ranging applications, it is essential to remember that LLMs,\n",
      "similar to any AI system, are only as good as the data they have\n",
      "been trained on.\n",
      "Medicine: The application of LLMs in the field of medicine is\n",
      "reshaping healthcare delivery and research. For example, LLMs\n",
      "are increasingly used in clinical decision support systems to\n",
      "----\n",
      "are increasingly used in clinical decision support systems to\n",
      "provide physicians with evidence-based treatment recommen-\n",
      "dations [436, 437, 438]. By analyzing patient data and medical\n",
      "literature, they can help identify potential diagnoses, suggest\n",
      "appropriate tests, and recommend optimal treatment strategies.\n",
      "Moreover, LLMs can also enhance patient interactions with\n",
      "healthcare systems; e.g., they can be used in chatbot applica-\n",
      "tions [439, 440, 441] to answer patient queries about symptoms\n",
      "----\n",
      "tions [439, 440, 441] to answer patient queries about symptoms\n",
      "or medications, schedule appointments, and even provide es-\n",
      "sential health advice. For medical research, LLMs are used to\n",
      "extract and filter information from a considerable amount of\n",
      "medical literature, identify relevant studies, summarize find-\n",
      "ings, and even predict future research trends [442, 443, 444].\n",
      "For medical education, LLMs can help create training mate-\n",
      "rials, generate exam questions, provide detailed explanations\n",
      "----\n",
      "rials, generate exam questions, provide detailed explanations\n",
      "of complex medical topics, and offer personalized feedback to\n",
      "students [445, 446, 447, 448]. They can also simulate patient\n",
      "interactions, enabling students to practice and improve their\n",
      "clinical skills. At a broader level, LLMs can assist in public\n",
      "health initiatives by analyzing media data to detect disease out-\n",
      "breaks, monitor public sentiment towards health policies, and\n",
      "disseminate health information in a clear and understandable\n",
      "----\n",
      "disseminate health information in a clear and understandable\n",
      "manner [449]. LLMs can be employed to support public health\n",
      "initiatives, addressing related issues such as data privacy, the\n",
      "necessity for explainability, and the potential risk of propagat-\n",
      "ing biases [450, 451].\n",
      "Education: The integration of LLMs into the educational sec-\n",
      "tor offers opportunities to enhance learning experiences, teacher\n",
      "support, and educational content development. For students, by\n",
      "----\n",
      "support, and educational content development. For students, by\n",
      "analyzing their learning styles, performance, and preferences,\n",
      "LLMs can provide customized study materials and practice\n",
      "questions to develop personalized learning experiences [452].\n",
      "For teachers, LLMs can help to create lesson plans and grade\n",
      "assignments and generate diverse and inclusive educational\n",
      "content, significantly saving more time for teaching and student\n",
      "interaction [453, 454]. In language learning, LLMs serve as\n",
      "----\n",
      "interaction [453, 454]. In language learning, LLMs serve as\n",
      "advanced conversational partners capable of simulating conver-\n",
      "sations in multiple languages, correcting grammar, enhancing\n",
      "vocabulary, and aiding pronunciation for the needs of fluency\n",
      "in practice [455]. Furthermore, LLMs improve accessibility\n",
      "in education by providing support for students with disabili-\n",
      "ties. They can generate real-time transcriptions for the hear-\n",
      "ing impaired, offer reading assistance for the visually impaired,\n",
      "----\n",
      "ing impaired, offer reading assistance for the visually impaired,\n",
      "and simplify complex texts for those with learning disabili-\n",
      "ties [451]. As LLMs continue to evolve, their applications in\n",
      "education can benefit more students and teachers from different\n",
      "perspectives in practice.\n",
      "Science: Similar to medical applications, LLMs can expedite\n",
      "the research process by quickly analyzing and summarizing sci-\n",
      "entific literature. By briefing comprehensible and accessible re-\n",
      "----\n",
      "entific literature. By briefing comprehensible and accessible re-\n",
      "search summaries, LLMs can assist researchers in staying up-\n",
      "to-date with the latest findings, even in fields outside their area\n",
      "of expertise [456, 457]. In addition, LLMs can aid scientists\n",
      "32\n",
      "----\n",
      "Table 12: Performance comparison of top performing LLMs across various NLU and NLG tasks. Here, “N-Shots” indicate the number of example prompts provided\n",
      "to the model during the evaluation, representing its capability in few-shot or zero-shot learning settings, “f” represents the fine-tuned version, and “B” represents the\n",
      "benchmark.\n",
      "Task\n",
      "Dataset/Benchmark\n",
      "Top-1\n",
      "Top-2\n",
      "Top-3\n",
      "Model (Size)\n",
      "Score (N-shots)\n",
      "Model (Size)\n",
      "Score (N-shots)\n",
      "Model (Size)\n",
      "Score (N-shots)\n",
      "Multi-Task\n",
      "BIG-bench (B)\n",
      "----\n",
      "Score (N-shots)\n",
      "Model (Size)\n",
      "Score (N-shots)\n",
      "Model (Size)\n",
      "Score (N-shots)\n",
      "Multi-Task\n",
      "BIG-bench (B)\n",
      "Chinchilla (70B)\n",
      "65.1 (5-shot)\n",
      "Gopher (280B)\n",
      "53.97 (5-shot)\n",
      "PaLM (540B)\n",
      "53.7 (5-shot)\n",
      "MMLU (B)\n",
      "GPT-4 (-)\n",
      "86.4 (5-shot)\n",
      "Gemini (Ultra)\n",
      "83.7 (5-shot)\n",
      "Flan-PaLM-2( f) (Large)\n",
      "81.2 (5-shot)\n",
      "Language Understanding\n",
      "SuperGLUE (B)\n",
      "ERNIE 3.0 (12B)\n",
      "90.6 (-)\n",
      "PaLM(f) (540B)\n",
      "90.4 (-)\n",
      "T5 (11B)\n",
      "88.9 (-)\n",
      "Story Comprehension and\n",
      "Generation\n",
      "HellaSwag\n",
      "GPT-4 (-)\n",
      "95.3 (10-shot)\n",
      "Gemini (Ultra)\n",
      "87.8 (10-shot)\n",
      "----\n",
      "Story Comprehension and\n",
      "Generation\n",
      "HellaSwag\n",
      "GPT-4 (-)\n",
      "95.3 (10-shot)\n",
      "Gemini (Ultra)\n",
      "87.8 (10-shot)\n",
      "PaLM-2 (Large)\n",
      "86.8 (one shot)\n",
      "StoryCloze\n",
      "GPT3 (175B)\n",
      "87.7 (few shot)\n",
      "PaLM-2 (Large)\n",
      "87.4 (one shot)\n",
      "OPT (175B)\n",
      "79.82 (-)\n",
      "Physical Knowledge and\n",
      "World Understanding\n",
      "PIQA\n",
      "PaLM-2 (Large)\n",
      "85.0 (one shot)\n",
      "LLaMa (65B)\n",
      "82.8 (zero shot)\n",
      "MT-NLG (530B)\n",
      "81.99 (zero shot)\n",
      "TriviaQA\n",
      "PaLM-2 (Large)\n",
      "86.1 (one shot)\n",
      "LLaMA-2 (70B)\n",
      "85.0 (one shot)\n",
      "PaLM (540B)\n",
      "81.4 (one shot)\n",
      "Contextual Language\n",
      "Understanding\n",
      "----\n",
      "LLaMA-2 (70B)\n",
      "85.0 (one shot)\n",
      "PaLM (540B)\n",
      "81.4 (one shot)\n",
      "Contextual Language\n",
      "Understanding\n",
      "LAMBADA\n",
      "PaLM (540B)\n",
      "89.7 (few shot)\n",
      "MT-NLG (530B)\n",
      "87.15 (few shot)\n",
      "PaLM-2 (Large)\n",
      "86.9 (one shot)\n",
      "Commonsense Reasoning\n",
      "WinoGrande\n",
      "GPT-4 (-)\n",
      "87.5 (5-shot)\n",
      "PaLM-2 (Large)\n",
      "83.0 (one shot)\n",
      "PaLM (540B)\n",
      "81.1 (zero shot)\n",
      "SIQA\n",
      "LLaMA (65B)\n",
      "52.3 (zero shot)\n",
      "Chinchilla (70B)\n",
      "51.3 (zero shot)\n",
      "Gopher (280B)\n",
      "50.6 (zero shot)\n",
      "Reading Comprehension\n",
      "BoolQ\n",
      "PaLM(f) (540B)\n",
      "92.2 (-)\n",
      "T5 (11B)\n",
      "91.2 (-)\n",
      "PaLM-2 (Large)\n",
      "----\n",
      "Reading Comprehension\n",
      "BoolQ\n",
      "PaLM(f) (540B)\n",
      "92.2 (-)\n",
      "T5 (11B)\n",
      "91.2 (-)\n",
      "PaLM-2 (Large)\n",
      "90.9 (one shot)\n",
      "Truthfulness\n",
      "Truthful-QA\n",
      "LLaMA (65B)\n",
      "57 (-)\n",
      "Mathematical Reasoning\n",
      "MATH\n",
      "Gemini (Ultra)\n",
      "53.2 (4-shot)\n",
      "PaLM-2 (Large)\n",
      "34.3 (4-shot)\n",
      "LLaMa-2 (65B)\n",
      "13.5 (4-shot)\n",
      "GSM8K\n",
      "GPT-4 (-)\n",
      "92.0 (5-shot)\n",
      "PaLM-2 (Large)\n",
      "80.7 (8-shot)\n",
      "U-PaLM (540B)\n",
      "58.5 (-)\n",
      "Problem Solving and\n",
      "Logical Reasoning\n",
      "HumanEval\n",
      "Gemini( f) (Ultra)\n",
      "74.4 (zero shot)\n",
      "GPT-4 (-)\n",
      "67.0 (zero shot)\n",
      "Code Llama (34B)\n",
      "48.8 (zero shot)\n",
      "----\n",
      "Gemini( f) (Ultra)\n",
      "74.4 (zero shot)\n",
      "GPT-4 (-)\n",
      "67.0 (zero shot)\n",
      "Code Llama (34B)\n",
      "48.8 (zero shot)\n",
      "in formulating new hypotheses and research questions since\n",
      "their ability to process large-scale datasets allows them to un-\n",
      "veil insights that might not be immediately apparent to human\n",
      "researchers [458]. Moreover, for scientific writing, LLMs can\n",
      "help researchers draft documents, suggest improvements, and\n",
      "ensure adherence to specific formatting guidelines [459, 460].\n",
      "----\n",
      "ensure adherence to specific formatting guidelines [459, 460].\n",
      "This not only saves time but also improves the clarity of scien-\n",
      "tific communication, enabling interdisciplinary teams to work\n",
      "together more effectively.\n",
      "Maths: In addition to providing mathematical research and\n",
      "education support, LLMs can assist in solving mathematical\n",
      "problems by giving step-by-step explanations and guiding users\n",
      "through complex proofs and calculations. They can help iden-\n",
      "----\n",
      "through complex proofs and calculations. They can help iden-\n",
      "tify errors in reasoning or computation and suggest corrections,\n",
      "serving as an invaluable tool for both learning and verification\n",
      "purposes [461, 462]. LLMs can be employed to check the valid-\n",
      "ity of mathematical proofs, offering a preliminary filter before\n",
      "human review. While they are not a substitute for the meticu-\n",
      "lous work of mathematicians, they can help simplify the process\n",
      "----\n",
      "lous work of mathematicians, they can help simplify the process\n",
      "of proof verification [463, 464]. Moreover, LLMs enhance ac-\n",
      "cessibility to mathematics by translating complex concepts and\n",
      "findings into understandable language for non-specialists [465],\n",
      "where the gap between theoretical mathematics and applied\n",
      "contexts such as physics, engineering, and economics can be\n",
      "bridged.\n",
      "Law: LLMs can assist with the thematic analysis of legal doc-\n",
      "----\n",
      "bridged.\n",
      "Law: LLMs can assist with the thematic analysis of legal doc-\n",
      "uments, including generating initial coding for datasets, iden-\n",
      "tifying themes, and classifying data according to these themes.\n",
      "This collaborative effort between legal experts and LLMs has\n",
      "proved to be effective in analyzing legal texts such as court\n",
      "opinions on theft, improving both the efficiency and quality of\n",
      "the research [466]. Additionally, LLMs have been evaluated for\n",
      "----\n",
      "the research [466]. Additionally, LLMs have been evaluated for\n",
      "their ability to generate explanations of legal terms, focusing\n",
      "on improving factual accuracy and relevance by incorporating\n",
      "sentences from case law. By feeding relevant case law into the\n",
      "LLM, the augmented models can generate higher-quality expla-\n",
      "nations with less factually incorrect information [467]. More-\n",
      "over, LLMs can be trained with specialized domain knowledge\n",
      "to perform legal reasoning tasks [468] and answer legal ques-\n",
      "----\n",
      "to perform legal reasoning tasks [468] and answer legal ques-\n",
      "tions [469].\n",
      "Finance: LLMs like BloombergGPT [151], trained on exten-\n",
      "sive proprietary financial datasets, exhibit superior performance\n",
      "on financial tasks. This indicates the value of domain-specific\n",
      "training in creating LLMs that can more accurately understand\n",
      "and process industry-specific language and concepts. The intro-\n",
      "duction of FinGPT [470] as an open-source model offers trans-\n",
      "----\n",
      "duction of FinGPT [470] as an open-source model offers trans-\n",
      "parent and accessible resources to develop novel applications\n",
      "such as robo-advising, algorithmic trading, and low-code so-\n",
      "lutions, ultimately expanding the capabilities of financial ser-\n",
      "vices. Both BloombergGPT and FinGPT show the adaptabil-\n",
      "ity of LLMs to the financial domain, with the former showing\n",
      "the power of custom datasets and the latter emphasizing a data-\n",
      "centric approach and low-rank adaptation techniques for cus-\n",
      "----\n",
      "centric approach and low-rank adaptation techniques for cus-\n",
      "tomization. Moreover, LLMs demonstrate an ability to break\n",
      "down complex financial tasks into actionable plans, enabling\n",
      "end-to-end solutions that were previously unfeasible with a sin-\n",
      "gle model [471].\n",
      "Robotics: In robotics research, LLMs have promising appli-\n",
      "cations, such as enhancing human-robot interaction [28, 472,\n",
      "473, 474], task planning [237], motion planning [246], nav-\n",
      "----\n",
      "473, 474], task planning [237], motion planning [246], nav-\n",
      "igation [246, 475], object manipulation [236], personalized\n",
      "robots [476], etc. LLMs enable robots to understand the en-\n",
      "vironment effectively and generate plans to complete tasks col-\n",
      "laboratively [240, 26]. They can facilitate continuous learning\n",
      "by allowing robots to access and integrate information from a\n",
      "wide range of sources, helping robots acquire new skills, adapt\n",
      "to changes, and refine their paths [224, 233, 234].\n",
      "----\n",
      "to changes, and refine their paths [224, 233, 234].\n",
      "7. Challenges and Future Directions\n",
      "LLMs such as GPT-4 and its predecessors have significantly\n",
      "advanced natural language processing. Nevertheless, they also\n",
      "bring along a set of challenges. The computational cost, ad-\n",
      "versarial robustness, and interpretability are among the tech-\n",
      "nical challenges that are intrinsic to these models.\n",
      "Further-\n",
      "more, as these models are scaled up to handle more complex\n",
      "33\n",
      "----\n",
      "tasks or to operate in more complex or dynamic environments,\n",
      "new challenges in scalability, privacy, and real-time processing\n",
      "emerge. On the frontier of foundational research, integrating\n",
      "multi-modality and the effectiveness of transfer learning are be-\n",
      "ing keenly explored. Additionally, the continuous learning as-\n",
      "pect of these models, which aims to have models that can adapt\n",
      "to new information over time, presents a fresh set of challenges.\n",
      "----\n",
      "to new information over time, presents a fresh set of challenges.\n",
      "These challenges not only underscore the technical intricacies\n",
      "involved but also highlight the broader impact and the future\n",
      "trajectory of LLMs in real-world applications. The following\n",
      "sections delve into these challenges, shedding light on the on-\n",
      "going and potential efforts to address them.\n",
      "Computational Cost: Training LLMs require extensive compu-\n",
      "tational resources, which increases production costs and raises\n",
      "----\n",
      "tational resources, which increases production costs and raises\n",
      "environmental concerns due to substantial energy consump-\n",
      "tion during large-scale training. Improved performance occurs\n",
      "as computational resources increase, but the rate of improve-\n",
      "ment gradually decreases when both the model and dataset\n",
      "size remain fixed, following the power law of diminishing re-\n",
      "turns [477].\n",
      "Bias and Fairness: LLMs can inherit and amplify societal bi-\n",
      "----\n",
      "turns [477].\n",
      "Bias and Fairness: LLMs can inherit and amplify societal bi-\n",
      "ases in their training data. These biases can manifest in the\n",
      "model’s outputs, leading to potential ethical and fairness is-\n",
      "sues [478].\n",
      "Overfitting: Although LLMs possess substantial learning ca-\n",
      "pabilities, they are susceptible to overfitting noisy and peculiar\n",
      "patterns within their extensive training data. Consequently, this\n",
      "may cause them to generate illogical responses [479]. The de-\n",
      "----\n",
      "may cause them to generate illogical responses [479]. The de-\n",
      "bate about Memorization vs. Generalization in LLMs is about\n",
      "finding the right balance. Memorization allows the model to\n",
      "remember specific details from its training data, ensuring it can\n",
      "provide accurate answers to precise questions. However, gen-\n",
      "eralization enables the model to make inferences and produce\n",
      "responses for inputs it has not seen before, which is essential\n",
      "for handling various real-world tasks. Striking the right bal-\n",
      "----\n",
      "for handling various real-world tasks. Striking the right bal-\n",
      "ance is the challenge: too much memorization can lead to over-\n",
      "fitting, making the model inflexible and struggling with new\n",
      "inputs [480].\n",
      "Economic and Research Inequality: The high cost of train-\n",
      "ing and deploying LLMs may make their development concen-\n",
      "trated within well-funded organizations, potentially worsening\n",
      "economic and research inequalities in AI [481].\n",
      "Reasoning and Planning: Some reasoning and planning tasks,\n",
      "----\n",
      "Reasoning and Planning: Some reasoning and planning tasks,\n",
      "even as seemingly simple as common-sense planning, which\n",
      "humans find easy, remain well beyond the current capabilities\n",
      "of LLMs evaluated using an assessment framework. This is not\n",
      "entirely unexpected, considering that LLMs primarily generate\n",
      "text completions based on likelihood and offer no solid guaran-\n",
      "tees in terms of reasoning abilities [482].\n",
      "Hallucinations: LLMs exhibit “hallucinations\", where they\n",
      "----\n",
      "Hallucinations: LLMs exhibit “hallucinations\", where they\n",
      "generate responses that, while sounding plausible, are incorrect\n",
      "or do not align with the provided information [483]. Hallucina-\n",
      "tions can be categorized into three categories.\n",
      "• Input-conflicting hallucination, wherein LLMs produce\n",
      "content that diverges from the input given by users.\n",
      "• Context-conflicting hallucination, where LLMs generate\n",
      "content that contradicts information they have generated\n",
      "earlier.\n",
      "----\n",
      "content that contradicts information they have generated\n",
      "earlier.\n",
      "• Fact-conflicting hallucination involves LLM’s generation\n",
      "of content that does not align with established world\n",
      "knowledge.\n",
      "Prompt Engineering: Prompts serve as inputs to LLMs, and\n",
      "their syntax and semantics play a crucial role in determining\n",
      "the model’s output. The prompt variations, sometimes counter-\n",
      "intuitive to humans, can result in significant changes in model\n",
      "output and are addressed through prompt engineering, which\n",
      "----\n",
      "output and are addressed through prompt engineering, which\n",
      "involves designing natural language queries to guide LLMs\n",
      "responses effectively [484, 32].\n",
      "Limited Knowledge: Information acquired during pretraining\n",
      "is limited and may become obsolete after some time.\n",
      "Re-\n",
      "training the model using updated data is costly. To generate\n",
      "factually accurate responses, people use a retrieval augmen-\n",
      "tation pipeline [198].\n",
      "However, pre-trained models are not\n",
      "----\n",
      "tation pipeline [198].\n",
      "However, pre-trained models are not\n",
      "trained with retrieval augmentation generation (RAG) [6, 21];\n",
      "hence, adapting the training pipeline is necessary [193, 25].\n",
      "Safety and Controllability: Using LLMs comes with the risk\n",
      "of generating harmful, misleading, or inappropriate content,\n",
      "whether by accident or when given specific prompts. Ensuring\n",
      "these models are safely utilized is a significant concern [485].\n",
      "Security and Privacy: LLMs are prone to leaking personal\n",
      "----\n",
      "Security and Privacy: LLMs are prone to leaking personal\n",
      "information and generating false, unethical, misaligned re-\n",
      "sponses. Researchers have explored various security attacks,\n",
      "i.e., backdoor attacks, jailbreaking, prompt injection, and data\n",
      "poisoning, that lead to breaking LLMs security.\n",
      "Therefore,\n",
      "developing better defense mechanisms is essential to ensure\n",
      "LLMs are safe, reliable, and trustworthy for complex AI\n",
      "applications [486].\n",
      "Multi-Modality:\n",
      "Multi-modal learning, where LLMs are\n",
      "----\n",
      "applications [486].\n",
      "Multi-Modality:\n",
      "Multi-modal learning, where LLMs are\n",
      "trained on diverse data like text, images, and videos, aims to\n",
      "create models with richer understanding but faces challenges\n",
      "in data alignment, fusion strategies, and higher computational\n",
      "demands.\n",
      "Catastrophic Forgetting:\n",
      "LLMs are often pre-trained on\n",
      "large datasets and then fine-tuned on domain-specific data,\n",
      "reducing training resources.\n",
      "However, they face issues like\n",
      "----\n",
      "reducing training resources.\n",
      "However, they face issues like\n",
      "domain adaptation and catastrophic forgetting, which hinder\n",
      "the retention of original knowledge when learning new tasks.\n",
      "Adversarial Robustness:\n",
      "Large Language Models (LLMs)\n",
      "have shown great capabilities in various tasks but are vul-\n",
      "nerable to adversarial attacks, where slight, deliberate input\n",
      "alterations can mislead them.\n",
      "Especially with models like\n",
      "BERT, adversarial fine-tuning can enhance robustness, al-\n",
      "----\n",
      "Especially with models like\n",
      "BERT, adversarial fine-tuning can enhance robustness, al-\n",
      "though it sometimes compromises generalization [487].\n",
      "As\n",
      "LLMs integrate more into complex systems, examining their\n",
      "security properties becomes crucial, given the emerging field\n",
      "of adversarial attacks on LLMs within trustworthy ML [488].\n",
      "This vulnerability is notable in safety-critical domains, ne-\n",
      "cessitating robust adversarial evaluation tools to ensure LLM\n",
      "reliability [489].\n",
      "----\n",
      "cessitating robust adversarial evaluation tools to ensure LLM\n",
      "reliability [489].\n",
      "Interpretability and Explainability: The “black-box” nature\n",
      "of LLMs poses challenges in understanding their decision-\n",
      "making, which is crucial for broader acceptance and trust,\n",
      "34\n",
      "----\n",
      "especially in sensitive domains.\n",
      "Despite their advanced\n",
      "capabilities, the lack of insight into their operation limits their\n",
      "effectiveness and trustworthiness [490, 491]. Efforts are being\n",
      "made to make LLMs more explainable to promote user trust\n",
      "and to ensure responsible AI usage. Understanding the logic\n",
      "behind LLMs’ responses is essential for fostering trust and\n",
      "ensuring they align with human values and legal standards.\n",
      "Privacy Concerns:\n",
      "Privacy concerns in Large Language\n",
      "----\n",
      "Privacy Concerns:\n",
      "Privacy concerns in Large Language\n",
      "Models (LLMs) have escalated with their growth in complexity\n",
      "and size, particularly around data sharing and potential misuse.\n",
      "There is a risk of malicious content creation, filter bypass,\n",
      "and data privacy issues, especially in e-commerce, where\n",
      "protecting customer privacy is crucial. If models are trained\n",
      "on private data, additional concerns arise if such models are\n",
      "made publicly available. LLMs tend to memorize phrases from\n",
      "----\n",
      "made publicly available. LLMs tend to memorize phrases from\n",
      "their training sets, which an adversary could exploit to extract\n",
      "sensitive data, posing a threat to personal privacy [492, 493].\n",
      "Real-Time Processing: Real-time processing in Large Lan-\n",
      "guage Models (LLMs) is pivotal for various applications,\n",
      "especially with the rising popularity of mobile AI applications\n",
      "and concerns regarding information security and privacy.\n",
      "However, LLMs often have hundreds of layers and millions\n",
      "----\n",
      "However, LLMs often have hundreds of layers and millions\n",
      "of parameters, which impede real-time processing due to the\n",
      "high computational demands and limited weight storage on\n",
      "hardware platforms, particularly in edge computing environ-\n",
      "ments [494].\n",
      "While certain efforts like MobileBERT aim\n",
      "to reduce memory requirements, they still face substantial\n",
      "execution overhead due to the large number of model layers,\n",
      "leading to high inference latency.\n",
      "Long-Term Dependencies:\n",
      "Large Language Models have\n",
      "----\n",
      "leading to high inference latency.\n",
      "Long-Term Dependencies:\n",
      "Large Language Models have\n",
      "shown considerable progress in understanding and generating\n",
      "text, yet they often struggle with preserving context and\n",
      "handling long-term dependencies, particularly in complex,\n",
      "multi-turn conversations or long documents. This limitation\n",
      "can lead to incoherent or irrelevant responses.\n",
      "Hardware Acceleration: The growth of LLMs presents signif-\n",
      "icant hardware challenges due to the increasing computational\n",
      "----\n",
      "icant hardware challenges due to the increasing computational\n",
      "and memory demands associated with training and deploying\n",
      "these models. GPUs have played a crucial role in meeting the\n",
      "hardware requirements for training LLMs, with the networking\n",
      "industry also evolving to optimize hardware for training\n",
      "workloads. However, the growing size of LLMs, which has\n",
      "been outpacing hardware progress, makes model inference in-\n",
      "creasingly costly. Model quantization is a promising approach\n",
      "----\n",
      "creasingly costly. Model quantization is a promising approach\n",
      "to bridge the widening gap between LLM size and hardware\n",
      "capacity [495].\n",
      "Although specialized hardware acceleration\n",
      "like GPUs or TPUs can significantly reduce the computational\n",
      "cost, making real-time applications more feasible, they may not\n",
      "fully resolve all limitations, necessitating further advancements\n",
      "in hardware technology.\n",
      "Regulatory and Ethical Frameworks: The rapid advancements\n",
      "----\n",
      "in hardware technology.\n",
      "Regulatory and Ethical Frameworks: The rapid advancements\n",
      "in artificial intelligence have given rise to sophisticated Large\n",
      "Language Models (LLMs) like OpenAI’s GPT-4 [157] and\n",
      "Google’s Bard. These developments underscore the imperative\n",
      "for regulatory oversight to manage the ethical and social\n",
      "challenges accompanying LLMs’ widespread use [496]. For\n",
      "instance, LLMs can generate content that can be used posi-\n",
      "tively or negatively, emphasizing the need for proactive ethical\n",
      "----\n",
      "tively or negatively, emphasizing the need for proactive ethical\n",
      "frameworks and policy measures to guide their responsible\n",
      "use and assign accountability for their outputs [497]. Auditing\n",
      "is identified as a promising governance mechanism to ensure\n",
      "that AI systems, including LLMs, are designed and deployed\n",
      "ethically, legally, and technically robust [498].\n",
      "8. Conclusion\n",
      "This article has comprehensively reviewed the develop-\n",
      "ments in LLMs.\n",
      "It contributes to summarizing significant\n",
      "----\n",
      "ments in LLMs.\n",
      "It contributes to summarizing significant\n",
      "findings of LLMs in the existing literature and provides a\n",
      "detailed analysis of the design aspects, including architec-\n",
      "tures, datasets, and training pipelines.\n",
      "We identified crucial\n",
      "architectural components and training strategies employed by\n",
      "different LLMs.\n",
      "These aspects are presented as summaries\n",
      "and discussions throughout the article.\n",
      "Moreover, we have\n",
      "discussed the performance differences of LLMs in zero-shot\n",
      "----\n",
      "Moreover, we have\n",
      "discussed the performance differences of LLMs in zero-shot\n",
      "and few-shot settings, explored the impact of fine-tuning, and\n",
      "compared supervised and generalized models and encoder vs.\n",
      "decoder vs. encoder-decoder architectures. A comprehensive\n",
      "review of multi-modal LLMs, retrieval augmented LLMs,\n",
      "LLMs-powered agents, efficient LLMs, datasets, evaluation,\n",
      "applications, and challenges is also provided. This article is\n",
      "anticipated to serve as a valuable resource for researchers,\n",
      "----\n",
      "anticipated to serve as a valuable resource for researchers,\n",
      "offering insights into the recent advancements in LLMs and\n",
      "providing fundamental concepts and details to develop better\n",
      "LLMs.\n",
      "Acknowledgement:\n",
      "The author/s would like to acknowl-\n",
      "edge the support received from Saudi Data and AI Authority\n",
      "(SDAIA) and King Fahd University of Petroleum and Miner-\n",
      "als (KFUPM) under SDAIA-KFUPM Joint Research Center for\n",
      "Artificial Intelligence Grant No. JRC-AI-RFP-11.\n",
      "References\n",
      "----\n",
      "Artificial Intelligence Grant No. JRC-AI-RFP-11.\n",
      "References\n",
      "[1] A. Chernyavskiy, D. Ilvovsky, P. Nakov, Transformers:“the end of his-\n",
      "tory” for natural language processing?, in:\n",
      "Machine Learning and\n",
      "Knowledge Discovery in Databases. Research Track: European Con-\n",
      "ference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021,\n",
      "Proceedings, Part III 21, Springer, 2021, pp. 677–693. 1\n",
      "[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\n",
      "----\n",
      "[2] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill,\n",
      "O. Levy, S. Bowman, Superglue: A stickier benchmark for general-\n",
      "purpose language understanding systems, Advances in neural informa-\n",
      "tion processing systems 32 (2019). 1, 26, 29\n",
      "[3] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\n",
      "Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu, et al., Towards a human-\n",
      "like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\n",
      "----\n",
      "like open-domain chatbot, arXiv preprint arXiv:2001.09977 (2020). 1\n",
      "[4] B. A. y Arcas, Do large language models understand us?, Daedalus\n",
      "151 (2) (2022) 183–197. 2\n",
      "[5] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al.,\n",
      "Language models are unsupervised multitask learners, OpenAI blog\n",
      "1 (8) (2019) 9. 2, 7\n",
      "[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\n",
      "A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\n",
      "----\n",
      "A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\n",
      "are few-shot learners, Advances in neural information processing sys-\n",
      "tems 33 (2020) 1877–1901. 2, 6, 7, 8, 9, 16, 18, 23, 24, 25, 34\n",
      "[7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training\n",
      "of deep bidirectional transformers for language understanding, arXiv\n",
      "preprint arXiv:1810.04805 (2018). 2, 18, 24\n",
      "35\n",
      "----\n",
      "[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\n",
      "L. Zettlemoyer, Deep contextualized word representations, in: NAACL-\n",
      "HLT, Association for Computational Linguistics, 2018, pp. 2227–2237.\n",
      "2\n",
      "[9] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\n",
      "V. Stoyanov, L. Zettlemoyer, Bart: Denoising sequence-to-sequence pre-\n",
      "training for natural language generation, translation, and comprehen-\n",
      "sion, arXiv preprint arXiv:1910.13461 (2019). 2\n",
      "----\n",
      "sion, arXiv preprint arXiv:1910.13461 (2019). 2\n",
      "[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\n",
      "Y. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with\n",
      "a unified text-to-text transformer, The Journal of Machine Learning Re-\n",
      "search 21 (1) (2020) 5485–5551. 2, 7, 8, 18, 19, 24, 25, 28, 30, 31\n",
      "[11] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\n",
      "A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-\n",
      "----\n",
      "A. Barua, C. Raffel, mt5: A massively multilingual pre-trained text-to-\n",
      "text transformer, arXiv preprint arXiv:2010.11934 (2020). 2, 7, 8, 24,\n",
      "25, 28, 30\n",
      "[12] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun, Y. Yao, F. Qi,\n",
      "J. Guan, P. Ke, et al., Cpm-2: Large-scale cost-effective pre-trained lan-\n",
      "guage models, AI Open 2 (2021) 216–224. 2, 8, 25\n",
      "[13] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\n",
      "R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\n",
      "----\n",
      "R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-\n",
      "parameter open-access multilingual language model, arXiv preprint\n",
      "arXiv:2211.05100 (2022). 2, 4, 9, 11, 23, 24, 25, 30\n",
      "[14] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\n",
      "M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer\n",
      "language models, arXiv preprint arXiv:2205.01068 (2022). 2, 9, 11, 24,\n",
      "25\n",
      "[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\n",
      "----\n",
      "25\n",
      "[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\n",
      "P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scal-\n",
      "ing language modeling with pathways, arXiv preprint arXiv:2204.02311\n",
      "(2022). 2, 6, 9, 11, 23, 24, 25\n",
      "[16] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li,\n",
      "X. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned\n",
      "language models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\n",
      "17, 22, 24, 25, 28, 31\n",
      "----\n",
      "language models, arXiv preprint arXiv:2210.11416 (2022). 2, 7, 11, 16,\n",
      "17, 22, 24, 25, 28, 31\n",
      "[17] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\n",
      "A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, et al., Multitask\n",
      "prompted training enables zero-shot task generalization, arXiv preprint\n",
      "arXiv:2110.08207 (2021). 2, 11, 16, 25, 28, 31\n",
      "[18] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\n",
      "A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\n",
      "----\n",
      "A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, et al.,\n",
      "Super-naturalinstructions: Generalization via declarative instructions on\n",
      "1600+ nlp tasks, in: Proceedings of the 2022 Conference on Empirical\n",
      "Methods in Natural Language Processing, 2022, pp. 5085–5109. 2, 7,\n",
      "11, 16, 17, 24, 25, 28, 31\n",
      "[19] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, H. Ha-\n",
      "jishirzi, Self-instruct: Aligning language model with self generated in-\n",
      "----\n",
      "jishirzi, Self-instruct: Aligning language model with self generated in-\n",
      "structions, arXiv preprint arXiv:2212.10560 (2022). 2, 16, 19, 22, 28\n",
      "[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\n",
      "C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\n",
      "els to follow instructions with human feedback, Advances in Neural In-\n",
      "formation Processing Systems 35 (2022) 27730–27744. 2, 7, 11, 16,\n",
      "22\n",
      "[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\n",
      "----\n",
      "22\n",
      "[21] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\n",
      "N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open\n",
      "foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288\n",
      "(2023). 2, 7, 10, 16, 25, 34\n",
      "[22] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-\n",
      "gatama, M. Bosma, D. Zhou, D. Metzler, et al., Emergent abilities of\n",
      "large language models, arXiv preprint arXiv:2206.07682 (2022). 2\n",
      "----\n",
      "large language models, arXiv preprint arXiv:2206.07682 (2022). 2\n",
      "[23] T. Webb, K. J. Holyoak, H. Lu, Emergent analogical reasoning in large\n",
      "language models, Nature Human Behaviour 7 (9) (2023) 1526–1541. 2\n",
      "[24] D. A. Boiko, R. MacKnight, G. Gomes, Emergent autonomous sci-\n",
      "entific research capabilities of large language models, arXiv preprint\n",
      "arXiv:2304.05332 (2023). 2\n",
      "[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\n",
      "----\n",
      "[25] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\n",
      "J. Dwivedi-Yu, A. Joulin, S. Riedel, E. Grave, Few-shot learning with\n",
      "retrieval augmented language models, arXiv preprint arXiv:2208.03299\n",
      "(2022). 2, 18, 19, 34\n",
      "[26] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\n",
      "A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied\n",
      "multimodal language model, arXiv preprint arXiv:2303.03378 (2023).\n",
      "2, 20, 22, 33\n",
      "----\n",
      "multimodal language model, arXiv preprint arXiv:2303.03378 (2023).\n",
      "2, 20, 22, 33\n",
      "[27] A. Parisi, Y. Zhao, N. Fiedel, Talm: Tool augmented language models,\n",
      "arXiv preprint arXiv:2205.12255 (2022). 2, 19, 20\n",
      "[28] B. Zhang, H. Soh, Large language models as zero-shot human models\n",
      "for human-robot interaction, arXiv preprint arXiv:2303.03548 (2023). 2,\n",
      "33\n",
      "[29] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi,\n",
      "Y. Shi, et al., mplug-owl: Modularization empowers large language\n",
      "----\n",
      "Y. Shi, et al., mplug-owl: Modularization empowers large language\n",
      "models with multimodality, arXiv preprint arXiv:2304.14178 (2023). 2,\n",
      "22\n",
      "[30] W. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng, P. Luo,\n",
      "T. Lu, J. Zhou, Y. Qiao, et al., Visionllm: Large language model\n",
      "is also an open-ended decoder for vision-centric tasks, arXiv preprint\n",
      "arXiv:2305.11175 (2023). 2, 22\n",
      "[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:\n",
      "----\n",
      "[31] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, Y. Shan, Gpt4tools:\n",
      "Teaching large language model to use tools via self-instruction, arXiv\n",
      "preprint arXiv:2305.18752 (2023). 2, 19, 22, 23\n",
      "[32] E.\n",
      "Saravia,\n",
      "Prompt\n",
      "Engineering\n",
      "Guide,\n",
      "https://github.com/dair-\n",
      "ai/Prompt-Engineering-Guide (12 2022). 2, 7, 18, 34\n",
      "[33] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,\n",
      "W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\n",
      "----\n",
      "W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained\n",
      "model, arXiv preprint arXiv:2210.02414 (2022). 2, 10, 23, 24, 25\n",
      "[34] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, S. C. Hoi, Codet5+:\n",
      "Open code large language models for code understanding and genera-\n",
      "tion, arXiv preprint arXiv:2305.07922 (2023). 2, 11, 24, 25\n",
      "[35] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong, S. Feng, J. Shang,\n",
      "Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\n",
      "----\n",
      "Y. Zhao, C. Pang, et al., Ernie 3.0 titan: Exploring larger-scale knowl-\n",
      "edge enhanced pre-training for language understanding and generation,\n",
      "arXiv preprint arXiv:2112.12731 (2021). 2, 8, 24, 25\n",
      "[36] J. Rasley, S. Rajbhandari, O. Ruwase, Y. He, Deepspeed: System op-\n",
      "timizations enable training deep learning models with over 100 billion\n",
      "parameters, in: Proceedings of the 26th ACM SIGKDD International\n",
      "Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\n",
      "3506. 2, 5\n",
      "----\n",
      "Conference on Knowledge Discovery & Data Mining, 2020, pp. 3505–\n",
      "3506. 2, 5\n",
      "[37] S. Rajbhandari, J. Rasley, O. Ruwase, Y. He, Zero: Memory optimiza-\n",
      "tions toward training trillion parameter models, in: SC20: International\n",
      "Conference for High Performance Computing, Networking, Storage and\n",
      "Analysis, IEEE, 2020, pp. 1–16. 2, 4, 24\n",
      "[38] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, Towards\n",
      "a unified view of parameter-efficient transfer learning, arXiv preprint\n",
      "----\n",
      "a unified view of parameter-efficient transfer learning, arXiv preprint\n",
      "arXiv:2110.04366 (2021). 2, 20, 21\n",
      "[39] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee, L. Bing, S. Po-\n",
      "ria, Llm-adapters: An adapter family for parameter-efficient fine-tuning\n",
      "of large language models, arXiv preprint arXiv:2304.01933 (2023). 2,\n",
      "20\n",
      "[40] B. Lester, R. Al-Rfou, N. Constant, The power of scale for parameter-\n",
      "efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\n",
      "20, 21\n",
      "----\n",
      "efficient prompt tuning, arXiv preprint arXiv:2104.08691 (2021). 2, 8,\n",
      "20, 21\n",
      "[41] X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for\n",
      "generation, arXiv preprint arXiv:2101.00190 (2021). 2, 20, 21\n",
      "[42] X. Ma, G. Fang, X. Wang, Llm-pruner: On the structural pruning of\n",
      "large language models, arXiv preprint arXiv:2305.11627 (2023). 2, 22\n",
      "[43] R. Xu, F. Luo, C. Wang, B. Chang, J. Huang, S. Huang, F. Huang,\n",
      "From dense to sparse: Contrastive pruning for better pre-trained lan-\n",
      "----\n",
      "From dense to sparse: Contrastive pruning for better pre-trained lan-\n",
      "guage model compression, in: Proceedings of the AAAI Conference on\n",
      "Artificial Intelligence, Vol. 36, 2022, pp. 11547–11555. 2, 22\n",
      "[44] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, S. Han, Smoothquant:\n",
      "Accurate and efficient post-training quantization for large language\n",
      "models, in: ICML, Vol. 202 of Proceedings of Machine Learning Re-\n",
      "search, PMLR, 2023, pp. 38087–38099. 2, 21\n",
      "----\n",
      "search, PMLR, 2023, pp. 38087–38099. 2, 21\n",
      "[45] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, N. Wong,\n",
      "Compression of generative pre-trained language models via quantiza-\n",
      "tion, arXiv preprint arXiv:2203.10705 (2022). 2, 21\n",
      "[46] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, S. Naidu,\n",
      "Giraffe: Adventures in expanding context lengths in llms, arXiv preprint\n",
      "arXiv:2308.10882 (2023). 2, 17\n",
      "[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\n",
      "Efficient con-\n",
      "----\n",
      "[47] B. Peng, J. Quesnelle, H. Fan, E. Shippole, Yarn:\n",
      "Efficient con-\n",
      "text window extension of large language models, arXiv preprint\n",
      "arXiv:2309.00071 (2023). 2, 17\n",
      "[48] M. Guo, J. Ainslie, D. Uthus, S. Ontanon, J. Ni, Y.-H. Sung, Y. Yang,\n",
      "36\n",
      "----\n",
      "Longt5: Efficient text-to-text transformer for long sequences, arXiv\n",
      "preprint arXiv:2112.07916 (2021). 2, 18\n",
      "[49] S. Chen, S. Wong, L. Chen, Y. Tian, Extending context window\n",
      "of large language models via positional interpolation, arXiv preprint\n",
      "arXiv:2306.15595 (2023). 2, 17\n",
      "[50] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\n",
      "J. Zhang, Z. Dong, et al., A survey of large language models, arXiv\n",
      "preprint arXiv:2303.18223 (2023). 2, 3, 7\n",
      "----\n",
      "preprint arXiv:2303.18223 (2023). 2, 3, 7\n",
      "[51] U. Naseem, I. Razzak, S. K. Khan, M. Prasad, A comprehensive sur-\n",
      "vey on word representation models: From classical to state-of-the-art\n",
      "word representation language models, Transactions on Asian and Low-\n",
      "Resource Language Information Processing 20 (5) (2021) 1–35. 2, 3\n",
      "[52] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\n",
      "E. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\n",
      "----\n",
      "E. Agirre, I. Heinz, D. Roth, Recent advances in natural language pro-\n",
      "cessing via large pre-trained language models: A survey, arXiv preprint\n",
      "arXiv:2111.01243 (2021). 2, 3\n",
      "[53] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\n",
      "L. He, et al., A comprehensive survey on pretrained foundation models:\n",
      "A history from bert to chatgpt, arXiv preprint arXiv:2302.09419 (2023).\n",
      "2, 3\n",
      "[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\n",
      "----\n",
      "2, 3\n",
      "[54] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\n",
      "J. Xu, Z. Sui, A survey for in-context learning, arXiv preprint\n",
      "arXiv:2301.00234 (2022). 2, 7, 18\n",
      "[55] J. Huang, K. C.-C. Chang, Towards reasoning in large language models:\n",
      "A survey, arXiv preprint arXiv:2212.10403 (2022). 2, 7, 18\n",
      "[56] Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang, L. Shang, X. Jiang,\n",
      "Q. Liu, Aligning large language models with human: A survey, arXiv\n",
      "preprint arXiv:2307.12966 (2023). 2\n",
      "----\n",
      "preprint arXiv:2307.12966 (2023). 2\n",
      "[57] X. Zhu, J. Li, Y. Liu, C. Ma, W. Wang, A survey on model compression\n",
      "for large language models, arXiv preprint arXiv:2308.07633 (2023). 2\n",
      "[58] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey on multi-\n",
      "modal large language models, arXiv preprint arXiv:2306.13549 (2023).\n",
      "2, 22, 23\n",
      "[59] J. J. Webster, C. Kit, Tokenization as the initial phase in nlp, in: COL-\n",
      "ING 1992 volume 4: The 14th international conference on computa-\n",
      "----\n",
      "ING 1992 volume 4: The 14th international conference on computa-\n",
      "tional linguistics, 1992. 4\n",
      "[60] T. Kudo, Subword regularization: Improving neural network translation\n",
      "models with multiple subword candidates, in: Proceedings of the 56th\n",
      "Annual Meeting of the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers), 2018, pp. 66–75. 4\n",
      "[61] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\n",
      "words with subword units, in: Proceedings of the 54th Annual Meet-\n",
      "----\n",
      "words with subword units, in: Proceedings of the 54th Annual Meet-\n",
      "ing of the Association for Computational Linguistics (Volume 1: Long\n",
      "Papers), 2016, pp. 1715–1725. 4\n",
      "[62] M. Schuster, K. Nakajima, Japanese and korean voice search, in: 2012\n",
      "IEEE international conference on acoustics, speech and signal process-\n",
      "ing (ICASSP), IEEE, 2012, pp. 5149–5152. 4\n",
      "[63] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey, M. Gallé,\n",
      "A. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-\n",
      "----\n",
      "A. Raja, C. Si, W. Y. Lee, B. Sagot, et al., Between words and char-\n",
      "acters: A brief history of open-vocabulary modeling and tokenization in\n",
      "nlp, arXiv preprint arXiv:2112.10508 (2021). 4\n",
      "[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n",
      "Ł. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\n",
      "information processing systems 30 (2017). 4, 7\n",
      "[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\n",
      "----\n",
      "[65] O. Press, N. Smith, M. Lewis, Train short, test long: Attention with\n",
      "linear biases enables input length extrapolation, in: International Con-\n",
      "ference on Learning Representations, 2022.\n",
      "URL https://openreview.net/forum?id=R8sQPpGCv0 4, 17\n",
      "[66] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, Roformer: En-\n",
      "hanced transformer with rotary position embedding, arXiv preprint\n",
      "arXiv:2104.09864 (2021). 4, 9, 17\n",
      "[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\n",
      "----\n",
      "[67] R. Child, S. Gray, A. Radford, I. Sutskever, Generating long sequences\n",
      "with sparse transformers, arXiv preprint arXiv:1904.10509 (2019). 4, 7,\n",
      "23\n",
      "[68] T. Dao, D. Fu, S. Ermon, A. Rudra, C. Ré, Flashattention: Fast and\n",
      "memory-efficient exact attention with io-awareness, Advances in Neural\n",
      "Information Processing Systems 35 (2022) 16344–16359. 4\n",
      "[69] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks\n",
      "are universal approximators, Neural networks 2 (5) (1989) 359–366. 4\n",
      "----\n",
      "are universal approximators, Neural networks 2 (5) (1989) 359–366. 4\n",
      "[70] V. Nair, G. E. Hinton, Rectified linear units improve restricted boltz-\n",
      "mann machines, in: Proceedings of the 27th international conference on\n",
      "machine learning (ICML-10), 2010, pp. 807–814. 4\n",
      "[71] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\n",
      "preprint arXiv:1606.08415 (2016). 4\n",
      "[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\n",
      "----\n",
      "[72] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,\n",
      "Dropout: a simple way to prevent neural networks from overfitting, The\n",
      "journal of machine learning research 15 (1) (2014) 1929–1958. 4\n",
      "[73] D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. R.\n",
      "Ke, A. Goyal, Y. Bengio, A. Courville, C. Pal, Zoneout: Regular-\n",
      "izing rnns by randomly preserving hidden activations, arXiv preprint\n",
      "arXiv:1606.01305 (2016). 4\n",
      "[74] N.\n",
      "Shazeer,\n",
      "Glu\n",
      "variants\n",
      "improve\n",
      "transformer,\n",
      "----\n",
      "arXiv:1606.01305 (2016). 4\n",
      "[74] N.\n",
      "Shazeer,\n",
      "Glu\n",
      "variants\n",
      "improve\n",
      "transformer,\n",
      "arXiv\n",
      "preprint\n",
      "arXiv:2002.05202 (2020). 4\n",
      "[75] Y. N. Dauphin, A. Fan, M. Auli, D. Grangier, Language modeling with\n",
      "gated convolutional networks, in: International conference on machine\n",
      "learning, PMLR, 2017, pp. 933–941. 4\n",
      "[76] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\n",
      "arXiv:1607.06450 (2016). 4\n",
      "[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\n",
      "----\n",
      "[77] B. Zhang, R. Sennrich, Root mean square layer normalization, Advances\n",
      "in Neural Information Processing Systems 32 (2019). 4\n",
      "[78] A. Baevski, M. Auli, Adaptive input representations for neural language\n",
      "modeling, arXiv preprint arXiv:1809.10853 (2018). 4\n",
      "[79] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, F. Wei, Deepnet: Scaling\n",
      "transformers to 1,000 layers, arXiv preprint arXiv:2203.00555 (2022). 4\n",
      "[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\n",
      "----\n",
      "[80] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro,\n",
      "Megatron-lm: Training multi-billion parameter language models using\n",
      "model parallelism, arXiv preprint arXiv:1909.08053 (2019). 4, 5\n",
      "[81] \"bmtrain: Efficient training for big models.\".\n",
      "URL https://github.com/OpenBMB/BMTrain 4, 5\n",
      "[82] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cis-\n",
      "tac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\n",
      "----\n",
      "tac, T. Rault, R. Louf, M. Funtowicz, et al., Transformers: State-of-the-\n",
      "art natural language processing, in: Proceedings of the 2020 conference\n",
      "on empirical methods in natural language processing: system demon-\n",
      "strations, 2020, pp. 38–45. 5\n",
      "[83] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclau-\n",
      "rin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al.,\n",
      "Jax: composable transformations of python+ numpy programs (2018).\n",
      "5\n",
      "----\n",
      "Jax: composable transformations of python+ numpy programs (2018).\n",
      "5\n",
      "[84] S. Li, J. Fang, Z. Bian, H. Liu, Y. Liu, H. Huang, B. Wang, Y. You,\n",
      "Colossal-ai: A unified deep learning system for large-scale parallel train-\n",
      "ing, arXiv preprint arXiv:2110.14883 (2021). 5\n",
      "[85] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, J. Tang, Fastmoe:\n",
      "A\n",
      "fast mixture-of-expert training system, arXiv preprint arXiv:2103.13262\n",
      "(2021). 5\n",
      "[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\n",
      "----\n",
      "(2021). 5\n",
      "[86] L. Huawei Technologies Co., Huawei mindspore ai development frame-\n",
      "work, in: Artificial Intelligence Technology, Springer, 2022, pp. 137–\n",
      "162. 5\n",
      "[87] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\n",
      "T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imper-\n",
      "ative style, high-performance deep learning library, Advances in neural\n",
      "information processing systems 32 (2019). 5\n",
      "[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\n",
      "----\n",
      "[88] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\n",
      "S. Ghemawat, G. Irving, M. Isard, et al., Tensorflow: a system for large-\n",
      "scale machine learning., in: Osdi, Vol. 16, Savannah, GA, USA, 2016,\n",
      "pp. 265–283. 5\n",
      "[89] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao,\n",
      "B. Xu, C. Zhang, Z. Zhang, Mxnet: A flexible and efficient machine\n",
      "learning library for heterogeneous distributed systems, arXiv preprint\n",
      "arXiv:1512.01274 (2015). 5\n",
      "----\n",
      "learning library for heterogeneous distributed systems, arXiv preprint\n",
      "arXiv:1512.01274 (2015). 5\n",
      "[90] W. Fedus, B. Zoph, N. Shazeer, Switch transformers: Scaling to tril-\n",
      "lion parameter models with simple and efficient sparsity, The Journal of\n",
      "Machine Learning Research 23 (1) (2022) 5232–5270. 5, 9\n",
      "[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\n",
      "Y. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\n",
      "----\n",
      "Y. Zhou, A. W. Yu, O. Firat, et al., Glam: Efficient scaling of language\n",
      "models with mixture-of-experts, in: International Conference on Ma-\n",
      "chine Learning, PMLR, 2022, pp. 5547–5569. 5, 9, 23, 24, 25\n",
      "[92] X. Ren, P. Zhou, X. Meng, X. Huang, Y. Wang, W. Wang, P. Li,\n",
      "X. Zhang, A. Podolskiy, G. Arshinov, et al., Pangu-P: Towards trillion\n",
      "parameter language model with sparse heterogeneous computing, arXiv\n",
      "preprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\n",
      "----\n",
      "preprint arXiv:2303.10845 (2023). 5, 10, 16, 23, 24, 25\n",
      "[93] T. Wang, A. Roberts, D. Hesslow, T. Le Scao, H. W. Chung, I. Beltagy,\n",
      "J. Launay, C. Raffel, What language model architecture and pretrain-\n",
      "37\n",
      "----\n",
      "ing objective works best for zero-shot generalization?, in: International\n",
      "Conference on Machine Learning, PMLR, 2022, pp. 22964–22984. 5\n",
      "[94] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou,\n",
      "H.-W. Hon, Unified language model pre-training for natural language\n",
      "understanding and generation, Advances in neural information process-\n",
      "ing systems 32 (2019). 6\n",
      "[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\n",
      "----\n",
      "[95] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\n",
      "S. Gray, A. Radford, J. Wu, D. Amodei, Scaling laws for neural language\n",
      "models, arXiv preprint arXiv:2001.08361 (2020). 6\n",
      "[96] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\n",
      "E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,\n",
      "et al., Training compute-optimal large language models, arXiv preprint\n",
      "arXiv:2203.15556 (2022). 6, 9, 25, 29\n",
      "----\n",
      "arXiv:2203.15556 (2022). 6, 9, 25, 29\n",
      "[97] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster,\n",
      "T. Wang, Q. Liu, P. S. Koura, et al., Opt-iml: Scaling language model in-\n",
      "struction meta learning through the lens of generalization, arXiv preprint\n",
      "arXiv:2212.12017 (2022). 7, 11, 16, 17, 22, 25, 28\n",
      "[98] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, C. Gan,\n",
      "Principle-driven self-alignment of language models from scratch with\n",
      "----\n",
      "Principle-driven self-alignment of language models from scratch with\n",
      "minimal human supervision, arXiv preprint arXiv:2305.03047 (2023).\n",
      "7, 17\n",
      "[99] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones,\n",
      "N. Joseph, B. Mann, N. DasSarma, et al., A general language assistant\n",
      "as a laboratory for alignment, arXiv preprint arXiv:2112.00861 (2021).\n",
      "7\n",
      "[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\n",
      "----\n",
      "7\n",
      "[100] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei,\n",
      "P. Christiano, G. Irving, Fine-tuning language models from human pref-\n",
      "erences, arXiv preprint arXiv:1909.08593 (2019). 7\n",
      "[101] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, M. Seo, The cot collec-\n",
      "tion: Improving zero-shot and few-shot learning of language models via\n",
      "chain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\n",
      "7, 16\n",
      "----\n",
      "chain-of-thought fine-tuning, arXiv preprint arXiv:2305.14045 (2023).\n",
      "7, 16\n",
      "[102] Q. Liu, F. Zhou, Z. Jiang, L. Dou, M. Lin, From zero to hero: Exam-\n",
      "ining the power of symbolic tasks in instruction tuning, arXiv preprint\n",
      "arXiv:2304.07995 (2023). 7, 16\n",
      "[103] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\n",
      "D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large\n",
      "language models, Advances in Neural Information Processing Systems\n",
      "35 (2022) 24824–24837. 7, 20, 23\n",
      "----\n",
      "language models, Advances in Neural Information Processing Systems\n",
      "35 (2022) 24824–24837. 7, 20, 23\n",
      "[104] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowd-\n",
      "hery, D. Zhou, Self-consistency improves chain of thought reasoning in\n",
      "language models, arXiv preprint arXiv:2203.11171 (2022). 7, 20\n",
      "[105] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, K. Narasimhan,\n",
      "Tree of thoughts: Deliberate problem solving with large language mod-\n",
      "----\n",
      "Tree of thoughts: Deliberate problem solving with large language mod-\n",
      "els, arXiv preprint arXiv:2305.10601 (2023). 7, 20\n",
      "[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\n",
      "A. Gesmundo, M. Attariyan, S. Gelly, Parameter-efficient transfer learn-\n",
      "ing for nlp, in: International Conference on Machine Learning, PMLR,\n",
      "2019, pp. 2790–2799. 7, 20\n",
      "[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\n",
      "----\n",
      "[107] S. McCandlish, J. Kaplan, D. Amodei, O. D. Team, An empirical model\n",
      "of large-batch training, arXiv preprint arXiv:1812.06162 (2018). 7\n",
      "[108] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang,\n",
      "K. Wang, X. Zhang, et al., Pangu-α : Large-scale autoregressive pre-\n",
      "trained chinese language models with auto-parallel computation, arXiv\n",
      "preprint arXiv:2104.12369 (2021). 8, 23, 24, 25\n",
      "[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\n",
      "----\n",
      "[109] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y. Cen, X. Zou, Z. Yang,\n",
      "J. Tang, Wudaocorpora: A super large-scale chinese corpora for pre-\n",
      "training language models, AI Open 2 (2021) 65–68. 8, 30\n",
      "[110] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\n",
      "Y. Zhao, Y. Lu, et al., Ernie 3.0: Large-scale knowledge enhanced\n",
      "pre-training for language understanding and generation, arXiv preprint\n",
      "arXiv:2107.02137 (2021). 8, 25\n",
      "----\n",
      "arXiv:2107.02137 (2021). 8, 25\n",
      "[111] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, R. Salakhutdinov,\n",
      "Transformer-xl: Attentive language models beyond a fixed-length con-\n",
      "text, arXiv preprint arXiv:1901.02860 (2019). 8\n",
      "[112] O. Lieber, O. Sharir, B. Lenz, Y. Shoham, Jurassic-1: Technical details\n",
      "and evaluation, White Paper. AI21 Labs 1 (2021). 8, 24, 25\n",
      "[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\n",
      "----\n",
      "[113] Y. Levine, N. Wies, O. Sharir, H. Bata, A. Shashua, Limits to depth ef-\n",
      "ficiencies of self-attention, Advances in Neural Information Processing\n",
      "Systems 33 (2020) 22640–22651. 8, 11\n",
      "[114] B. Kim, H. Kim, S.-W. Lee, G. Lee, D. Kwak, D. H. Jeon, S. Park,\n",
      "S. Kim, S. Kim, D. Seo, et al., What changes can large-scale language\n",
      "models bring?\n",
      "intensive study on hyperclova: Billions-scale korean\n",
      "generative pretrained transformers, arXiv preprint arXiv:2109.04650\n",
      "(2021). 8, 25\n",
      "----\n",
      "generative pretrained transformers, arXiv preprint arXiv:2109.04650\n",
      "(2021). 8, 25\n",
      "[115] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li, H. Zhu, J. Luo,\n",
      "L. Xu, et al., Yuan 1.0: Large-scale pre-trained language model in zero-\n",
      "shot and few-shot learning, arXiv preprint arXiv:2110.04725 (2021). 8,\n",
      "24, 25\n",
      "[116] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\n",
      "J. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\n",
      "----\n",
      "J. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\n",
      "guage models: Methods, analysis & insights from training gopher, arXiv\n",
      "preprint arXiv:2112.11446 (2021). 8, 9, 25, 28\n",
      "[117] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\n",
      "J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al.,\n",
      "Using deepspeed and megatron to train megatron-turing nlg 530b, a\n",
      "large-scale generative language model, arXiv preprint arXiv:2201.11990\n",
      "(2022). 8, 9, 24, 25\n",
      "----\n",
      "large-scale generative language model, arXiv preprint arXiv:2201.11990\n",
      "(2022). 8, 9, 24, 25\n",
      "[118] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding,\n",
      "H. He, C. Leahy, K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-\n",
      "source autoregressive language model, arXiv preprint arXiv:2204.06745\n",
      "(2022). 9, 23, 24, 25\n",
      "[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\n",
      "guage model (2021). 9\n",
      "----\n",
      "[119] W. Ben, K. Aran, Gpt-j-6b: A 6 billion parameter autoregressive lan-\n",
      "guage model (2021). 9\n",
      "[120] P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia,\n",
      "B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al., Mixed pre-\n",
      "cision training, arXiv preprint arXiv:1710.03740 (2017). 9, 23\n",
      "[121] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hin-\n",
      "ton, J. Dean, Outrageously large neural networks: The sparsely-gated\n",
      "----\n",
      "ton, J. Dean, Outrageously large neural networks: The sparsely-gated\n",
      "mixture-of-experts layer, arXiv preprint arXiv:1701.06538 (2017). 9, 23\n",
      "[122] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\n",
      "H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky, et al., Alex-\n",
      "atm 20b: Few-shot learning using a large-scale multilingual seq2seq\n",
      "model, arXiv preprint arXiv:2208.01448 (2022). 9, 23, 24, 25\n",
      "[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\n",
      "----\n",
      "[123] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\n",
      "S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report,\n",
      "arXiv preprint arXiv:2305.10403 (2023). 9, 25\n",
      "[124] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Garcia,\n",
      "H. S. Zheng, J. Rao, A. Chowdhery, et al., Transcending scaling laws\n",
      "with 0.1% extra compute, arXiv preprint arXiv:2210.11399 (2022). 9,\n",
      "24, 25\n",
      "[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\n",
      "----\n",
      "24, 25\n",
      "[125] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, J. Wei, X. Wang, H. W.\n",
      "Chung, D. Bahri, T. Schuster, S. Zheng, et al., Ul2: Unifying lan-\n",
      "guage learning paradigms, in: The Eleventh International Conference\n",
      "on Learning Representations, 2022. 9, 10, 24, 25\n",
      "[126] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang, Glm: Gen-\n",
      "eral language model pretraining with autoregressive blank infilling, in:\n",
      "Proceedings of the 60th Annual Meeting of the Association for Compu-\n",
      "----\n",
      "Proceedings of the 60th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. 10\n",
      "[127] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\n",
      "T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.,\n",
      "Llama: Open and efficient foundation language models, arXiv preprint\n",
      "arXiv:2302.13971 (2023). 10, 23, 25\n",
      "[128] M. N. Rabe, C. Staats, Self-attention does not need o(n2) memory, arXiv\n",
      "preprint arXiv:2112.05682 (2021). 10\n",
      "----\n",
      "preprint arXiv:2112.05682 (2021). 10\n",
      "[129] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch,\n",
      "M. Shoeybi, B. Catanzaro, Reducing activation recomputation in large\n",
      "transformer models, Proceedings of Machine Learning and Systems 5\n",
      "(2023). 10\n",
      "[130] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\n",
      "A. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of\n",
      "models, arXiv preprint arXiv:2407.21783 (2024). 10, 25\n",
      "----\n",
      "models, arXiv preprint arXiv:2407.21783 (2024). 10, 25\n",
      "[131] https://mistral.ai/news/mixtral-8x22b/. 10, 25\n",
      "[132] https://github.com/Snowflake-Labs/snowflake-arctic. 10,\n",
      "25\n",
      "[133] https://github.com/xai-org/grok-1. 10\n",
      "[134] https://x.ai/blog/grok-1.5. 10\n",
      "[135] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,\n",
      "J. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: a family of highly\n",
      "capable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\n",
      "10\n",
      "----\n",
      "capable multimodal models, arXiv preprint arXiv:2312.11805 (2023).\n",
      "10\n",
      "[136] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b.\n",
      "38\n",
      "----\n",
      "Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gem-\n",
      "ini 1.5: Unlocking multimodal understanding across millions of tokens\n",
      "of context, arXiv preprint arXiv:2403.05530 (2024). 10\n",
      "[137] B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brun-\n",
      "dyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al., Nemotron-4 340b\n",
      "technical report, arXiv preprint arXiv:2406.11704 (2024). 10, 25\n",
      "[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\n",
      "----\n",
      "[138] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong,\n",
      "Q. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models\n",
      "with longtermism, arXiv preprint arXiv:2401.02954 (2024). 10, 25\n",
      "[139] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,\n",
      "C. Deng, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li,\n",
      "F. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu, H. Yang,\n",
      "H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\n",
      "----\n",
      "H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang,\n",
      "J. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu, J. Song, K. Dong, K. Gao,\n",
      "K. Guan, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Zhang, M. Li,\n",
      "M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang,\n",
      "P. Wang, P. Zhang, Q. Zhu, Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge,\n",
      "R. Pan, R. Xu, R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye,\n",
      "S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\n",
      "----\n",
      "S. Ma, S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,\n",
      "T. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang, W. Gao,\n",
      "W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu, X. Wang, X. Shen,\n",
      "X. Chen, X. Chen, X. Nie, X. Sun, Deepseek-v2: A strong, economical,\n",
      "and efficient mixture-of-experts language model, CoRR abs/2405.04434\n",
      "(2024). 10, 25\n",
      "[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,\n",
      "----\n",
      "(2024). 10, 25\n",
      "[140] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,\n",
      "C. Xiong, Codegen: An open large language model for code with multi-\n",
      "turn program synthesis, arXiv preprint arXiv:2203.13474 (2022). 11,\n",
      "23, 25, 28\n",
      "[141] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\n",
      "wards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\n",
      "guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\n",
      "11, 25, 29, 31\n",
      "----\n",
      "guage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\n",
      "11, 25, 29, 31\n",
      "[142] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,\n",
      "T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level\n",
      "code generation with alphacode, Science 378 (6624) (2022) 1092–1097.\n",
      "11, 23, 25, 29\n",
      "[143] N. Shazeer, Fast transformer decoding: One write-head is all you need,\n",
      "arXiv preprint arXiv:1911.02150 (2019). 11\n",
      "----\n",
      "arXiv preprint arXiv:1911.02150 (2019). 11\n",
      "[144] R. Y. Pang, H. He, Text generation by learning from demonstrations,\n",
      "arXiv preprint arXiv:2009.07839 (2020). 11\n",
      "[145] R. Dabre, A. Fujita, Softmax tempering for training neural machine\n",
      "translation models, arXiv preprint arXiv:2009.09372 (2020). 11\n",
      "[146] Y. Wang, W. Wang, S. Joty, S. C. Hoi, Codet5: Identifier-aware unified\n",
      "pre-trained encoder-decoder models for code understanding and genera-\n",
      "tion, arXiv preprint arXiv:2109.00859 (2021). 11\n",
      "----\n",
      "tion, arXiv preprint arXiv:2109.00859 (2021). 11\n",
      "[147] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,\n",
      "M. Marone, C. Akiki, J. Li, J. Chim, et al., Starcoder: may the source be\n",
      "with you!, arXiv preprint arXiv:2305.06161 (2023). 11, 25\n",
      "[148] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Saravia,\n",
      "A. Poulton, V. Kerkez, R. Stojnic, Galactica: A large language model for\n",
      "science, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\n",
      "----\n",
      "science, arXiv preprint arXiv:2211.09085 (2022). 11, 24, 25, 29\n",
      "[149] FairScale authors, Fairscale: A general purpose modular pytorch library\n",
      "for high performance and large scale training, https://github.com/\n",
      "facebookresearch/fairscale (2021). 11\n",
      "[150] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\n",
      "Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al., Lamda: Language models\n",
      "for dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\n",
      "----\n",
      "for dialog applications, arXiv preprint arXiv:2201.08239 (2022). 11, 25\n",
      "[151] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\n",
      "P. Kambadur, D. Rosenberg, G. Mann, Bloomberggpt: A large language\n",
      "model for finance, arXiv preprint arXiv:2303.17564 (2023). 11, 25, 33\n",
      "[152] X. Zhang, Q. Yang, D. Xu, Xuanyuan 2.0: A large chinese finan-\n",
      "cial chat model with hundreds of billions parameters, arXiv preprint\n",
      "arXiv:2305.12002 (2023). 11, 17, 25\n",
      "----\n",
      "arXiv:2305.12002 (2023). 11, 17, 25\n",
      "[153] W. Ben, Mesh-transformer-jax: Model-parallel implementation of trans-\n",
      "former language model with jax (2021). 12, 24\n",
      "[154] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts, S. Biderman,\n",
      "T. L. Scao, M. S. Bari, S. Shen, Z.-X. Yong, H. Schoelkopf, et al.,\n",
      "Crosslingual generalization through multitask finetuning, arXiv preprint\n",
      "arXiv:2211.01786 (2022). 16, 25, 28, 31\n",
      "[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\n",
      "----\n",
      "[155] D. Yin, X. Liu, F. Yin, M. Zhong, H. Bansal, J. Han, K.-W. Chang,\n",
      "Dynosaur: A dynamic growth paradigm for instruction-tuning data cu-\n",
      "ration, arXiv preprint arXiv:2305.14327 (2023). 16\n",
      "[156] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu,\n",
      "C. He, X. Yue, et al., Llama-adapter v2: Parameter-efficient visual in-\n",
      "struction model, arXiv preprint arXiv:2304.15010 (2023). 16, 24\n",
      "[157] Openai. gpt-4 technical report (2023). 16, 35\n",
      "----\n",
      "[157] Openai. gpt-4 technical report (2023). 16, 35\n",
      "[158] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,\n",
      "T. B. Hashimoto, Stanford alpaca:\n",
      "An instruction-following llama\n",
      "model,\n",
      "https://github.com/tatsu-lab/stanford_alpaca\n",
      "(2023). 16, 25, 28\n",
      "[159] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\n",
      "S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna: An\n",
      "open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\n",
      "2023).\n",
      "----\n",
      "open-source chatbot impressing gpt-4 with 90%* chatgpt quality (March\n",
      "2023).\n",
      "URL https://lmsys.org/blog/2023-03-30-vicuna/ 16, 22, 25,\n",
      "28\n",
      "[160] B. Peng, C. Li, P. He, M. Galley, J. Gao, Instruction tuning with gpt-4,\n",
      "arXiv preprint arXiv:2304.03277 (2023). 16, 28\n",
      "[161] T. Liu, B. K. H. Low, Goat: Fine-tuned llama outperforms gpt-4 on\n",
      "arithmetic tasks, arXiv preprint arXiv:2305.14201 (2023). 16\n",
      "[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\n",
      "----\n",
      "[162] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, T. Liu, Huatuo:\n",
      "Tuning llama model with chinese medical knowledge, arXiv preprint\n",
      "arXiv:2304.06975 (2023). 16\n",
      "[163] C. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao, D. Jiang,\n",
      "Wizardlm: Empowering large language models to follow complex in-\n",
      "structions, arXiv preprint arXiv:2304.12244 (2023). 16\n",
      "[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\n",
      "----\n",
      "[164] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,\n",
      "D. Jiang, Wizardcoder: Empowering code large language models with\n",
      "evol-instruct, arXiv preprint arXiv:2306.08568 (2023). 16, 25\n",
      "[165] J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick,\n",
      "M. Glaese, S. Young, L. Campbell-Gillingham, G. Irving, et al., Teach-\n",
      "ing language models to support answers with verified quotes, arXiv\n",
      "preprint arXiv:2203.11147 (2022). 17\n",
      "----\n",
      "preprint arXiv:2203.11147 (2022). 17\n",
      "[166] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\n",
      "C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al., Webgpt: Browser-\n",
      "assisted question-answering with human feedback, arXiv preprint\n",
      "arXiv:2112.09332 (2021). 17, 19, 20, 25, 31\n",
      "[167] A. Glaese, N. McAleese, M. Tr˛ebacz, J. Aslanides, V. Firoiu, T. Ewalds,\n",
      "M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al., Improving\n",
      "alignment of dialogue agents via targeted human judgements, arXiv\n",
      "----\n",
      "alignment of dialogue agents via targeted human judgements, arXiv\n",
      "preprint arXiv:2209.14375 (2022). 17, 20, 25\n",
      "[168] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn,\n",
      "Direct preference optimization: Your language model is secretly a re-\n",
      "ward model, arXiv preprint arXiv:2305.18290 (2023). 17\n",
      "[169] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum,\n",
      "T. Zhang, Raft: Reward ranked finetuning for generative foundation\n",
      "----\n",
      "T. Zhang, Raft: Reward ranked finetuning for generative foundation\n",
      "model alignment, arXiv preprint arXiv:2304.06767 (2023). 17\n",
      "[170] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, F. Huang, Rrhf: Rank\n",
      "responses to align language models with human feedback without tears,\n",
      "arXiv preprint arXiv:2304.05302 (2023). 17\n",
      "[171] F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, H. Wang, Preference rank-\n",
      "ing optimization for human alignment, arXiv preprint arXiv:2306.17492\n",
      "(2023). 17\n",
      "----\n",
      "ing optimization for human alignment, arXiv preprint arXiv:2306.17492\n",
      "(2023). 17\n",
      "[172] H. Liu, C. Sferrazza, P. Abbeel, Languages are rewards: Hindsight fine-\n",
      "tuning using human feedback, arXiv preprint arXiv:2302.02676 (2023).\n",
      "17\n",
      "[173] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\n",
      "A. Goldie, A. Mirhoseini, C. McKinnon, et al., Constitutional ai: Harm-\n",
      "lessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\n",
      "----\n",
      "lessness from ai feedback, arXiv preprint arXiv:2212.08073 (2022). 17\n",
      "[174] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,\n",
      "P. Liang,\n",
      "T. B. Hashimoto,\n",
      "Alpacafarm:\n",
      "A simulation frame-\n",
      "work for methods that learn from human feedback, arXiv preprint\n",
      "arXiv:2305.14387 (2023). 17\n",
      "[175] C. Si, Z. Gan, Z. Yang, S. Wang, J. Wang, J. Boyd-Graber, L. Wang,\n",
      "Prompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\n",
      "17\n",
      "----\n",
      "Prompting gpt-3 to be reliable, arXiv preprint arXiv:2210.09150 (2022).\n",
      "17\n",
      "[176] D. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯ut˙e, A. Chen,\n",
      "A. Goldie, A. Mirhoseini, C. Olsson, D. Hernandez, et al., The capac-\n",
      "ity for moral self-correction in large language models, arXiv preprint\n",
      "arXiv:2302.07459 (2023). 17\n",
      "[177] A. Wei, N. Haghtalab, J. Steinhardt, Jailbroken: How does llm safety\n",
      "training fail?, arXiv preprint arXiv:2307.02483 (2023). 17\n",
      "39\n",
      "----\n",
      "[178] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath,\n",
      "B. Mann, E. Perez, N. Schiefer, K. Ndousse, et al., Red teaming lan-\n",
      "guage models to reduce harms: Methods, scaling behaviors, and lessons\n",
      "learned, arXiv preprint arXiv:2209.07858 (2022). 17, 28\n",
      "[179] S. Casper, J. Lin, J. Kwon, G. Culp, D. Hadfield-Menell, Explore, estab-\n",
      "lish, exploit: Red teaming language models from scratch, arXiv preprint\n",
      "arXiv:2306.09442 (2023). 17\n",
      "----\n",
      "lish, exploit: Red teaming language models from scratch, arXiv preprint\n",
      "arXiv:2306.09442 (2023). 17\n",
      "[180] E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese,\n",
      "N. McAleese, G. Irving, Red teaming language models with language\n",
      "models, arXiv preprint arXiv:2202.03286 (2022). 17\n",
      "[181] T. Scialom, T. Chakrabarty, S. Muresan, Fine-tuned language models are\n",
      "continual learners, in: Proceedings of the 2022 Conference on Empirical\n",
      "----\n",
      "continual learners, in: Proceedings of the 2022 Conference on Empirical\n",
      "Methods in Natural Language Processing, 2022, pp. 6107–6122. 17\n",
      "[182] Z. Shi, A. Lipani, Don’t stop pretraining? make prompt-based fine-\n",
      "tuning powerful learner, arXiv preprint arXiv:2305.01711 (2023). 17\n",
      "[183] H. Gupta, S. A. Sawant, S. Mishra, M. Nakamura, A. Mitra, S. Mashetty,\n",
      "C. Baral, Instruction tuned models are quick learners, arXiv preprint\n",
      "arXiv:2306.05539 (2023). 17\n",
      "----\n",
      "C. Baral, Instruction tuned models are quick learners, arXiv preprint\n",
      "arXiv:2306.05539 (2023). 17\n",
      "[184] H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong,\n",
      "J. Zhao, Maybe only 0.5% data is needed: A preliminary exploration\n",
      "of low training data instruction tuning, arXiv preprint arXiv:2305.09246\n",
      "(2023). 17\n",
      "[185] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\n",
      "P. Yu, L. Yu, et al., Lima: Less is more for alignment, arXiv preprint\n",
      "arXiv:2305.11206 (2023). 17, 25, 28\n",
      "----\n",
      "arXiv:2305.11206 (2023). 17, 25, 28\n",
      "[186] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, S. Wang, Lm-infinite: Sim-\n",
      "ple on-the-fly length generalization for large language models, arXiv\n",
      "preprint arXiv:2308.16137 (2023). 17, 18\n",
      "[187] J. Ainslie, T. Lei, M. de Jong, S. Ontañón, S. Brahma, Y. Zemlyan-\n",
      "skiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay, et al., Colt5: Faster\n",
      "long-range transformers with conditional computation, arXiv preprint\n",
      "arXiv:2303.09752 (2023). 18\n",
      "----\n",
      "long-range transformers with conditional computation, arXiv preprint\n",
      "arXiv:2303.09752 (2023). 18\n",
      "[188] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang, W. Wang, F. Wei,\n",
      "Longnet: Scaling transformers to 1,000,000,000 tokens, arXiv preprint\n",
      "arXiv:2307.02486 (2023). 18\n",
      "[189] Y. Chen, S. Qian, H. Tang, X. Lai, Z. Liu, S. Han, J. Jia, Longlora: Effi-\n",
      "cient fine-tuning of long-context large language models, arXiv preprint\n",
      "arXiv:2309.12307 (2023). 18\n",
      "----\n",
      "cient fine-tuning of long-context large language models, arXiv preprint\n",
      "arXiv:2309.12307 (2023). 18\n",
      "[190] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar, O. Abend,\n",
      "E. Karpas, A. Shashua, K. Leyton-Brown, Y. Shoham, Parallel context\n",
      "windows for large language models, in: Proceedings of the 61st Annual\n",
      "Meeting of the Association for Computational Linguistics (Volume 1:\n",
      "Long Papers), 2023, pp. 6383–6402. 18\n",
      "[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\n",
      "----\n",
      "[191] W. Wang, L. Dong, H. Cheng, X. Liu, X. Yan, J. Gao, F. Wei,\n",
      "Augmenting language models with long-term memory, arXiv preprint\n",
      "arXiv:2306.07174 (2023). 18\n",
      "[192] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, S. Wang, Long\n",
      "time no see! open-domain conversation with long-term persona memory,\n",
      "arXiv preprint arXiv:2203.05797 (2022). 18\n",
      "[193] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\n",
      "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\n",
      "----\n",
      "can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, et al.,\n",
      "Improving language models by retrieving from trillions of tokens, in:\n",
      "International conference on machine learning, PMLR, 2022, pp. 2206–\n",
      "2240. 18, 19, 34\n",
      "[194] W. Zhong, L. Guo, Q. Gao, Y. Wang, Memorybank:\n",
      "Enhanc-\n",
      "ing large language models with long-term memory, arXiv preprint\n",
      "arXiv:2305.10250 (2023). 18\n",
      "[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\n",
      "----\n",
      "[195] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, S. Yao,\n",
      "Reflexion: Language agents with verbal reinforcement learning, arXiv\n",
      "preprint arXiv:2303.11366 14 (2023). 18, 20\n",
      "[196] C. Hu, J. Fu, C. Du, S. Luo, J. Zhao, H. Zhao, Chatdb: Augment-\n",
      "ing llms with databases as their symbolic memory, arXiv preprint\n",
      "arXiv:2306.03901 (2023). 18\n",
      "[197] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\n",
      "J. Callan, G. Neubig, Active retrieval augmented generation, arXiv\n",
      "----\n",
      "J. Callan, G. Neubig, Active retrieval augmented generation, arXiv\n",
      "preprint arXiv:2305.06983 (2023). 18\n",
      "[198] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\n",
      "Brown, Y. Shoham, In-context retrieval-augmented language models,\n",
      "arXiv preprint arXiv:2302.00083 (2023). 18, 34\n",
      "[199] X. Li, X. Qiu, Mot: Pre-thinking and recalling enable chatgpt to self-\n",
      "improve with memory-of-thoughts, arXiv preprint arXiv:2305.05181\n",
      "(2023). 18\n",
      "----\n",
      "improve with memory-of-thoughts, arXiv preprint arXiv:2305.05181\n",
      "(2023). 18\n",
      "[200] D. Schuurmans, Memory augmented large language models are compu-\n",
      "tationally universal, arXiv preprint arXiv:2301.04589 (2023). 18\n",
      "[201] A. Modarressi, A. Imani, M. Fayyaz, H. Schütze, Ret-llm: Towards a\n",
      "general read-write memory for large language models, arXiv preprint\n",
      "arXiv:2305.14322 (2023). 18\n",
      "[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\n",
      "----\n",
      "[202] S. Robertson, H. Zaragoza, et al., The probabilistic relevance frame-\n",
      "work: Bm25 and beyond, Foundations and Trends® in Information Re-\n",
      "trieval 3 (4) (2009) 333–389. 18\n",
      "[203] X. Wang,\n",
      "J. Wei,\n",
      "D. Schuurmans,\n",
      "Q. Le,\n",
      "E. Chi,\n",
      "D. Zhou,\n",
      "Rationale-augmented ensembles in language models, arXiv preprint\n",
      "arXiv:2207.00747 (2022). 18\n",
      "[204] F. Zhang, B. Chen, Y. Zhang, J. Liu, D. Zan, Y. Mao, J.-G. Lou, W. Chen,\n",
      "Repocoder: Repository-level code completion through iterative retrieval\n",
      "----\n",
      "Repocoder: Repository-level code completion through iterative retrieval\n",
      "and generation, arXiv preprint arXiv:2303.12570 (2023). 18\n",
      "[205] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\n",
      "O. Kuchaiev, B. Li, C. Xiao, et al., Shall we pretrain autoregressive\n",
      "language models with retrieval? a comprehensive study, arXiv preprint\n",
      "arXiv:2304.06762 (2023). 19\n",
      "[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\n",
      "----\n",
      "[206] L. Wang, N. Yang, F. Wei, Learning to retrieve in-context examples for\n",
      "large language models, arXiv preprint arXiv:2307.07164 (2023). 19\n",
      "[207] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What makes\n",
      "good in-context examples for gpt-3?, arXiv preprint arXiv:2101.06804\n",
      "(2021). 19\n",
      "[208] O. Rubin, J. Herzig, J. Berant, Learning to retrieve prompts for in-\n",
      "context learning, arXiv preprint arXiv:2112.08633 (2021). 19\n",
      "----\n",
      "context learning, arXiv preprint arXiv:2112.08633 (2021). 19\n",
      "[209] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\n",
      "moyer, W.-t. Yih, Replug: Retrieval-augmented black-box language\n",
      "models, arXiv preprint arXiv:2301.12652 (2023). 19\n",
      "[210] O. Rubin, J. Berant, Long-range language modeling with self-retrieval,\n",
      "arXiv preprint arXiv:2306.13421 (2023). 19\n",
      "[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\n",
      "----\n",
      "[211] K. Guu, K. Lee, Z. Tung, P. Pasupat, M. Chang, Retrieval augmented\n",
      "language model pre-training, in: International conference on machine\n",
      "learning, PMLR, 2020, pp. 3929–3938. 19\n",
      "[212] S. Hofstätter, J. Chen, K. Raman, H. Zamani, Fid-light: Efficient and ef-\n",
      "fective retrieval-augmented text generation, in: Proceedings of the 46th\n",
      "International ACM SIGIR Conference on Research and Development in\n",
      "Information Retrieval, 2023, pp. 1437–1447. 19\n",
      "----\n",
      "Information Retrieval, 2023, pp. 1437–1447. 19\n",
      "[213] M. Komeili, K. Shuster, J. Weston, Internet-augmented dialogue gener-\n",
      "ation, arXiv preprint arXiv:2107.07566 (2021). 19\n",
      "[214] A. Lazaridou, E. Gribovskaya, W. Stokowiec, N. Grigorev, Internet-\n",
      "augmented language models through few-shot prompting for open-\n",
      "domain question answering, arXiv preprint arXiv:2203.05115 (2022).\n",
      "19\n",
      "[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\n",
      "----\n",
      "19\n",
      "[215] D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, M. Z. Shou, Assist-\n",
      "gpt: A general multi-modal assistant that can plan, execute, inspect, and\n",
      "learn, arXiv preprint arXiv:2306.08640 (2023). 19\n",
      "[216] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C. Zhu,\n",
      "J. Gao, Chameleon: Plug-and-play compositional reasoning with large\n",
      "language models, arXiv preprint arXiv:2304.09842 (2023). 19, 20, 23\n",
      "[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\n",
      "----\n",
      "[217] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, M. T.\n",
      "Ribeiro, Art: Automatic multi-step reasoning and tool-use for large lan-\n",
      "guage models, arXiv preprint arXiv:2303.09014 (2023). 19\n",
      "[218] C.-Y. Hsieh, S.-A. Chen, C.-L. Li, Y. Fujii, A. Ratner, C.-Y. Lee, R. Kr-\n",
      "ishna, T. Pfister, Tool documentation enables zero-shot tool-usage with\n",
      "large language models, arXiv preprint arXiv:2308.00675 (2023). 19\n",
      "----\n",
      "large language models, arXiv preprint arXiv:2308.00675 (2023). 19\n",
      "[219] Y. Song, W. Xiong, D. Zhu, C. Li, K. Wang, Y. Tian, S. Li, Restgpt:\n",
      "Connecting large language models with real-world applications via rest-\n",
      "ful apis, arXiv preprint arXiv:2306.06624 (2023). 19\n",
      "[220] S. Hao, T. Liu, Z. Wang, Z. Hu, Toolkengpt: Augmenting frozen lan-\n",
      "guage models with massive tools via tool embeddings, arXiv preprint\n",
      "arXiv:2305.11554 (2023). 19\n",
      "----\n",
      "guage models with massive tools via tool embeddings, arXiv preprint\n",
      "arXiv:2305.11554 (2023). 19\n",
      "[221] S. G. Patil, T. Zhang, X. Wang, J. E. Gonzalez, Gorilla: Large language\n",
      "model connected with massive apis, arXiv preprint arXiv:2305.15334\n",
      "(2023). 19\n",
      "[222] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, J. Zhang, On the tool manipu-\n",
      "lation capability of open-source large language models, arXiv preprint\n",
      "arXiv:2305.16504 (2023). 19\n",
      "----\n",
      "lation capability of open-source large language models, arXiv preprint\n",
      "arXiv:2305.16504 (2023). 19\n",
      "[223] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,\n",
      "B. Qian, et al., Toolllm: Facilitating large language models to master\n",
      "16000+ real-world apis, arXiv preprint arXiv:2307.16789 (2023). 19,\n",
      "40\n",
      "----\n",
      "20\n",
      "[224] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt: Solv-\n",
      "ing ai tasks with chatgpt and its friends in huggingface, arXiv preprint\n",
      "arXiv:2303.17580 (2023). 19, 20, 33\n",
      "[225] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou, S. Lu, L. Ji,\n",
      "S. Mao, et al., Taskmatrix. ai: Completing tasks by connecting foun-\n",
      "dation models with millions of apis, arXiv preprint arXiv:2303.16434\n",
      "(2023). 19\n",
      "[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\n",
      "----\n",
      "(2023). 19\n",
      "[226] D. Surís, S. Menon, C. Vondrick, Vipergpt: Visual inference via python\n",
      "execution for reasoning, arXiv preprint arXiv:2303.08128 (2023). 20\n",
      "[227] A. Maedche, S. Morana, S. Schacht, D. Werth, J. Krumeich, Advanced\n",
      "user assistance systems, Business & Information Systems Engineering\n",
      "58 (2016) 367–370. 20\n",
      "[228] M. Campbell, A. J. Hoane Jr, F.-h. Hsu, Deep blue, Artificial intelligence\n",
      "134 (1-2) (2002) 57–83. 20\n",
      "[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\n",
      "----\n",
      "134 (1-2) (2002) 57–83. 20\n",
      "[229] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang,\n",
      "S. K. S. Yau, Z. Lin, L. Zhou, et al., Metagpt: Meta programming for\n",
      "multi-agent collaborative framework, arXiv preprint arXiv:2308.00352\n",
      "(2023). 20\n",
      "[230] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\n",
      "S. Jin, E. Zhou, et al., The rise and potential of large language model\n",
      "based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\n",
      "----\n",
      "based agents: A survey, arXiv preprint arXiv:2309.07864 (2023). 20\n",
      "[231] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\n",
      "X. Chen, Y. Lin, et al., A survey on large language model based au-\n",
      "tonomous agents, arXiv preprint arXiv:2308.11432 (2023). 20\n",
      "[232] W. Huang, P. Abbeel, D. Pathak, I. Mordatch, Language models as zero-\n",
      "shot planners: Extracting actionable knowledge for embodied agents,\n",
      "in: International Conference on Machine Learning, PMLR, 2022, pp.\n",
      "9118–9147. 20\n",
      "----\n",
      "in: International Conference on Machine Learning, PMLR, 2022, pp.\n",
      "9118–9147. 20\n",
      "[233] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, Z. Hu, Reason-\n",
      "ing with language model is planning with world model, arXiv preprint\n",
      "arXiv:2305.14992 (2023). 20, 33\n",
      "[234] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. Murthy,\n",
      "Z. Chen, J. Zhang, D. Arpit, et al., Retroformer:\n",
      "Retrospective\n",
      "large language agents with policy gradient optimization, arXiv preprint\n",
      "----\n",
      "Retrospective\n",
      "large language agents with policy gradient optimization, arXiv preprint\n",
      "arXiv:2308.02151 (2023). 20, 33\n",
      "[235] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\n",
      "J. Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, T. Jackson,\n",
      "N. Brown, L. Luu, S. Levine, K. Hausman, brian ichter, Inner mono-\n",
      "logue: Embodied reasoning through planning with language models, in:\n",
      "6th Annual Conference on Robot Learning, 2022.\n",
      "URL https://openreview.net/forum?id=3R3Pz5i0tye 20\n",
      "----\n",
      "6th Annual Conference on Robot Learning, 2022.\n",
      "URL https://openreview.net/forum?id=3R3Pz5i0tye 20\n",
      "[236] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, J. Fu, Alphablock:\n",
      "Embodied finetuning for vision-language reasoning in robot manipula-\n",
      "tion, arXiv preprint arXiv:2305.18898 (2023). 20, 33\n",
      "[237] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\n",
      "J. Thomason, A. Garg, Progprompt: Generating situated robot task plans\n",
      "----\n",
      "J. Thomason, A. Garg, Progprompt: Generating situated robot task plans\n",
      "using large language models, in: 2023 IEEE International Conference on\n",
      "Robotics and Automation (ICRA), IEEE, 2023, pp. 11523–11530. 20,\n",
      "33\n",
      "[238] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.-T. L.\n",
      "Chiang, T. Erez, L. Hasenclever, J. Humplik, et al., Language to rewards\n",
      "for robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\n",
      "----\n",
      "for robotic skill synthesis, arXiv preprint arXiv:2306.08647 (2023). 20\n",
      "[239] X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, M. Gerstein,\n",
      "Medagents: Large language models as collaborators for zero-shot med-\n",
      "ical reasoning, arXiv preprint arXiv:2311.10537 (2023). 20\n",
      "[240] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\n",
      "J. Ibarz, A. Irpan, E. Jang, R. Julian, et al., Do as i can, not as i say:\n",
      "Grounding language in robotic affordances, in: Conference on Robot\n",
      "----\n",
      "Grounding language in robotic affordances, in: Conference on Robot\n",
      "Learning, PMLR, 2023, pp. 287–318. 20, 33\n",
      "[241] H. Ha, P. Florence, S. Song, Scaling up and distilling down: Language-\n",
      "guided robot skill acquisition, arXiv preprint arXiv:2307.14535 (2023).\n",
      "20\n",
      "[242] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, A. Velasquez, Say-\n",
      "nav: Grounding large language models for dynamic planning to navi-\n",
      "gation in new environments, arXiv preprint arXiv:2309.04077 (2023).\n",
      "20\n",
      "----\n",
      "gation in new environments, arXiv preprint arXiv:2309.04077 (2023).\n",
      "20\n",
      "[243] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su,\n",
      "Llm-planner: Few-shot grounded planning for embodied agents with\n",
      "large language models, arXiv preprint arXiv:2212.04088 (2022). 20\n",
      "[244] V. S. Dorbala, J. F. Mullen Jr, D. Manocha, Can an embodied agent find\n",
      "your\" cat-shaped mug\"? llm-based zero-shot object navigation, arXiv\n",
      "preprint arXiv:2303.03480 (2023). 20\n",
      "----\n",
      "preprint arXiv:2303.03480 (2023). 20\n",
      "[245] C. Huang, O. Mees, A. Zeng, W. Burgard, Visual language maps for\n",
      "robot navigation, in: 2023 IEEE International Conference on Robotics\n",
      "and Automation (ICRA), IEEE, 2023, pp. 10608–10615. 20\n",
      "[246] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Task and motion planning\n",
      "with large language models for object rearrangement, arXiv preprint\n",
      "arXiv:2303.06247 (2023). 20, 33\n",
      "[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\n",
      "----\n",
      "[247] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt under-\n",
      "stands, too, arXiv preprint arXiv:2103.10385 (2021). 20, 21\n",
      "[248] G. Chen, F. Liu, Z. Meng, S. Liang, Revisiting parameter-efficient tun-\n",
      "ing: Are we really there yet?, arXiv preprint arXiv:2202.07962 (2022).\n",
      "20\n",
      "[249] Y. Wang, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah, J. Gao,\n",
      "Adamix: Mixture-of-adapter for parameter-efficient tuning of large lan-\n",
      "guage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\n",
      "----\n",
      "guage models, arXiv preprint arXiv:2205.12410 1 (2) (2022) 4. 20\n",
      "[250] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\n",
      "W. Chen, Lora: Low-rank adaptation of large language models, arXiv\n",
      "preprint arXiv:2106.09685 (2021). 21, 22, 23\n",
      "[251] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, J. Tang, P-tuning: Prompt\n",
      "tuning can be comparable to fine-tuning across scales and tasks, in: Pro-\n",
      "ceedings of the 60th Annual Meeting of the Association for Computa-\n",
      "----\n",
      "ceedings of the 60th Annual Meeting of the Association for Computa-\n",
      "tional Linguistics (Volume 2: Short Papers), 2022, pp. 61–68. 21\n",
      "[252] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, A. Almahairi,\n",
      "Progressive prompts: Continual learning for language models, arXiv\n",
      "preprint arXiv:2301.12314 (2023). 21\n",
      "[253] Z.-R. Zhang, C. Tan, H. Xu, C. Wang, J. Huang, S. Huang, To-\n",
      "wards adaptive prefix tuning for parameter-efficient language model\n",
      "----\n",
      "wards adaptive prefix tuning for parameter-efficient language model\n",
      "fine-tuning, arXiv preprint arXiv:2305.15212 (2023). 21\n",
      "[254] E. B. Zaken, S. Ravfogel, Y. Goldberg, Bitfit:\n",
      "Simple parameter-\n",
      "efficient fine-tuning for transformer-based masked language-models,\n",
      "arXiv preprint arXiv:2106.10199 (2021). 21\n",
      "[255] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8 ():\n",
      "8-bit matrix multiplication for transformers at scale, arXiv preprint\n",
      "arXiv:2208.07339 (2022). 21, 22\n",
      "----\n",
      "arXiv:2208.07339 (2022). 21, 22\n",
      "[256] E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, Gptq:\n",
      "Accurate\n",
      "post-training quantization for generative pre-trained transformers, arXiv\n",
      "preprint arXiv:2210.17323 (2022). 21\n",
      "[257] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, X. Liu, Outlier sup-\n",
      "pression+: Accurate quantization of large language models by equiva-\n",
      "lent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\n",
      "(2023). 21\n",
      "----\n",
      "lent and optimal shifting and scaling, arXiv preprint arXiv:2304.09145\n",
      "(2023). 21\n",
      "[258] E. Frantar, D. Alistarh, Optimal brain compression: A framework for\n",
      "accurate post-training quantization and pruning, Advances in Neural In-\n",
      "formation Processing Systems 35 (2022) 4475–4488. 21\n",
      "[259] C. Lee, J. Jin, T. Kim, H. Kim, E. Park, Owq: Lessons learned from ac-\n",
      "tivation outliers for weight quantization in large language models, arXiv\n",
      "preprint arXiv:2306.02272 (2023). 21\n",
      "----\n",
      "preprint arXiv:2306.02272 (2023). 21\n",
      "[260] S. J. Kwon, J. Kim, J. Bae, K. M. Yoo, J.-H. Kim, B. Park, B. Kim, J.-\n",
      "W. Ha, N. Sung, D. Lee, Alphatuning: Quantization-aware parameter-\n",
      "efficient adaptation of large-scale pre-trained language models, arXiv\n",
      "preprint arXiv:2210.03858 (2022). 21\n",
      "[261] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient\n",
      "finetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\n",
      "21, 22\n",
      "----\n",
      "finetuning of quantized llms, arXiv preprint arXiv:2305.14314 (2023).\n",
      "21, 22\n",
      "[262] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Kr-\n",
      "ishnamoorthi, V. Chandra, Llm-qat: Data-free quantization aware train-\n",
      "ing for large language models, arXiv preprint arXiv:2305.17888 (2023).\n",
      "21, 22\n",
      "[263] Y. Guo, A. Yao, H. Zhao, Y. Chen, Network sketching: Exploiting bi-\n",
      "nary structure in deep cnns, in: Proceedings of the IEEE Conference on\n",
      "----\n",
      "nary structure in deep cnns, in: Proceedings of the IEEE Conference on\n",
      "Computer Vision and Pattern Recognition, 2017, pp. 5955–5963. 21\n",
      "[264] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, D. Lee,\n",
      "Memory-efficient fine-tuning of compressed large language models via\n",
      "sub-4-bit integer quantization, arXiv preprint arXiv:2305.14152 (2023).\n",
      "22\n",
      "[265] M. Sun, Z. Liu, A. Bair, J. Z. Kolter, A simple and effective pruning\n",
      "approach for large language models, arXiv preprint arXiv:2306.11695\n",
      "----\n",
      "approach for large language models, arXiv preprint arXiv:2306.11695\n",
      "(2023). 22\n",
      "[266] Z. Wang, J. Wohlwend, T. Lei, Structured pruning of large language\n",
      "models, arXiv preprint arXiv:1910.04732 (2019). 22\n",
      "41\n",
      "----\n",
      "[267] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, M. Pechenizkiy,\n",
      "Y. Liang, Z. Wang, S. Liu, Outlier weighed layerwise sparsity (owl): A\n",
      "missing secret sauce for pruning llms to high sparsity, arXiv preprint\n",
      "arXiv:2310.05175 (2023). 22\n",
      "[268] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, N. Wong,\n",
      "Structured pruning for efficient generative pre-trained language models,\n",
      "in: Findings of the Association for Computational Linguistics: ACL\n",
      "2023, 2023, pp. 10880–10895. 22\n",
      "----\n",
      "in: Findings of the Association for Computational Linguistics: ACL\n",
      "2023, 2023, pp. 10880–10895. 22\n",
      "[269] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,\n",
      "A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual lan-\n",
      "guage model for few-shot learning, Advances in Neural Information Pro-\n",
      "cessing Systems 35 (2022) 23716–23736. 22\n",
      "[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\n",
      "----\n",
      "[270] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image\n",
      "pre-training with frozen image encoders and large language models,\n",
      "arXiv preprint arXiv:2301.12597 (2023). 22\n",
      "[271] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning, arXiv preprint\n",
      "arXiv:2304.08485 (2023). 22\n",
      "[272] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang,\n",
      "Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\n",
      "arXiv:2305.06355 (2023). 22\n",
      "----\n",
      "Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint\n",
      "arXiv:2305.06355 (2023). 22\n",
      "[273] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt: Towards de-\n",
      "tailed video understanding via large vision and language models, arXiv\n",
      "preprint arXiv:2306.05424 (2023). 22\n",
      "[274] H. Zhang,\n",
      "X. Li,\n",
      "L. Bing,\n",
      "Video-llama:\n",
      "An instruction-tuned\n",
      "audio-visual language model for video understanding, arXiv preprint\n",
      "arXiv:2306.02858 (2023). 22\n",
      "----\n",
      "audio-visual language model for video understanding, arXiv preprint\n",
      "arXiv:2306.02858 (2023). 22\n",
      "[275] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley,\n",
      "Y. Zou, W. Wang, Wavcaps: A chatgpt-assisted weakly-labelled au-\n",
      "dio captioning dataset for audio-language multimodal research, arXiv\n",
      "preprint arXiv:2303.17395 (2023). 22\n",
      "[276] C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, Z. Tu, Macaw-\n",
      "llm: Multi-modal language modeling with image, audio, video, and text\n",
      "----\n",
      "llm: Multi-modal language modeling with image, audio, video, and text\n",
      "integration, arXiv preprint arXiv:2306.09093 (2023). 22\n",
      "[277] D. Zhu, J. Chen, X. Shen, X. Li, M. Elhoseiny, Minigpt-4: Enhancing\n",
      "vision-language understanding with advanced large language models,\n",
      "arXiv preprint arXiv:2304.10592 (2023). 22\n",
      "[278] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\n",
      "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\n",
      "----\n",
      "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\n",
      "An image is worth 16x16 words: Transformers for image recognition at\n",
      "scale, arXiv preprint arXiv:2010.11929 (2020). 22\n",
      "[279] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung,\n",
      "S. Hoi, Instructblip: Towards general-purpose vision-language models\n",
      "with instruction tuning, arXiv preprint arXiv:2305.06500 (2023). 22\n",
      "[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\n",
      "----\n",
      "[280] Z. Xu, Y. Shen, L. Huang, Multiinstruct: Improving multi-modal zero-\n",
      "shot learning via instruction tuning, arXiv preprint arXiv:2212.10773\n",
      "(2022). 22\n",
      "[281] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, J. Liu,\n",
      "Chatbridge: Bridging modalities with large language model as a lan-\n",
      "guage catalyst, arXiv preprint arXiv:2305.16103 (2023). 22\n",
      "[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,\n",
      "----\n",
      "[282] L. Li, Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu,\n",
      "X. Sun, et al., M3 it: A large-scale dataset towards multi-modal multi-\n",
      "lingual instruction tuning, arXiv preprint arXiv:2306.04387 (2023). 22\n",
      "[283] R. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao, J. Han,\n",
      "H. Xu, L. K. T. Zhang, Detgpt: Detect what you need via reasoning,\n",
      "arXiv preprint arXiv:2305.14167 (2023). 22\n",
      "[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\n",
      "----\n",
      "[284] G. Luo, Y. Zhou, T. Ren, S. Chen, X. Sun, R. Ji, Cheap and quick:\n",
      "Efficient vision-language instruction tuning for large language models,\n",
      "arXiv preprint arXiv:2305.15023 (2023). 22\n",
      "[285] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, Y. Qiao,\n",
      "Llama-adapter: Efficient fine-tuning of language models with zero-init\n",
      "attention, arXiv preprint arXiv:2303.16199 (2023). 22\n",
      "[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\n",
      "----\n",
      "[286] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, I. Sutskever,\n",
      "Robust speech recognition via large-scale weak supervision, in: Inter-\n",
      "national Conference on Machine Learning, PMLR, 2023, pp. 28492–\n",
      "28518. 22\n",
      "[287] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, A. Smola, Multi-\n",
      "modal chain-of-thought reasoning in language models, arXiv preprint\n",
      "arXiv:2302.00923 (2023). 23\n",
      "[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt\n",
      "----\n",
      "[288] J. Ge, H. Luo, S. Qian, Y. Gan, J. Fu, S. Zhan, Chain of thought prompt\n",
      "tuning in vision language models, arXiv preprint arXiv:2304.07919\n",
      "(2023). 23\n",
      "[289] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, N. Duan, Visual chatgpt: Talk-\n",
      "ing, drawing and editing with visual foundation models, arXiv preprint\n",
      "arXiv:2303.04671 (2023). 23\n",
      "[290] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,\n",
      "M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\n",
      "----\n",
      "M. Zeng, L. Wang, Mm-react: Prompting chatgpt for multimodal rea-\n",
      "soning and action, arXiv preprint arXiv:2303.11381 (2023). 23\n",
      "[291] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao,\n",
      "S. Zhao, Y. Shan, et al., Caption anything: Interactive image descrip-\n",
      "tion with diverse multimodal controls, arXiv preprint arXiv:2305.02677\n",
      "(2023). 23\n",
      "[292] X. Zhu, R. Zhang, B. He, Z. Zeng, S. Zhang, P. Gao, Pointclip v2:\n",
      "Adapting clip for powerful 3d open-world learning, arXiv preprint\n",
      "----\n",
      "Adapting clip for powerful 3d open-world learning, arXiv preprint\n",
      "arXiv:2211.11682 (2022). 23\n",
      "[293] T. Gupta, A. Kembhavi, Visual programming: Compositional visual rea-\n",
      "soning without training, in: Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.\n",
      "23\n",
      "[294] P. Gao, Z. Jiang, H. You, P. Lu, S. C. Hoi, X. Wang, H. Li, Dynamic\n",
      "fusion with intra-and inter-modality attention flow for visual question\n",
      "----\n",
      "fusion with intra-and inter-modality attention flow for visual question\n",
      "answering, in: Proceedings of the IEEE/CVF conference on computer\n",
      "vision and pattern recognition, 2019, pp. 6639–6648. 23\n",
      "[295] Z. Yu, J. Yu, Y. Cui, D. Tao, Q. Tian, Deep modular co-attention net-\n",
      "works for visual question answering, in: Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition, 2019, pp. 6281–\n",
      "6290. 23\n",
      "[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\n",
      "----\n",
      "6290. 23\n",
      "[296] H. You, R. Sun, Z. Wang, L. Chen, G. Wang, H. A. Ayyubi, K.-\n",
      "W. Chang, S.-F. Chang, Idealgpt:\n",
      "Iteratively decomposing vision\n",
      "and language reasoning via large language models, arXiv preprint\n",
      "arXiv:2305.14985 (2023). 23\n",
      "[297] R. Zhang, X. Hu, B. Li, S. Huang, H. Deng, Y. Qiao, P. Gao, H. Li,\n",
      "Prompt, generate, then cache: Cascade of foundation models makes\n",
      "strong few-shot learners, in: Proceedings of the IEEE/CVF Conference\n",
      "----\n",
      "strong few-shot learners, in: Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, 2023, pp. 15211–15222.\n",
      "23\n",
      "[298] T. Q. Nguyen, J. Salazar, Transformers without tears: Improving the\n",
      "normalization of self-attention, CoRR abs/1910.05895 (2019). 24\n",
      "[299] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\n",
      "L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-\n",
      "training approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\n",
      "----\n",
      "training approach, arXiv preprint arXiv:1907.11692 (2019). 24, 30\n",
      "[300] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\n",
      "D. Song, Koala: A dialogue model for academic research, Blog post\n",
      "(April 2023).\n",
      "URL\n",
      "https://bair.berkeley.edu/blog/2023/04/03/koala/\n",
      "25\n",
      "[301] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,\n",
      "J. Phang, H. He, A. Thite, N. Nabeshima, et al., The pile:\n",
      "An\n",
      "800gb dataset of diverse text for language modeling, arXiv preprint\n",
      "----\n",
      "An\n",
      "800gb dataset of diverse text for language modeling, arXiv preprint\n",
      "arXiv:2101.00027 (2020). 28, 30\n",
      "[302] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,\n",
      "T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen,\n",
      "et al., The bigscience roots corpus: A 1.6 tb composite multilingual\n",
      "dataset, Advances in Neural Information Processing Systems 35 (2022)\n",
      "31809–31826. 28\n",
      "[303] Wikipedia.\n",
      "URL https://en.wikipedia.org/wiki/Main_Page 28\n",
      "----\n",
      "31809–31826. 28\n",
      "[303] Wikipedia.\n",
      "URL https://en.wikipedia.org/wiki/Main_Page 28\n",
      "[304] Together Computer, Redpajama: An open source recipe to reproduce\n",
      "llama training dataset (Apr. 2023).\n",
      "URL\n",
      "https://github.com/togethercomputer/\n",
      "RedPajama-Data 28\n",
      "[305] O. Honovich, T. Scialom, O. Levy, T. Schick, Unnatural instructions:\n",
      "Tuning language models with (almost) no human labor, arXiv preprint\n",
      "arXiv:2212.09689 (2022). 28\n",
      "[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\n",
      "----\n",
      "arXiv:2212.09689 (2022). 28\n",
      "[306] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,\n",
      "D. Drain, S. Fort, D. Ganguli, T. Henighan, et al., Training a helpful and\n",
      "harmless assistant with reinforcement learning from human feedback,\n",
      "arXiv preprint arXiv:2204.05862 (2022). 28\n",
      "[307] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song,\n",
      "J. Steinhardt, Measuring massive multitask language understanding,\n",
      "arXiv preprint arXiv:2009.03300 (2020). 26, 29\n",
      "----\n",
      "arXiv preprint arXiv:2009.03300 (2020). 26, 29\n",
      "[308] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\n",
      "A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al., Beyond\n",
      "42\n",
      "----\n",
      "the imitation game: Quantifying and extrapolating the capabilities of\n",
      "language models, arXiv preprint arXiv:2206.04615 (2022). 26, 29\n",
      "[309] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue:\n",
      "A multi-task benchmark and analysis platform for natural language un-\n",
      "derstanding, arXiv preprint arXiv:1804.07461 (2018). 26, 29\n",
      "[310] Y. Yao, Q. Dong, J. Guan, B. Cao, Z. Zhang, C. Xiao, X. Wang, F. Qi,\n",
      "J. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\n",
      "----\n",
      "J. Bao, J. Nie, et al., Cuge: A chinese language understanding and gen-\n",
      "eration evaluation benchmark, arXiv preprint arXiv:2112.13610 (2021).\n",
      "29\n",
      "[311] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu,\n",
      "C. Yu, et al., Clue: A chinese language understanding evaluation bench-\n",
      "mark, arXiv preprint arXiv:2004.05986 (2020). 29\n",
      "[312] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan,\n",
      "X. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\n",
      "----\n",
      "X. Tian, L. Qin, et al., Fewclue: A chinese few-shot learning evaluation\n",
      "benchmark, arXiv preprint arXiv:2107.07498 (2021). 29\n",
      "[313] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y.-L. Boureau, Can\n",
      "you put it all together: Evaluating conversational agents’ ability to blend\n",
      "skills, arXiv preprint arXiv:2004.08449 (2020). 29\n",
      "[314] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga,\n",
      "Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of\n",
      "----\n",
      "Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al., Holistic evaluation of\n",
      "language models, arXiv preprint arXiv:2211.09110 (2022). 29\n",
      "[315] S. Park, J. Moon, S. Kim, W. I. Cho, J. Han, J. Park, C. Song, J. Kim,\n",
      "Y. Song, T. Oh, et al., Klue: Korean language understanding evaluation,\n",
      "arXiv preprint arXiv:2105.09680 (2021). 29\n",
      "[316] S. Reddy, D. Chen, C. D. Manning, Coqa: A conversational question\n",
      "answering challenge, Transactions of the Association for Computational\n",
      "----\n",
      "answering challenge, Transactions of the Association for Computational\n",
      "Linguistics 7 (2019) 249–266. 27, 29\n",
      "[317] M.\n",
      "T.\n",
      "Pilehvar,\n",
      "J.\n",
      "Camacho-Collados,\n",
      "Wic:\n",
      "10,000\n",
      "example\n",
      "pairs for evaluating context-sensitive representations, arXiv preprint\n",
      "arXiv:1808.09121 6 (2018). 27, 29\n",
      "[318] S. Merity, C. Xiong, J. Bradbury, R. Socher, Pointer sentinel mixture\n",
      "models, arXiv preprint arXiv:1609.07843 (2016). 28, 29\n",
      "[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\n",
      "----\n",
      "[319] J. W. Rae, A. Potapenko, S. M. Jayakumar, T. P. Lillicrap, Compres-\n",
      "sive transformers for long-range sequence modelling, arXiv preprint\n",
      "arXiv:1911.05507 (2019). 28, 29\n",
      "[320] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, B. Tang, Lcqmc: A\n",
      "large-scale chinese question matching corpus, in: Proceedings of the\n",
      "27th international conference on computational linguistics, 2018, pp.\n",
      "1952–1962. 28, 29\n",
      "[321] S.\n",
      "Iyer,\n",
      "N.\n",
      "Dandekar,\n",
      "K.\n",
      "Csernai,\n",
      "First\n",
      "quora\n",
      "dataset\n",
      "re-\n",
      "lease:\n",
      "Question\n",
      "pairs,\n",
      "----\n",
      "[321] S.\n",
      "Iyer,\n",
      "N.\n",
      "Dandekar,\n",
      "K.\n",
      "Csernai,\n",
      "First\n",
      "quora\n",
      "dataset\n",
      "re-\n",
      "lease:\n",
      "Question\n",
      "pairs,\n",
      "https://quoradata.quora.com/\n",
      "First-Quora-Dataset-Release-Question-Pairs. 29\n",
      "[322] R. Rudinger, J. Naradowsky, B. Leonard, B. Van Durme, Gender bias in\n",
      "coreference resolution, arXiv preprint arXiv:1804.09301 (2018). 29\n",
      "[323] M.-C. De Marneffe, M. Simons, J. Tonhauser, The commitmentbank: In-\n",
      "vestigating projection in naturally occurring discourse, in: proceedings\n",
      "----\n",
      "vestigating projection in naturally occurring discourse, in: proceedings\n",
      "of Sinn und Bedeutung, Vol. 23, 2019, pp. 107–124. 29\n",
      "[324] Z. Li, N. Ding, Z. Liu, H. Zheng, Y. Shen, Chinese relation extraction\n",
      "with multi-grained information and external linguistic knowledge, in:\n",
      "Proceedings of the 57th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics, 2019, pp. 4377–4386. 29\n",
      "[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\n",
      "----\n",
      "[325] J. Xu, J. Wen, X. Sun, Q. Su, A discourse-level named entity recognition\n",
      "and relation extraction dataset for chinese literature text, arXiv preprint\n",
      "arXiv:1711.07010 (2017). 29\n",
      "[326] J. Chen, Q. Chen, X. Liu, H. Yang, D. Lu, B. Tang, The bq corpus: A\n",
      "large-scale domain-specific chinese corpus for sentence semantic equiv-\n",
      "alence identification, in: Proceedings of the 2018 conference on empiri-\n",
      "cal methods in natural language processing, 2018, pp. 4946–4951. 29\n",
      "----\n",
      "cal methods in natural language processing, 2018, pp. 4946–4951. 29\n",
      "[327] B. Liu, D. Niu, H. Wei, J. Lin, Y. He, K. Lai, Y. Xu, Matching arti-\n",
      "cle pairs with graphical decomposition and convolutions, arXiv preprint\n",
      "arXiv:1802.07459 (2018). 29\n",
      "[328] P. Li, W. Li, Z. He, X. Wang, Y. Cao, J. Zhou, W. Xu, Dataset and neu-\n",
      "ral recurrent sequence labeling model for open-domain factoid question\n",
      "answering, arXiv preprint arXiv:1607.06275 (2016). 29\n",
      "----\n",
      "answering, arXiv preprint arXiv:1607.06275 (2016). 29\n",
      "[329] N. Peng, M. Dredze, Named entity recognition for chinese social media\n",
      "with jointly trained embeddings, in: Proceedings of the 2015 conference\n",
      "on empirical methods in natural language processing, 2015, pp. 548–\n",
      "554. 29\n",
      "[330] W. Ling, D. Yogatama, C. Dyer, P. Blunsom, Program induction by ratio-\n",
      "nale generation: Learning to solve and explain algebraic word problems,\n",
      "arXiv preprint arXiv:1705.04146 (2017). 29\n",
      "----\n",
      "arXiv preprint arXiv:1705.04146 (2017). 29\n",
      "[331] R. Weischedel, S. Pradhan, L. Ramshaw, M. Palmer, N. Xue, M. Mar-\n",
      "cus, A. Taylor, C. Greenberg, E. Hovy, R. Belvin, et al., Ontonotes re-\n",
      "lease 4.0, LDC2011T03, Philadelphia, Penn.: Linguistic Data Consor-\n",
      "tium (2011). 29\n",
      "[332] D. Vilares, C. Gómez-Rodríguez, Head-qa: A healthcare dataset for\n",
      "complex reasoning, arXiv preprint arXiv:1906.04701 (2019). 29\n",
      "[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\n",
      "----\n",
      "[333] S. L. Blodgett, L. Green, B. O’Connor, Demographic dialectal variation\n",
      "in social media: A case study of african-american english, arXiv preprint\n",
      "arXiv:1608.08868 (2016). 29\n",
      "[334] N. Mostafazadeh, N. Chambers, X. He, D. Parikh, D. Batra, L. Van-\n",
      "derwende, P. Kohli, J. Allen, A corpus and evaluation framework\n",
      "for deeper understanding of commonsense stories, arXiv preprint\n",
      "arXiv:1604.01696 (2016). 28, 29\n",
      "[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\n",
      "----\n",
      "[335] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi,\n",
      "S. Pezzelle, M. Baroni, G. Boleda, R. Fernández, The lambada dataset:\n",
      "Word prediction requiring a broad discourse context, arXiv preprint\n",
      "arXiv:1606.06031 (2016). 28, 29\n",
      "[336] B. Hu, Q. Chen, F. Zhu, Lcsts: A large scale chinese short text summa-\n",
      "rization dataset, arXiv preprint arXiv:1506.05865 (2015). 29\n",
      "[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\n",
      "----\n",
      "[337] Z. Shao, M. Huang, J. Wen, W. Xu, X. Zhu, Long and diverse text gener-\n",
      "ation with planning-based hierarchical variational model, arXiv preprint\n",
      "arXiv:1908.06605 (2019). 29\n",
      "[338] J. Novikova, O. Dušek, V. Rieser, The e2e dataset: New challenges for\n",
      "end-to-end generation, arXiv preprint arXiv:1706.09254 (2017). 29\n",
      "[339] C. Zheng, M. Huang, A. Sun, Chid: A large-scale chinese idiom dataset\n",
      "for cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\n",
      "----\n",
      "for cloze test, arXiv preprint arXiv:1906.01265 (2019). 29\n",
      "[340] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al., Piqa: Reasoning about phys-\n",
      "ical commonsense in natural language, in: Proceedings of the AAAI\n",
      "conference on artificial intelligence, Vol. 34, 2020, pp. 7432–7439. 28,\n",
      "29\n",
      "[341] M. Joshi, E. Choi, D. S. Weld, L. Zettlemoyer, Triviaqa: A large scale\n",
      "distantly supervised challenge dataset for reading comprehension, arXiv\n",
      "preprint arXiv:1705.03551 (2017). 28, 29, 31\n",
      "----\n",
      "preprint arXiv:1705.03551 (2017). 28, 29, 31\n",
      "[342] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\n",
      "O. Tafjord, Think you have solved question answering? try arc, the ai2\n",
      "reasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 28, 29,\n",
      "31\n",
      "[343] S. Aroca-Ouellette, C. Paik, A. Roncone, K. Kann, Prost:\n",
      "Phys-\n",
      "ical reasoning of objects through space and time, arXiv preprint\n",
      "arXiv:2106.03634 (2021). 29\n",
      "----\n",
      "Phys-\n",
      "ical reasoning of objects through space and time, arXiv preprint\n",
      "arXiv:2106.03634 (2021). 29\n",
      "[344] T. Mihaylov, P. Clark, T. Khot, A. Sabharwal, Can a suit of armor con-\n",
      "duct electricity? a new dataset for open book question answering, arXiv\n",
      "preprint arXiv:1809.02789 (2018). 29\n",
      "[345] T. C. Ferreira, C. Gardent, N. Ilinykh, C. Van Der Lee, S. Mille,\n",
      "D. Moussallem, A. Shimorina, The 2020 bilingual, bi-directional\n",
      "webnlg+ shared task overview and evaluation results (webnlg+ 2020),\n",
      "----\n",
      "webnlg+ shared task overview and evaluation results (webnlg+ 2020),\n",
      "in: Proceedings of the 3rd International Workshop on Natural Language\n",
      "Generation from the Semantic Web (WebNLG+), 2020. 29\n",
      "[346] C. Xu, W. Zhou, T. Ge, K. Xu, J. McAuley, F. Wei, Blow the dog whistle:\n",
      "A chinese dataset for cant understanding with common sense and world\n",
      "knowledge, arXiv preprint arXiv:2104.02704 (2021). 29\n",
      "[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\n",
      "Large-scale\n",
      "----\n",
      "[347] G. Lai, Q. Xie, H. Liu, Y. Yang, E. Hovy, Race:\n",
      "Large-scale\n",
      "reading comprehension dataset from examinations, arXiv preprint\n",
      "arXiv:1704.04683 (2017). 29\n",
      "[348] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,\n",
      "L. Zettlemoyer, Quac: Question answering in context, arXiv preprint\n",
      "arXiv:1808.07036 (2018). 29\n",
      "[349] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, J. Berant, Did aristo-\n",
      "tle use a laptop? a question answering benchmark with implicit reason-\n",
      "----\n",
      "tle use a laptop? a question answering benchmark with implicit reason-\n",
      "ing strategies, Transactions of the Association for Computational Lin-\n",
      "guistics 9 (2021) 346–361. 29, 31\n",
      "[350] J. Boyd-Graber, B. Satinoff, H. He, H. Daumé III, Besting the quiz mas-\n",
      "ter: Crowdsourcing incremental classification games, in: Proceedings of\n",
      "the 2012 joint conference on empirical methods in natural language pro-\n",
      "cessing and computational natural language learning, 2012, pp. 1290–\n",
      "1301. 29\n",
      "----\n",
      "cessing and computational natural language learning, 2012, pp. 1290–\n",
      "1301. 29\n",
      "[351] S. Zhang, X. Zhang, H. Wang, J. Cheng, P. Li, Z. Ding, Chinese medical\n",
      "question answer matching using end-to-end character-level multi-scale\n",
      "cnns, Applied Sciences 7 (8) (2017) 767. 29\n",
      "[352] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive in-\n",
      "teraction networks for chinese medical question answer selection, IEEE\n",
      "43\n",
      "----\n",
      "Access 6 (2018) 74061–74071. 29\n",
      "[353] C. Xu, J. Pei, H. Wu, Y. Liu, C. Li, Matinf: A jointly labeled large-scale\n",
      "dataset for classification, question answering and summarization, arXiv\n",
      "preprint arXiv:2004.12302 (2020). 29\n",
      "[354] K. Sakaguchi, R. L. Bras, C. Bhagavatula, Y. Choi, Winogrande: An\n",
      "adversarial winograd schema challenge at scale, Communications of the\n",
      "ACM 64 (9) (2021) 99–106. 27, 29\n",
      "[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a\n",
      "----\n",
      "[355] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, Y. Choi, Hellaswag: Can a\n",
      "machine really finish your sentence?, arXiv preprint arXiv:1905.07830\n",
      "(2019). 29\n",
      "[356] M. Roemmele, C. A. Bejan, A. S. Gordon, Choice of plausible alter-\n",
      "natives: An evaluation of commonsense causal reasoning., in: AAAI\n",
      "spring symposium: logical formalizations of commonsense reasoning,\n",
      "2011, pp. 90–95. 29\n",
      "[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\n",
      "----\n",
      "2011, pp. 90–95. 29\n",
      "[357] H. Levesque, E. Davis, L. Morgenstern, The winograd schema chal-\n",
      "lenge, in: Thirteenth international conference on the principles of knowl-\n",
      "edge representation and reasoning, 2012. 27, 29\n",
      "[358] A. Talmor, J. Herzig, N. Lourie, J. Berant, Commonsenseqa: A question\n",
      "answering challenge targeting commonsense knowledge, arXiv preprint\n",
      "arXiv:1811.00937 (2018). 29, 31\n",
      "[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:\n",
      "----\n",
      "arXiv:1811.00937 (2018). 29, 31\n",
      "[359] M. Sap, H. Rashkin, D. Chen, R. LeBras, Y. Choi, Socialiqa:\n",
      "Commonsense reasoning about social interactions, arXiv preprint\n",
      "arXiv:1904.09728 (2019). 29\n",
      "[360] K. Sun, D. Yu, D. Yu, C. Cardie, Investigating prior knowledge for chal-\n",
      "lenging chinese machine reading comprehension, Transactions of the\n",
      "Association for Computational Linguistics 8 (2020) 141–155. 29\n",
      "[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\n",
      "----\n",
      "[361] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, B. Van Durme, Record: Bridg-\n",
      "ing the gap between human and machine commonsense reading compre-\n",
      "hension, arXiv preprint arXiv:1810.12885 (2018). 29\n",
      "[362] P. Rajpurkar, J. Zhang, K. Lopyrev, P. Liang, Squad: 100,000+ questions\n",
      "for machine comprehension of text, arXiv preprint arXiv:1606.05250\n",
      "(2016). 29, 31\n",
      "[363] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins,\n",
      "K. Toutanova, Boolq: Exploring the surprising difficulty of natural\n",
      "----\n",
      "K. Toutanova, Boolq: Exploring the surprising difficulty of natural\n",
      "yes/no questions, arXiv preprint arXiv:1905.10044 (2019). 29, 31\n",
      "[364] P. Rajpurkar, R. Jia, P. Liang, Know what you don’t know: Unanswer-\n",
      "able questions for squad, arXiv preprint arXiv:1806.03822 (2018). 29,\n",
      "31\n",
      "[365] D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, M. Gardner, Drop:\n",
      "A reading comprehension benchmark requiring discrete reasoning over\n",
      "paragraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\n",
      "----\n",
      "paragraphs, arXiv preprint arXiv:1903.00161 (2019). 29, 31\n",
      "[366] I. Dagan, O. Glickman, B. Magnini, The pascal recognising textual en-\n",
      "tailment challenge, in: Machine learning challenges workshop, Springer,\n",
      "2005, pp. 177–190. 29, 31\n",
      "[367] Y. Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, Y. Bisk, Webqa: Mul-\n",
      "tihop and multimodal qa, in: Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\n",
      "29, 31\n",
      "----\n",
      "on Computer Vision and Pattern Recognition, 2022, pp. 16495–16504.\n",
      "29, 31\n",
      "[368] Y. Cui, T. Liu, Z. Chen, W. Ma, S. Wang, G. Hu, Dataset for the first\n",
      "evaluation on chinese machine reading comprehension, arXiv preprint\n",
      "arXiv:1709.08299 (2017). 29\n",
      "[369] Y. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, G. Hu,\n",
      "A span-extraction dataset for chinese machine reading comprehension,\n",
      "arXiv preprint arXiv:1810.07366 (2018). 29, 31\n",
      "----\n",
      "arXiv preprint arXiv:1810.07366 (2018). 29, 31\n",
      "[370] Y. Cui, T. Liu, Z. Yang, Z. Chen, W. Ma, W. Che, S. Wang, G. Hu,\n",
      "A sentence cloze dataset for chinese machine reading comprehension,\n",
      "arXiv preprint arXiv:2004.03116 (2020). 29\n",
      "[371] Y. Li, T. Liu, D. Li, Q. Li, J. Shi, Y. Wang, Character-based bilstm-crf\n",
      "incorporating pos and dictionaries for chinese opinion target extraction,\n",
      "in: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\n",
      "29\n",
      "----\n",
      "in: Asian Conference on Machine Learning, PMLR, 2018, pp. 518–533.\n",
      "29\n",
      "[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, D. Roth, Look-\n",
      "ing beyond the surface: A challenge set for reading comprehension\n",
      "over multiple sentences, in: Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguis-\n",
      "tics: Human Language Technologies, Volume 1 (Long Papers), 2018,\n",
      "pp. 252–262. 29\n",
      "----\n",
      "tics: Human Language Technologies, Volume 1 (Long Papers), 2018,\n",
      "pp. 252–262. 29\n",
      "[373] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Al-\n",
      "berti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al., Natural ques-\n",
      "tions: a benchmark for question answering research, Transactions of the\n",
      "Association for Computational Linguistics 7 (2019) 453–466. 29\n",
      "[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-\n",
      "----\n",
      "[374] C. C. Shao, T. Liu, Y. Lai, Y. Tseng, S. Tsai, Drcd: A chinese ma-\n",
      "chine reading comprehension dataset, arXiv preprint arXiv:1806.00920\n",
      "(2018). 29\n",
      "[375] W. He, K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu,\n",
      "Q. She, et al., Dureader: a chinese machine reading comprehension\n",
      "dataset from real-world applications, arXiv preprint arXiv:1711.05073\n",
      "(2017). 29\n",
      "[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A\n",
      "----\n",
      "(2017). 29\n",
      "[376] H. Tang, J. Liu, H. Li, Y. Hong, H. Wu, H. Wang, Dureaderrobust: A\n",
      "chinese dataset towards evaluating the robustness of machine reading\n",
      "comprehension models, arXiv preprint arXiv:2004.11142 (2020). 29\n",
      "[377] J. Welbl, N. F. Liu, M. Gardner, Crowdsourcing multiple choice science\n",
      "questions, arXiv preprint arXiv:1707.06209 (2017). 29\n",
      "[378] C. Xiong, Z. Dai, J. Callan, Z. Liu, R. Power, End-to-end neural ad-hoc\n",
      "ranking with kernel pooling, in: Proceedings of the 40th International\n",
      "----\n",
      "ranking with kernel pooling, in: Proceedings of the 40th International\n",
      "ACM SIGIR conference on research and development in information\n",
      "retrieval, 2017, pp. 55–64. 29\n",
      "[379] A. Peñas, E. Hovy, P. Forner, Á. Rodrigo, R. Sutcliffe, R. Morante,\n",
      "Qa4mre 2011-2013: Overview of question answering for machine read-\n",
      "ing evaluation, in: Information Access Evaluation. Multilinguality, Mul-\n",
      "timodality, and Visualization: 4th International Conference of the CLEF\n",
      "----\n",
      "timodality, and Visualization: 4th International Conference of the CLEF\n",
      "Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Pro-\n",
      "ceedings 4, Springer, 2013, pp. 303–320. 29\n",
      "[380] S. Lim, M. Kim, J. Lee, Korquad1. 0: Korean qa dataset for machine\n",
      "reading comprehension, arXiv preprint arXiv:1909.07005 (2019). 29\n",
      "[381] C. Xiao, H. Zhong, Z. Guo, C. Tu, Z. Liu, M. Sun, Y. Feng, X. Han,\n",
      "Z. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\n",
      "----\n",
      "Z. Hu, H. Wang, et al., Cail2018: A large-scale legal dataset for judg-\n",
      "ment prediction, arXiv preprint arXiv:1807.02478 (2018). 29\n",
      "[382] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\n",
      "C. Burns, S. Puranik, H. He, D. Song, et al., Measuring coding challenge\n",
      "competence with apps, arXiv preprint arXiv:2105.09938 (2021). 29, 31\n",
      "[383] Y. Wang, X. Liu, S. Shi, Deep neural solver for math word problems,\n",
      "in: Proceedings of the 2017 conference on empirical methods in natural\n",
      "----\n",
      "in: Proceedings of the 2017 conference on empirical methods in natural\n",
      "language processing, 2017, pp. 845–854. 29, 31\n",
      "[384] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\n",
      "M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al., Training verifiers\n",
      "to solve math word problems, arXiv preprint arXiv:2110.14168 (2021).\n",
      "29, 31\n",
      "[385] J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan,\n",
      "E. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with\n",
      "----\n",
      "E. Jiang, C. J. Cai, M. Terry, Q. V. Le, C. Sutton, Program synthesis with\n",
      "large language models, CoRR abs/2108.07732 (2021). 29\n",
      "[386] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W.\n",
      "Chung, Y. Tay, S. Ruder, D. Zhou, et al., Language models are mul-\n",
      "tilingual chain-of-thought reasoners, arXiv preprint arXiv:2210.03057\n",
      "(2022). 29\n",
      "[387] S. Roy, D. Roth, Solving general arithmetic word problems, arXiv\n",
      "preprint arXiv:1608.01413 (2016). 29\n",
      "----\n",
      "preprint arXiv:1608.01413 (2016). 29\n",
      "[388] S.-Y. Miao, C.-C. Liang, K.-Y. Su, A diverse corpus for evaluating\n",
      "and developing english math word problem solvers, arXiv preprint\n",
      "arXiv:2106.15772 (2021). 29\n",
      "[389] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman, H. Hajishirzi,\n",
      "Mawps: A math word problem repository, in: Proceedings of the 2016\n",
      "conference of the north american chapter of the association for computa-\n",
      "tional linguistics: human language technologies, 2016, pp. 1152–1157.\n",
      "29\n",
      "----\n",
      "tional linguistics: human language technologies, 2016, pp. 1152–1157.\n",
      "29\n",
      "[390] A. Patel, S. Bhattamishra, N. Goyal, Are nlp models really able to solve\n",
      "simple math word problems?, arXiv preprint arXiv:2103.07191 (2021).\n",
      "29\n",
      "[391] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W.-t. Yih,\n",
      "D. Fried, S. Wang, T. Yu, Ds-1000: A natural and reliable benchmark for\n",
      "data science code generation, in: International Conference on Machine\n",
      "Learning, PMLR, 2023, pp. 18319–18345. 29\n",
      "----\n",
      "Learning, PMLR, 2023, pp. 18319–18345. 29\n",
      "[392] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\n",
      "E. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large\n",
      "language models, arXiv preprint arXiv:2108.07732 (2021). 29\n",
      "[393] Y. Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, D. Kiela, Adver-\n",
      "sarial nli: A new benchmark for natural language understanding, arXiv\n",
      "preprint arXiv:1910.14599 (2019). 29, 31\n",
      "----\n",
      "preprint arXiv:1910.14599 (2019). 29, 31\n",
      "[394] A. Williams, N. Nangia, S. R. Bowman, A broad-coverage challenge\n",
      "corpus for sentence understanding through inference, arXiv preprint\n",
      "arXiv:1704.05426 (2017). 29\n",
      "[395] R. T. McCoy, E. Pavlick, T. Linzen, Right for the wrong reasons: Diag-\n",
      "44\n",
      "----\n",
      "nosing syntactic heuristics in natural language inference, arXiv preprint\n",
      "arXiv:1902.01007 (2019). 29\n",
      "[396] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, Y. Zhang, Logiqa: A chal-\n",
      "lenge dataset for machine reading comprehension with logical reason-\n",
      "ing, arXiv preprint arXiv:2007.08124 (2020). 29\n",
      "[397] P. Lewis, B. O˘guz, R. Rinott, S. Riedel, H. Schwenk, Mlqa: Eval-\n",
      "uating cross-lingual extractive question answering, arXiv preprint\n",
      "arXiv:1910.07475 (2019). 29\n",
      "----\n",
      "uating cross-lingual extractive question answering, arXiv preprint\n",
      "arXiv:1910.07475 (2019). 29\n",
      "[398] A. Conneau, G. Lample, R. Rinott, A. Williams, S. R. Bowman,\n",
      "H. Schwenk, V. Stoyanov, Xnli: Evaluating cross-lingual sentence rep-\n",
      "resentations, arXiv preprint arXiv:1809.05053 (2018). 29, 31\n",
      "[399] Y. Yang,\n",
      "Y. Zhang,\n",
      "C. Tar,\n",
      "J. Baldridge,\n",
      "Paws-x:\n",
      "A cross-\n",
      "lingual adversarial dataset for paraphrase identification, arXiv preprint\n",
      "arXiv:1908.11828 (2019). 29, 31\n",
      "----\n",
      "arXiv:1908.11828 (2019). 29, 31\n",
      "[400] S. Narayan, S. B. Cohen, M. Lapata, Don’t give me the details, just the\n",
      "summary!, Topic-Aware Convolutional Neural Networks for Extreme\n",
      "Summarization. ArXiv, abs (1808). 29\n",
      "[401] E. M. Ponti, G. Glavaš, O. Majewska, Q. Liu, I. Vuli´c, A. Korhonen,\n",
      "Xcopa: A multilingual dataset for causal commonsense reasoning, arXiv\n",
      "preprint arXiv:2005.00333 (2020). 29\n",
      "[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\n",
      "----\n",
      "[402] A. Tikhonov, M. Ryabinin, It’s all in the heads: Using attention heads\n",
      "as a baseline for cross-lingual transfer in commonsense reasoning, arXiv\n",
      "preprint arXiv:2106.12066 (2021). 29\n",
      "[403] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Niko-\n",
      "laev, J. Palomaki, Tydi qa: A benchmark for information-seeking ques-\n",
      "tion answering in typologically diverse languages, Transactions of the\n",
      "Association for Computational Linguistics 8 (2020) 454–470. 29\n",
      "----\n",
      "Association for Computational Linguistics 8 (2020) 454–470. 29\n",
      "[404] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano,\n",
      "Mlsum:\n",
      "The multilingual summarization corpus,\n",
      "arXiv preprint\n",
      "arXiv:2004.14900 (2020). 29\n",
      "[405] S. Lin, J. Hilton, O. Evans, Truthfulqa: Measuring how models mimic\n",
      "human falsehoods, arXiv preprint arXiv:2109.07958 (2021). 29, 32\n",
      "[406] I. Augenstein,\n",
      "C. Lioma,\n",
      "D. Wang,\n",
      "L. C. Lima,\n",
      "C. Hansen,\n",
      "C. Hansen, J. G. Simonsen, Multifc:\n",
      "A real-world multi-domain\n",
      "----\n",
      "D. Wang,\n",
      "L. C. Lima,\n",
      "C. Hansen,\n",
      "C. Hansen, J. G. Simonsen, Multifc:\n",
      "A real-world multi-domain\n",
      "dataset for evidence-based fact checking of claims, arXiv preprint\n",
      "arXiv:1909.03242 (2019). 29\n",
      "[407] J. Thorne, A. Vlachos, C. Christodoulopoulos, A. Mittal, Fever: a\n",
      "large-scale dataset for fact extraction and verification, arXiv preprint\n",
      "arXiv:1803.05355 (2018). 29\n",
      "[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\n",
      "----\n",
      "[408] I. Mollas, Z. Chrysopoulou, S. Karlos, G. Tsoumakas, Ethos: an online\n",
      "hate speech detection dataset, arXiv preprint arXiv:2006.08328 (2020).\n",
      "29, 32\n",
      "[409] M. Nadeem, A. Bethke, S. Reddy, Stereoset: Measuring stereotypical\n",
      "bias in pretrained language models, arXiv preprint arXiv:2004.09456\n",
      "(2020). 29, 32\n",
      "[410] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thomp-\n",
      "son, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\n",
      "----\n",
      "son, P. M. Htut, S. R. Bowman, Bbq: A hand-built bias benchmark for\n",
      "question answering, arXiv preprint arXiv:2110.08193 (2021). 29\n",
      "[411] J. Zhao, T. Wang, M. Yatskar, V. Ordonez, K.-W. Chang, Gender bias\n",
      "in coreference resolution: Evaluation and debiasing methods, arXiv\n",
      "preprint arXiv:1804.06876 (2018). 29\n",
      "[412] N. Nangia, C. Vania, R. Bhalerao, S. R. Bowman, Crows-pairs: A chal-\n",
      "lenge dataset for measuring social biases in masked language models,\n",
      "arXiv preprint arXiv:2010.00133 (2020). 29\n",
      "----\n",
      "arXiv preprint arXiv:2010.00133 (2020). 29\n",
      "[413] S. Gehman, S. Gururangan, M. Sap, Y. Choi, N. A. Smith, Realtoxic-\n",
      "ityprompts: Evaluating neural toxic degeneration in language models,\n",
      "arXiv preprint arXiv:2009.11462 (2020). 29\n",
      "[414] D. Borkan, L. Dixon, J. Sorensen, N. Thain, L. Vasserman, Nuanced\n",
      "metrics for measuring unintended bias with real data for text classifica-\n",
      "tion, in: Companion proceedings of the 2019 world wide web confer-\n",
      "ence, 2019, pp. 491–500. 29\n",
      "----\n",
      "tion, in: Companion proceedings of the 2019 world wide web confer-\n",
      "ence, 2019, pp. 491–500. 29\n",
      "[415] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\n",
      "M. Huck, A. J. Yepes, P. Koehn, V. Logacheva, C. Monz, et al., Find-\n",
      "ings of the 2016 conference on machine translation, in: Proceedings of\n",
      "the First Conference on Machine Translation: Volume 2, Shared Task\n",
      "Papers, 2016, pp. 131–198. 29\n",
      "[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-\n",
      "----\n",
      "[416] B. Loïc, B. Magdalena, B. Ondˇrej, F. Christian, G. Yvette, G. Ro-\n",
      "man, H. Barry, H. Matthias, J. Eric, K. Tom, et al., Findings of the\n",
      "2020 conference on machine translation (wmt20), in: Proceedings of\n",
      "the Fifth Conference on Machine Translation, Association for Compu-\n",
      "tational Linguistics„ 2020, pp. 1–55. 29\n",
      "[417] W. Li, F. Qi, M. Sun, X. Yi, J. Zhang, Ccpm: A chinese classical poetry\n",
      "matching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\n",
      "----\n",
      "matching dataset, arXiv preprint arXiv:2106.01979 (2021). 29\n",
      "[418] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, J. Weston, Wizard of\n",
      "wikipedia: Knowledge-powered conversational agents, arXiv preprint\n",
      "arXiv:1811.01241 (2018). 29\n",
      "[419] H. Rashkin, E. M. Smith, M. Li, Y.-L. Boureau, Towards empathetic\n",
      "open-domain conversation models: A new benchmark and dataset, arXiv\n",
      "preprint arXiv:1811.00207 (2018). 29\n",
      "[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,\n",
      "----\n",
      "[420] E. Dinan, V. Logacheva, V. Malykh, A. Miller, K. Shuster, J. Urbanek,\n",
      "D. Kiela, A. Szlam, I. Serban, R. Lowe, et al., The second conversa-\n",
      "tional intelligence challenge (convai2), in: The NeurIPS’18 Competi-\n",
      "tion: From Machine Learning to Intelligent Conversations, Springer,\n",
      "2020, pp. 187–208. 29\n",
      "[421] H. Zhou, C. Zheng, K. Huang, M. Huang, X. Zhu, Kdconv: A chinese\n",
      "multi-domain dialogue dataset towards multi-turn knowledge-driven\n",
      "conversation, arXiv preprint arXiv:2004.04100 (2020). 29\n",
      "----\n",
      "conversation, arXiv preprint arXiv:2004.04100 (2020). 29\n",
      "[422] L. CO, Iflytek: a multiple categories chinese text classifier. competition\n",
      "official website (2019). 29\n",
      "[423] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, J. Blackburn, The\n",
      "pushshift reddit dataset, in: Proceedings of the international AAAI con-\n",
      "ference on web and social media, Vol. 14, 2020, pp. 830–839. 30\n",
      "[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\n",
      "----\n",
      "[424] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, M. Auli, Eli5: Long\n",
      "form question answering, arXiv preprint arXiv:1907.09190 (2019). 31\n",
      "[425] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei,\n",
      "A. Arunkumar, A. Ashok, A. S. Dhanasekaran, A. Naik, D. Stap, et al.,\n",
      "Benchmarking generalization via in-context instructions on 1,600+ lan-\n",
      "guage tasks, arXiv preprint arXiv:2204.07705 (2022). 31\n",
      "[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\n",
      "----\n",
      "[426] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu,\n",
      "M. Zhong, P. Yin, S. I. Wang, et al., Unifiedskg: Unifying and multi-\n",
      "tasking structured knowledge grounding with text-to-text language mod-\n",
      "els, arXiv preprint arXiv:2201.05966 (2022). 31\n",
      "[427] Q. Ye, B. Y. Lin, X. Ren, Crossfit: A few-shot learning challenge\n",
      "for cross-task generalization in nlp, arXiv preprint arXiv:2104.08835\n",
      "(2021). 31\n",
      "[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,\n",
      "----\n",
      "(2021). 31\n",
      "[428] V. Aribandi, Y. Tay, T. Schuster, J. Rao, H. S. Zheng, S. V. Mehta,\n",
      "H. Zhuang, V. Q. Tran, D. Bahri, J. Ni, et al., Ext5: Towards extreme\n",
      "multi-task scaling for transfer learning, arXiv preprint arXiv:2111.10952\n",
      "(2021). 31\n",
      "[429] A. Williams, N. Nangia, S. Bowman, A broad-coverage challenge cor-\n",
      "pus for sentence understanding through inference, in: Proceedings of\n",
      "the 2018 Conference of the North American Chapter of the Associ-\n",
      "----\n",
      "the 2018 Conference of the North American Chapter of the Associ-\n",
      "ation for Computational Linguistics: Human Language Technologies,\n",
      "Volume 1 (Long Papers), Association for Computational Linguistics,\n",
      "New Orleans, Louisiana, 2018, pp. 1112–1122. doi:10.18653/v1/\n",
      "N18-1101.\n",
      "URL https://aclanthology.org/N18-1101 31\n",
      "[430] Y. Zhang, J. Baldridge, L. He, PAWS: Paraphrase adversaries from word\n",
      "scrambling, in: Proceedings of the 2019 Conference of the North Amer-\n",
      "----\n",
      "scrambling, in: Proceedings of the 2019 Conference of the North Amer-\n",
      "ican Chapter of the Association for Computational Linguistics: Human\n",
      "Language Technologies, Volume 1 (Long and Short Papers), Associa-\n",
      "tion for Computational Linguistics, Minneapolis, Minnesota, 2019, pp.\n",
      "1298–1308. doi:10.18653/v1/N19-1131.\n",
      "URL https://aclanthology.org/N19-1131 32\n",
      "[431] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, D. Yang, Is chat-\n",
      "GPT a general-purpose natural language processing task solver?, in: The\n",
      "----\n",
      "GPT a general-purpose natural language processing task solver?, in: The\n",
      "2023 Conference on Empirical Methods in Natural Language Process-\n",
      "ing, 2023.\n",
      "URL https://openreview.net/forum?id=u03xn1COsO 32\n",
      "[432] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. B. Shaikh,\n",
      "N. Akhtar, J. Wu, S. Mirjalili, et al., Large language models: a com-\n",
      "prehensive survey of its applications, challenges, limitations, and future\n",
      "prospects, TechRxiv (2023). 32\n",
      "----\n",
      "prospects, TechRxiv (2023). 32\n",
      "[433] X. L. Dong, S. Moon, Y. E. Xu, K. Malik, Z. Yu, Towards next-\n",
      "generation intelligent assistants leveraging llm techniques, in: Proceed-\n",
      "ings of the 29th ACM SIGKDD Conference on Knowledge Discovery\n",
      "and Data Mining, 2023, pp. 5792–5793. 32\n",
      "[434] K. Pandya, M. Holia, Automating customer service using langchain:\n",
      "Building custom open-source gpt chatbot for organizations, arXiv\n",
      "preprint arXiv:2310.05421 (2023). 32\n",
      "----\n",
      "preprint arXiv:2310.05421 (2023). 32\n",
      "[435] J. Li, B. Hui, G. Qu, B. Li, J. Yang, B. Li, B. Wang, B. Qin, R. Cao,\n",
      "R. Geng, et al., Can llm already serve as a database interface?\n",
      "a\n",
      "45\n",
      "----\n",
      "big bench for large-scale database grounded text-to-sqls, arXiv preprint\n",
      "arXiv:2305.03111 (2023). 32\n",
      "[436] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, M. D. Succi, Evaluating\n",
      "chatgpt as an adjunct for radiologic decision-making, medRxiv (2023)\n",
      "2023–02. 32\n",
      "[437] M. Benary, X. D. Wang, M. Schmidt, D. Soll, G. Hilfenhaus, M. Nas-\n",
      "sir, C. Sigler, M. Knödler, U. Keller, D. Beule, et al., Leveraging large\n",
      "language models for decision support in personalized oncology, JAMA\n",
      "----\n",
      "language models for decision support in personalized oncology, JAMA\n",
      "Network Open 6 (11) (2023) e2343689–e2343689. 32\n",
      "[438] C. M. Chiesa-Estomba, J. R. Lechien, L. A. Vaira, A. Brunet, G. Cam-\n",
      "maroto, M. Mayo-Yanez, A. Sanchez-Barrueco, C. Saga-Gutierrez, Ex-\n",
      "ploring the potential of chat-gpt as a supportive tool for sialendoscopy\n",
      "clinical decision making and patient information support, European\n",
      "Archives of Oto-Rhino-Laryngology (2023) 1–6. 32\n",
      "----\n",
      "Archives of Oto-Rhino-Laryngology (2023) 1–6. 32\n",
      "[439] S. Montagna, S. Ferretti, L. C. Klopfenstein, A. Florio, M. F. Pengo,\n",
      "Data decentralisation of llm-based chatbot systems in chronic disease\n",
      "self-management, in: Proceedings of the 2023 ACM Conference on In-\n",
      "formation Technology for Social Good, 2023, pp. 205–212. 32\n",
      "[440] D. Bill, T. Eriksson, Fine-tuning a llm using reinforcement learning from\n",
      "human feedback for a therapy chatbot application (2023). 32\n",
      "----\n",
      "human feedback for a therapy chatbot application (2023). 32\n",
      "[441] M. Abbasian, I. Azimi, A. M. Rahmani, R. Jain, Conversational health\n",
      "agents: A personalized llm-powered agent framework, arXiv preprint\n",
      "arXiv:2310.02374 (2023). 32\n",
      "[442] K. V. Lemley, Does chatgpt help us understand the medical literature?,\n",
      "Journal of the American Society of Nephrology (2023) 10–1681. 32\n",
      "[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\n",
      "----\n",
      "[443] S. Pal, M. Bhattacharya, S.-S. Lee, C. Chakraborty, A domain-specific\n",
      "next-generation large language model (llm) or chatgpt is required for\n",
      "biomedical engineering and research, Annals of Biomedical Engineering\n",
      "(2023) 1–4. 32\n",
      "[444] Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, B. Qin, The\n",
      "calla dataset: Probing llms’ interactive knowledge acquisition from chi-\n",
      "nese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\n",
      "----\n",
      "nese medical literature, arXiv preprint arXiv:2309.04198 (2023). 32\n",
      "[445] A. Abd-Alrazaq, R. AlSaad, D. Alhuwail, A. Ahmed, P. M. Healy,\n",
      "S. Latifi, S. Aziz, R. Damseh, S. A. Alrazak, J. Sheikh, et al., Large\n",
      "language models in medical education: Opportunities, challenges, and\n",
      "future directions, JMIR Medical Education 9 (1) (2023) e48291. 32\n",
      "[446] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, A. Dagan,\n",
      "Chatgpt passing usmle shines a spotlight on the flaws of medical educa-\n",
      "----\n",
      "Chatgpt passing usmle shines a spotlight on the flaws of medical educa-\n",
      "tion (2023). 32\n",
      "[447] S. Ahn, The impending impacts of large language models on medical\n",
      "education, Korean Journal of Medical Education 35 (1) (2023) 103. 32\n",
      "[448] E. Waisberg, J. Ong, M. Masalkhi, A. G. Lee, Large language model\n",
      "(llm)-driven chatbots for neuro-ophthalmic medical education, Eye\n",
      "(2023) 1–3. 32\n",
      "[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\n",
      "----\n",
      "(2023) 1–3. 32\n",
      "[449] G. Deiana, M. Dettori, A. Arghittu, A. Azara, G. Gabutti, P. Castiglia,\n",
      "Artificial intelligence and public health: Evaluating chatgpt responses to\n",
      "vaccination myths and misconceptions, Vaccines 11 (7) (2023) 1217. 32\n",
      "[450] L. De Angelis, F. Baglivo, G. Arzilli, G. P. Privitera, P. Ferragina, A. E.\n",
      "Tozzi, C. Rizzo, Chatgpt and the rise of large language models: the new\n",
      "ai-driven infodemic threat in public health, Frontiers in Public Health 11\n",
      "(2023) 1166120. 32\n",
      "----\n",
      "ai-driven infodemic threat in public health, Frontiers in Public Health 11\n",
      "(2023) 1166120. 32\n",
      "[451] N. L. Rane, A. Tawde, S. P. Choudhary, J. Rane, Contribution and per-\n",
      "formance of chatgpt and other large language models (llm) for scientific\n",
      "and research advancements: a double-edged sword, International Re-\n",
      "search Journal of Modernization in Engineering Technology and Science\n",
      "5 (10) (2023) 875–899. 32\n",
      "[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large\n",
      "----\n",
      "[452] W. Dai, J. Lin, H. Jin, T. Li, Y.-S. Tsai, D. Gaševi´c, G. Chen, Can large\n",
      "language models provide feedback to students? a case study on chatgpt,\n",
      "in: 2023 IEEE International Conference on Advanced Learning Tech-\n",
      "nologies (ICALT), IEEE, 2023, pp. 323–325. 32\n",
      "[453] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva,\n",
      "F. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.,\n",
      "Chatgpt for good? on opportunities and challenges of large language\n",
      "----\n",
      "Chatgpt for good? on opportunities and challenges of large language\n",
      "models for education, Learning and individual differences 103 (2023)\n",
      "102274. 32\n",
      "[454] N. Rane, Enhancing the quality of teaching and learning through chat-\n",
      "gpt and similar large language models: Challenges, future prospects,\n",
      "and ethical considerations in education, Future Prospects, and Ethical\n",
      "Considerations in Education (September 15, 2023) (2023). 32\n",
      "----\n",
      "Considerations in Education (September 15, 2023) (2023). 32\n",
      "[455] J. C. Young, M. Shishido, Investigating openai’s chatgpt potentials in\n",
      "generating chatbot’s dialogue for english as a foreign language learning,\n",
      "International Journal of Advanced Computer Science and Applications\n",
      "14 (6) (2023). 32\n",
      "[456] J. Irons, C. Mason, P. Cooper, S. Sidra, A. Reeson, C. Paris, Exploring\n",
      "the impacts of chatgpt on future scientific work, SocArXiv (2023). 32\n",
      "----\n",
      "the impacts of chatgpt on future scientific work, SocArXiv (2023). 32\n",
      "[457] P. G. Schmidt, A. J. Meir, Using generative ai for literature searches and\n",
      "scholarly writing: Is the integrity of the scientific discourse in jeopardy?,\n",
      "arXiv preprint arXiv:2311.06981 (2023). 32\n",
      "[458] Y. Zheng, H. Y. Koh, J. Ju, A. T. Nguyen, L. T. May, G. I. Webb, S. Pan,\n",
      "Large language models for scientific synthesis, inference and explana-\n",
      "tion, arXiv preprint arXiv:2310.07984 (2023). 33\n",
      "----\n",
      "tion, arXiv preprint arXiv:2310.07984 (2023). 33\n",
      "[459] B. Aczel, E.-J. Wagenmakers, Transparency guidance for chatgpt usage\n",
      "in scientific writing, PsyArXiv (2023). 33\n",
      "[460] S. Altmäe, A. Sola-Leyva, A. Salumets, Artificial intelligence in sci-\n",
      "entific writing: a friend or a foe?, Reproductive BioMedicine Online\n",
      "(2023). 33\n",
      "[461] S. Imani, L. Du, H. Shrivastava, Mathprompter: Mathematical reasoning\n",
      "using large language models, arXiv preprint arXiv:2303.05398 (2023).\n",
      "33\n",
      "----\n",
      "using large language models, arXiv preprint arXiv:2303.05398 (2023).\n",
      "33\n",
      "[462] Z. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, C. Zhou, Scaling relationship\n",
      "on learning mathematical reasoning with large language models, arXiv\n",
      "preprint arXiv:2308.01825 (2023). 33\n",
      "[463] K. Yang, A. M. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil,\n",
      "R. Prenger, A. Anandkumar, Leandojo: Theorem proving with retrieval-\n",
      "augmented language models, arXiv preprint arXiv:2306.15626 (2023).\n",
      "33\n",
      "----\n",
      "augmented language models, arXiv preprint arXiv:2306.15626 (2023).\n",
      "33\n",
      "[464] K. M. Collins, A. Q. Jiang, S. Frieder, L. Wong, M. Zilka, U. Bhatt,\n",
      "T. Lukasiewicz, Y. Wu, J. B. Tenenbaum, W. Hart, et al., Evaluating\n",
      "language models for mathematics through interactions, arXiv preprint\n",
      "arXiv:2306.01694 (2023). 33\n",
      "[465] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li, M. He,\n",
      "Z. Liu, et al., Summary of chatgpt-related research and perspective\n",
      "----\n",
      "Z. Liu, et al., Summary of chatgpt-related research and perspective\n",
      "towards the future of large language models, Meta-Radiology (2023)\n",
      "100017. 33\n",
      "[466] J. Drápal, H. Westermann, J. Savelka, Using large language models\n",
      "to support thematic analysis in empirical legal studies, arXiv preprint\n",
      "arXiv:2310.18729 (2023). 33\n",
      "[467] J. Savelka, K. D. Ashley, M. A. Gray, H. Westermann, H. Xu, Explain-\n",
      "ing legal concepts with augmented large language models (gpt-4), arXiv\n",
      "----\n",
      "ing legal concepts with augmented large language models (gpt-4), arXiv\n",
      "preprint arXiv:2306.09525 (2023). 33\n",
      "[468] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,\n",
      "A. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore, et al., Legal-\n",
      "bench: A collaboratively built benchmark for measuring legal reasoning\n",
      "in large language models, arXiv preprint arXiv:2308.11462 (2023). 33\n",
      "[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\n",
      "----\n",
      "[469] J. Cui, Z. Li, Y. Yan, B. Chen, L. Yuan, Chatlaw: Open-source legal\n",
      "large language model with integrated external knowledge bases, arXiv\n",
      "preprint arXiv:2306.16092 (2023). 33\n",
      "[470] H. Yang, X.-Y. Liu, C. D. Wang, Fingpt: Open-source financial large\n",
      "language models, arXiv preprint arXiv:2306.06031 (2023). 33\n",
      "[471] Y. Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\n",
      "survey, in: Proceedings of the Fourth ACM International Conference on\n",
      "AI in Finance, 2023, pp. 374–382. 33\n",
      "----\n",
      "AI in Finance, 2023, pp. 374–382. 33\n",
      "[472] A. Lykov, D. Tsetserukou, Llm-brain: Ai-driven fast generation of\n",
      "robot behaviour tree based on large language model, arXiv preprint\n",
      "arXiv:2305.19352 (2023). 33\n",
      "[473] E. Billing, J. Rosén, M. Lamb, Language models for human-robot inter-\n",
      "action, in: ACM/IEEE International Conference on Human-Robot Inter-\n",
      "action, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\n",
      "2023, pp. 905–906. 33\n",
      "----\n",
      "action, March 13–16, 2023, Stockholm, Sweden, ACM Digital Library,\n",
      "2023, pp. 905–906. 33\n",
      "[474] Y. Ye, H. You, J. Du, Improved trust in human-robot collaboration with\n",
      "chatgpt, IEEE Access (2023). 33\n",
      "[475] Y. Ding, X. Zhang, C. Paxton, S. Zhang, Leveraging commonsense\n",
      "knowledge from large language models for task and motion planning,\n",
      "in: RSS 2023 Workshop on Learning for Task and Motion Planning,\n",
      "2023. 33\n",
      "[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\n",
      "----\n",
      "2023. 33\n",
      "[476] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\n",
      "S. Rusinkiewicz, T. Funkhouser, Tidybot: Personalized robot assistance\n",
      "with large language models, arXiv preprint arXiv:2305.05658 (2023).\n",
      "33\n",
      "[477] E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations\n",
      "for deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). 34\n",
      "[478] E. M. Bender, T. Gebru, A. McMillan-Major, S. Shmitchell, On the dan-\n",
      "46\n",
      "----\n",
      "gers of stochastic parrots: Can language models be too big?, in: Pro-\n",
      "ceedings of the 2021 ACM conference on fairness, accountability, and\n",
      "transparency, 2021, pp. 610–623. 34\n",
      "[479] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding\n",
      "deep learning (still) requires rethinking generalization, Communications\n",
      "of the ACM 64 (3) (2021) 107–115. 34\n",
      "[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\n",
      "----\n",
      "[480] M. Tänzer, S. Ruder, M. Rei, Memorisation versus generalisation in pre-\n",
      "trained language models, arXiv preprint arXiv:2105.00828 (2021). 34\n",
      "[481] S. M. West, M. Whittaker, K. Crawford, Discriminating systems, AI\n",
      "Now (2019) 1–33. 34\n",
      "[482] K. Valmeekam, A. Olmo, S. Sreedharan, S. Kambhampati, Large lan-\n",
      "guage models still can’t plan (a benchmark for llms on planning and\n",
      "reasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\n",
      "----\n",
      "reasoning about change), arXiv preprint arXiv:2206.10498 (2022). 34\n",
      "[483] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\n",
      "Y. Zhang, Y. Chen, et al., Siren’s song in the ai ocean: A survey on hal-\n",
      "lucination in large language models, arXiv preprint arXiv:2309.01219\n",
      "(2023). 34\n",
      "[484] A. Webson, E. Pavlick, Do prompt-based models really understand the\n",
      "meaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\n",
      "----\n",
      "meaning of their prompts?, arXiv preprint arXiv:2109.01247 (2021). 34\n",
      "[485] O. Shaikh, H. Zhang, W. Held, M. Bernstein, D. Yang, On second\n",
      "thought, let’s not think step by step! bias and toxicity in zero-shot rea-\n",
      "soning, arXiv preprint arXiv:2212.08061 (2022). 34\n",
      "[486] B. C. Das, M. H. Amini, Y. Wu, Security and privacy challenges of large\n",
      "language models: A survey, arXiv preprint arXiv:2402.00888 (2024). 34\n",
      "[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-\n",
      "----\n",
      "[487] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, J. Gao, Adversar-\n",
      "ial training for large neural language models, ArXiv (April 2020).\n",
      "URL\n",
      "https://www.microsoft.com/en-us/research/\n",
      "publication/adversarial-training-for-large-neural-language-models/\n",
      "34\n",
      "[488] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong, N. Abu-\n",
      "Ghazaleh, Survey of vulnerabilities in large language models revealed\n",
      "by adversarial attacks (2023). arXiv:2310.10844. 34\n",
      "----\n",
      "by adversarial attacks (2023). arXiv:2310.10844. 34\n",
      "[489] X. Xu, K. Kong, N. Liu, L. Cui, D. Wang, J. Zhang, M. Kankanhalli, An\n",
      "llm can fool itself: A prompt-based adversarial attack (2023). arXiv:\n",
      "2310.13345. 34\n",
      "[490] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\n",
      "M. Du, Explainability for large language models: A survey (2023).\n",
      "arXiv:2309.01029. 35\n",
      "[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large\n",
      "----\n",
      "arXiv:2309.01029. 35\n",
      "[491] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, L. H. Gilpin, Can large\n",
      "language models explain themselves? a study of llm-generated self-\n",
      "explanations (2023). arXiv:2310.11207. 35\n",
      "[492] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, F. Tramèr, What does it\n",
      "mean for a language model to preserve privacy?, in: Proceedings of the\n",
      "2022 ACM Conference on Fairness, Accountability, and Transparency,\n",
      "2022, pp. 2280–2292. 35\n",
      "----\n",
      "2022 ACM Conference on Fairness, Accountability, and Transparency,\n",
      "2022, pp. 2280–2292. 35\n",
      "[493] R. Plant, V. Giuffrida, D. Gkatzia, You are what you write:\n",
      "Pre-\n",
      "serving privacy in the era of large language models, arXiv preprint\n",
      "arXiv:2204.09391 (2022). 35\n",
      "[494] W. Niu, Z. Kong, G. Yuan, W. Jiang, J. Guan, C. Ding, P. Zhao, S. Liu,\n",
      "B. Ren, Y. Wang, Real-time execution of large-scale language models\n",
      "on mobile (2020). arXiv:2009.06823. 35\n",
      "----\n",
      "on mobile (2020). arXiv:2009.06823. 35\n",
      "[495] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu, M. Guo,\n",
      "Y. Zhu, Olive:\n",
      "Accelerating large language models via hardware-\n",
      "friendly outlier-victim pair quantization, in: Proceedings of the 50th\n",
      "Annual International Symposium on Computer Architecture, 2023, pp.\n",
      "1–15. 35\n",
      "[496] B. Meskó, E. J. Topol, The imperative for regulatory oversight of large\n",
      "language models (or generative ai) in healthcare, npj Digital Medicine\n",
      "6 (1) (2023) 120. 35\n",
      "----\n",
      "language models (or generative ai) in healthcare, npj Digital Medicine\n",
      "6 (1) (2023) 120. 35\n",
      "[497] J. Zhang, X. Ji, Z. Zhao, X. Hei, K.-K. R. Choo, Ethical considerations\n",
      "and policy implications for large language models: Guiding responsible\n",
      "development and deployment, arXiv preprint arXiv:2308.02678 (2023).\n",
      "35\n",
      "[498] J. Mökander, J. Schuett, H. R. Kirk, L. Floridi, Auditing large language\n",
      "models: a three-layered approach, AI and Ethics (2023) 1–31. 35\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "for doc in chunked_documents:\n",
    "    print(\"----\")\n",
    "    print(doc.page_content[:500])  # Print first 500 characters of each chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d6e0aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retriever Pipeline Frome VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "84c1749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    #Handles query based retrieval from vector store\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager:EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str,any]]:\n",
    "        print(f\"Retrieveing docs for query: '{query}'\")\n",
    "        print(f\"Top k : {top_k}, score threshold: {score_threshold}\")\n",
    "\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "    #Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "    #Process results\n",
    "            retrieved_docs=[]\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "\n",
    "                    similarity_score = 1 -distance\n",
    "\n",
    "                    if similarity_score>= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "            return retrieved_docs\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return[]\n",
    "        \n",
    "rag_retriever=RAGRetriever(vectorestore,embedding_manager)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "820f2b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x24ecea9f380>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "05820cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieveing docs for query: 'large language models overview'\n",
      "Top k : 5, score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "Retrieved 4 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_3add7509_0',\n",
       "  'content': 'Langchain Introduction\\nLangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\\n\\nWith LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\\n\\nThis framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\\n',\n",
       "  'metadata': {'content_length': 546,\n",
       "   'source': '../data/text_files/langchain.txt',\n",
       "   'doc_index': 0},\n",
       "  'similarity_score': 0.4313049912452698,\n",
       "  'distance': 0.5686950087547302,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_e71dde46_0',\n",
       "  'content': 'Langchain Introduction\\nLangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\\n\\nWith LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\\n\\nThis framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\\n',\n",
       "  'metadata': {'content_length': 546,\n",
       "   'doc_index': 0,\n",
       "   'source': '../data/text_files/langchain.txt'},\n",
       "  'similarity_score': 0.4313049912452698,\n",
       "  'distance': 0.5686950087547302,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_3c9754ce_0',\n",
       "  'content': 'Langchain Introduction\\nLangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\\n\\nWith LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\\n\\nThis framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\\n',\n",
       "  'metadata': {'doc_index': 0,\n",
       "   'content_length': 546,\n",
       "   'source': '../data/text_files/langchain.txt'},\n",
       "  'similarity_score': 0.4313049912452698,\n",
       "  'distance': 0.5686950087547302,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_ea3d2b80_0',\n",
       "  'content': 'Langchain Introduction\\nLangChain is a powerful framework designed to help developers build applications with large language models (LLMs). It enables seamless integration of LLMs with data sources such as documents, APIs, and databases.\\n\\nWith LangChain, you can implement Retrieval-Augmented Generation (RAG), which combines document retrieval with LLMs to produce accurate and context-aware answers.\\n\\nThis framework supports various document loaders, text splitting, vector stores, and chaining methods to create sophisticated NLP applications.\\n',\n",
       "  'metadata': {'doc_index': 0,\n",
       "   'content_length': 546,\n",
       "   'source': '../data/text_files/langchain.txt'},\n",
       "  'similarity_score': 0.4313049912452698,\n",
       "  'distance': 0.5686950087547302,\n",
       "  'rank': 4}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"large language models overview\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
